\chapter{Introduzione}
\label{Introduzione}
\thispagestyle{empty}
I servizi informatici che utilizziamo quotidianamente---dalla posta elettronica ai motori di ricerca, dai \textit{social network} ai giochi online---sono sempre pi\`u spesso gratuiti e si sostengono grazie alla pubblicit\`a: direttamente, esponendo contenuto promozionale, o indirettamente, dalla valorizzazione delle nostre informazioni personali a fini pubblicitari. Per massimizzare l'efficacia della pubblicit\`a, ossia trasformare l'utente in cliente, \`e tuttavia essenziale identificare i gusti e gli interessi dell'individuo che si ha di fronte, e proporgli inserzioni personalizzate che rispondano o addirittura anticipino una sua reale esigenza. Nell'era odierna dei Big Data, una moltitudine di nuovi dati su masse di utenti \`e prontamente accessibile: l'attivit\`a sui siti internet, il profilo sui social network, le relazioni sociali e il sentiment (le opinioni, i gusti e l'affiliazione a idee, prodotti, marchi, individui).  Ci\`o ha portato a ridefinire l'idea stessa della segmentazione di mercato---la disaggregazione dei clienti in gruppi omogenei per esigenze e comportamento di acquisto---per cui la ripartizione in aree geografiche, demografiche e psicografiche \`e obsoleta o insufficiente. Queste realt\`a, la necessit\`a di profilare gli utenti e il diluvio di informazione grezza che ciascuno di noi divulga su internet, sono fortemente sinergiche, ma richiedono una laboriosa raffinazione per poter essere impiegate nel marketing.\\
La segmentazione di mercato pu\`o essere costruita a partire dalle divisioni identificate mediante tecniche di clustering, un processo che organizza una popolazione in gruppi formati da individui affini. Un esperto dovr\`a poi attribuire un significato (\textit{profiling}) a tali gruppi, comporli per caratterizzare una tipologia di consumatori tramite le variabili usate per il clustering nonch\'e le caratteristiche demografiche, geografiche e comportamentali degli individui, al fine di elaborare una strategia di marketing peculiare per ciascun segmento.\\
Pertanto, per avvalersi dell'informazione proveniente dai social network a fini commerciali, \`e necessario definire una metodologia per il clustering di grandi volumi di dati: profili utente con un elevato numero di attributi e talvolta arricchiti da una tela di relazioni sociali. Lo scopo della tesi \`e realizzare un framework per il clustering di vasti grafi con attributi, che sia di supporto al processo di segmentazione.\\
Questo lavoro \`e stato motivato dall'esigenza di Neosperience, una azienda che offre servizi di marketing e \textit{customer experience}, di valorizzare i dati degli utenti---sotto forma di profili Facebook, Twitter e Foursquare---raccolti tramite la propria piattaforma Engage. Il lavoro \`e iniziato dall'analisi degli algoritmi esistenti in letteratura che potessero adattarsi al tipo, dimensione e volume dei dati su cui avremmo operato. In seguito sono stati raccolti i dati, una ampia collezione di profili Facebook, composti da informazioni di profilo, preferenze (\textit{like}) e connessioni sociali (\textit{friend}). Una volta reperito il codice degli algoritmi, i dati sono stati ripuliti e preparati per le tecniche di clustering selezionate. Dall'esecuzione degli algoritmi su svariati campioni del dataset completo abbiamo potuto evidenziare i punti di forza e debolezza di ciascun approccio. Tramite la misurazione di indici di qualit\`a, siamo in grado di suggerire qual \`e la miglior parametrizzazione di ciascun algoritmo su un dataset in input e di svolgere una analisi comparativa sulle prestazioni delle diverse tecniche.

\section{Contesto del lavoro}
\label{sec:contesto}
Il clustering \`e un processo che, partendo da una definizione di similarit\`a e una popolazione, produce una suddivisione degli individui in gruppi o \textit{cluster}, al fine di massimizzare la similarit\`a intra-cluster, ovvero l'omogeneit\`a degli elementi raggruppati insieme, e la dissimilarit\`a inter-cluster, ossia accentuare le differenze tra gruppi diversi. Si tratta di un procedimento \textit{data-driven}, inevitabilmente determinato dai dati e dalle esigenze del committente; \textit{clustering is in the eye of the beholder}, e non ha pertanto senso dettare quale algoritmo e scelta dei parametri siano sistematicamente ottimali. Nel caso in esame, tuttavia, la struttura dei dati \`e stabile e ci\`o permette di avanzare delle linee guida per selezionare preliminarmente un ventaglio di approcci di maggior successo.\\
Negli ultimi anni, lo sviluppo delle capacit\`a di memorizzazione ed elaborazione dei dati, unitamente alla diffusione pervasiva di Internet, hanno prodotto una svolta nell'era digitale, nella quantit\`a e qualit\`a di informazione nascosta nei dati che una tecnica di clustering potrebbe svelare. Questa rivoluzione ha preso il nome di \textit{Big Data}. Big data indica la disponibilit\`a di enormi masse di dati, strutturati e non strutturati: una miniera di informazione grezza che richiede tecniche innovative di elaborazione per capire profondamente la realt\`a in cui si opera e prendere decisioni migliori\footnote{``Big data is high volume, high velocity, and/or high variety information assets that demand cost-effective, innovative forms of information processing to enable enhanced decision making, insight discovery and process optimization'' - Gartner \cite{laney01}}.
Il paradigma dei Big Data \`e articolato in tre V: Volume Velocit\`a Variet\`a.
\begin{description}
\item[Volume]
%Al crescere del volume dei dati, altri fattori oltre alla qualit\`a del risultato assumono rilievo nella valutazione di un algoritmo: la complessit\`a spaziale e temporale, ovvero quanta memoria e tempo di computazione sono necessari per produrre il clustering finale.
Al crescere del volume dei dati, nella valutazione di un algoritmo assumono rilievo non soltanto la qualit\`a dei suoi risultati, ma anche la complessit\`a spaziale e temporale. Inoltre, quando si parla di volume dei dati non ci si riferisce unicamente alla loro cardinalit\`a, ma anche al numero di propriet\`a o dimensioni associate ad ogni individuo. L'effetto della \textit{dimensionalit\`a} elevata sul clustering \`e duplice: da un lato, le dimensioni irrilevanti, quegli attributi rispetto ai quali non c'\`e aggregazione, costituiscono rumore per gli algoritmi; dall'altro, alcune definizioni di distanza (ad esempio quella euclidea) non riflettono pi\`u la reale entit\`a delle differenze tra gli individui \cite{Aggarwal01,Beyer99}.
\item[Velocit\`a e Variabilit\`a] Una rete sociale \`e un oggetto dinamico, nella struttura e nei contenuti. Ogni giorno si sviluppano nuove connessioni, tanto tra individui---\textit{friend} in Facebook e \textit{follower} in Twitter---quanto verso entit\`a---\textit{like} in Facebook e \textit{checkin} in Foursquare. Anche la velocit\`a con cui la nostra impronta digitale evolve varia a seconda dei singoli dati che consideriamo: se da un lato il luogo o la data di nascita sono immutabili, al contrario le amicizie, le relazioni sentimentali e i gusti sono via via pi\`u volubili, e devono essere raccolti ed analizzati prima che diventino obsoleti.
\item[Variet\`a] I dati si manifestano in innumerevoli forme: pur avendo fissate le sorgenti di informazione---Facebook, Twitter e Foursquare---da esse si ricavano tabelle strutturate, flussi di messaggi ed ogni sorta di contenuto multimediale. La sfida \`e trovare forme adeguate di organizzazione dei dati e tecniche di analisi capaci di adattarsi alla variet\`a.
\end{description}
Alcune propriet\`a notevoli delle reti sociali contribuiscono tuttavia a dominare la variabilit\`a nei dati. In una rete sociale ogni nodo \`e densamente connesso ai propri vicini, cio\`e esibisce un intrinseco clustering locale; per esempio, \`e frequente che i nostri amici siano anche amici tra loro. Inoltre, la distanza media tra una coppia arbitraria di nodi di una rete sociale \`e sorprendentemente bassa rispetto alla dimensione del grafo\footnote{Una rete \`e detta \textit{small-world network} se la distanza tra due individui scelti a caso \`e proporzionale a $log N$, dove $N$ \`e il numero di nodi}; tale propriet\`a \`e nota come \textit{teoria del mondo piccolo}. Da ultimo, la somiglianza genera connessioni; questo principio, l'\textit{omofilia}, spiega perch\'e siamo spesso simili ai nostri amici, per et\`a, esperienze passate e passioni. In conclusione, la variet\`a dei dati provenienti da reti sociali non \`e libera, ma esibisce delle regolarit\`a, specialmente nella cerchia ristretta di ciascun individuo.\\
Prima di poter essere sottoposti al clustering, i dati grezzi devono tuttavia essere elaborati per elevarne la qualit\`a. Questa fase preparatoria, detta \textit{preprocessing}, si sostanzia dei seguenti passi: pulizia, integrazione, riduzione e trasformazione dei dati. La \textit{pulizia} dei dati ha lo scopo di identificare le incongruenze, rimpiazzare i dati mancanti, attenuare il rumore e rimuovere le anomalie. Dall'\textit{integrazione}, le differenti sorgenti di dati confluiscono in un unico archivio, avendo cura di definire un formato unico, coerente e privo di ridondanza verso il quale convertire le sorgenti. Partendo da questo modello dei dati \`e possibile definire le caratteristiche dell'algoritmo di clustering ideale, che far\`a da guida nella nella selezione delle metodologie esistenti in letteratura. Poich\'e tecniche diverse richiedono specifiche propriet\`a degli input per offrire i migliori risultati, i due passi successivi sono necessari per adattare i dati all'algoritmo scelto. La \textit{riduzione} consiste nel comprimere la forma dei dati, in particolare il numero di attributi o il numero di individui, cercando di preservarne intatta la sostanza, l'informazione nascosta. In ultimo si esegue la \textit{trasformazione}, che agisce sulla scala, sul tipo e sulla granularit\`a dei singoli attributi. Oltre ai requisiti imposti dai dati, una caratteristica desiderabile per un algoritmo applicato alle reti sociali \`e la capacit\`a di individuare comunit\`a parzialmente sovrapposte o addirittura annidate, nonch\'e identificare cluster omogenei per diversi sottoinsiemi di attributi (\textit{subspace clustering}). Infine, l'algoritmo dovrebbe essere scalabile nelle dimensioni e nella cardinalit\`a dei dati.\\
Durante l'analisi sperimentale, si porr\`a il problema di decidere quale scelta dei parametri dell'algoritmo---ad esempio il numero di comunit\`a da individuare nella popolazione---produca il miglior risultato. Sebbene esistano svariati indici numerici di qualit\`a, le propriet\`a desiderabili di un clustering sono la purezza e la sintesi. La purezza misura quanto ciascun gruppo \`e composto da individui simili, privo di elementi estranei che appartengono ad altri gruppi o sono atipici rispetto all'intera popolazione. La sintesi giudica quanta inutile complessit\`a e ridondanza vi sia nel clustering, ed indirettamente la capacit\`a del modello di essere generalizzato per descrivere il fenomeno da cui i dati sono estratti.
%Il nostro studio non ha individuato una metodologia che rispondesse simultaneamente a tutte queste esigenze, pertanto abbiamo selezionato un ampio ventaglio di candidati con caratteristiche diverse e che almeno in parte soddisfacessero il profilo ideale che abbiamo delineato. Questa eterogeneit\`a rende difficile confrontare algoritmi di diversa natura, n\'e lo scopo del lavoro \`e stilare una inverosimile classifica. Piuttosto, l'obiettivo \`e selezionare le tecniche che possono produrre un risultato di rilievo, definire degli indici che aiutino l'analista dei dati a separare i clustering soddisfacenti da quelli insignificanti, offrire delle procedure per unificare diversi clustering potenziali \cite{Strehl03}.\\

\section{Breve descrizione del lavoro}
L'esigenza principale di Neosperience \`e quella di disporre di uno strumento, il pi\`u possibile automatico, che possa valorizzare i dati grezzi dei propri clienti. Questi infatti possiedono dei grandi database di \textit{clienti finali} che, di per s\'e, non presentano nessun valore addizionale in quanto mancano di una semantica associata ad essi. Una delle aree pi\`u importanti del marketing \`e la \textit{segmentazione di mercato}, che si prefigge lo scopo di trovare regolarit\`a (di esigenze e comportamenti d'acquisto) all'interno di un numeroso insieme di individui. \`e stato proprio questo l'obiettivo di Neosperience: dare un significato ai dati tratteggiando le diverse tipologie di clienti che ne emergono. Da un lato, la segmentazione \`e considerata un compito creativo basato sull'intuizione del dirigente aziendale nell'individuare aree potenziali di mercato, dall'altro necessita, sicuramente, di una metodologia precisa e scrupolosa che consente di determinare almeno i confini teorici entro i quali poter prendere delle decisioni ponderate. La cluster analysis viene in aiuto in questo senso, mettendo a disposizioni metodi automatici per identificare gruppi omogenei di individui secondo una definizione di similarit\`a tra questi. 
Delineiamo ora quali sono stati i passi fondamentali del lavoro di tesi.

\subsubsection{Analisi della letteratura}
Come in ogni campo della scienza, per un determinato problema non esiste un'unica soluzione ma sempre \`e possibile scegliere o progettare una vasta gamma di strumenti che propongono dei risultati diversi e con una qualit\`a che varia a seconda delle loro caratteristiche. Ci\`o porta a vantaggi immediati: a causa della variet\`a delle basi di dati e della loro mutevolezza nel tempo, \`e necessario affidarsi ad un ventaglio di soluzioni in modo tale da poter scegliere quella che, per un certo input, risulti essere ottimale. Di conseguenza, l'analisi della letteratura non \`e stata focalizzata sulla ricerca di un solo algoritmo capace di risolvere qualsiasi problema; si \`e cercato, infatti, di avvalersi di un insieme di algoritmi in cui ognuno potesse intervenire sui dati in input a seconda dei propri meriti: come, ad esempio, la capacit\`a di gestire grafi completi o con componenti isolate, di affrontare dataset con un elevato numero di attributi o con valori mancanti, oppure di essere scalabile in funzione della dimensione dell'input e delle performance dell'hardware [cit degli algo utilizzati].\\
Infine, oltre agli algoritmi di clustering, sono state studiate metodologie di \textit{data preprocessing} e pacchetti software (R [cit], Matlab [cit], Gremlin [cit], Weka [cit], RapidMiner [cit]) per la raccolta e l'analisi di dati.

\subsubsection{Raccolta e analisi dati}
Col fine di ottenere una base di dati su cui eseguire e valutare gli algoritmi selezionati abbiamo sviluppato un' applicazione PHP che recupera il profilo Facebook di un utente e dei suoi amici. In particolare, per ogni utente dell'applicazione, si raccolgono le informazioni di profilo (genere, citt\`a natale, citt\`a di residenza, istruzione), le preferenze (le pagine su cui si \`e cliccato \textit{like}), i legami d'amicizia e, di nuovo, informazioni di profilo e preferenze degli amici. In questa maniera, da ogni utente si ricava una rete personale (\textit{ego-network}) che, composta con le altre, ha permesso la creazione di un unico grafo con attributi.\\
% La richiesta fondamentale di Neosperience \`e che il framework potesse essere in grado di gestire un generico dataset in input in quanto, al momento della commissione, non possedevano ancora una base di dati completa. Ci\`o ha introdotto un importante ordine di problemi: da un lato c'\`e stata la necessit\`a di raccogliere un insieme di utenti di test su cui eseguire e valutare gli algoritmi, dall'altro si \`e cercato di creare, da questi, un ventaglio di sottoinsiemi di input che potessero modellare il generico dataset in ingresso.
Il primo passo \`e stato quello di ripulire il grafo da utenti con un'alta percentuale di informazioni mancanti (sia relazionali che di profilo); successivamente si \`e svolta, tramite considerazioni statistiche, l'imputazione dei \textit{missing value} e la selezione di attributi rilevanti per disporre di informazioni complete e significative su ogni utente. Infine, \`e stato sviluppato un algoritmo di campionamento che, da un lato, riuscisse a creare sottografi significativi dal punto di vista topologico e degli attributi, dall'altro, che presentasse una forte componente casuale per modellare la variet\`a dei dati in input che potrebbero essere usati in seguito. Conclusa questa fase abbiamo quindi ottenuto un insieme di sottografi su cui \`e possibile eseguire e valutare gli algoritmi di clustering. Infine, ogni sottografo \`e stato etichettato con una serie di misurazioni topologiche che lo \virgolette{identificano}.

\subsubsection{Valutazione e confronto degli algoritmi}
Ogni algoritmo presenta una serie di parametri di partenza che devono essere impostati accuratamente per elevare la qualit\`a dei risultati: \`e quasi sempre possibile specificare, ad esempio, il numero di cluster da identificare, oppure il numero massimo di attributi che caratterizzeranno un cluster. %Al fine di disporre di uno strumento il pi\`u possibile automatico e che possa essere usato con relativa semplicit\`a, uno degli obiettivi di questa fase \`e stato calibrare il valore dei parametri a seconda del dataset in ingresso. 
Tramite la misurazione dei tempi d'esecuzione e degli \textit{indici di validit\`a} --- stime che valutano quanto sia appropriato un certo clustering in relazione alle caratteristiche degli individui --- si \`e raggiunto l'obiettivo \virgolette{zero} del lavoro: capire qual \`e la miglior parametrizzazione di un algoritmo su un dataset in input. La fase successiva \`e stata quella di confrontare e combinare questi risultati per ogni algoritmo a disposizione: in questo modo abbiamo raccolto tutte le informazioni necessarie per condurre delle analisi pi\`u complesse.
Fissati i dati sotto esame, \`e possibile, infatti, rispondere a domande come le seguenti:
\begin{itemize}
\item Come varia la stessa prestazione P (ad esempio il tempo d'esecuzione) di due algoritmi al variare dei loro parametri? \`e possibile decidere quale dei due algoritmi \`e mediamente il migliore in relazione a P?
\item Valutando l'andamento dei vari indici prestazionali, esiste un trend che permette di decidere qual \`e l'algoritmo, in generale, pi\`u efficace?
\item Senza testare tutti gli algoritmi a disposizione, \`e possibile suggerire quale potrebbe essere il miglior algoritmo per un certo dataset?
\end{itemize}

In particolare la risposta all'ultimo punto si ottiene tramite questo ragionamento: come gi\`a affermato, ogni dataset di test \`e stato etichettato da misurazioni topologiche. Quindi, calcolando le stesse misurazioni su un dataset in input $ D_{new} $ \`e possibile individuare il dataset di test $ D_{test} $ ad esso pi\`u simile e proporre per $ D_{new} $ le valutazioni ottenute su $ D_{test} $. In questo modo si ottiene una stima, da valutare attentamente, su come potrebbero comportarsi gli algoritmi sul nuovo input, senza eseguire effettivamente tutte le valutazioni del caso. Questa particolarit\`a \`e dettata dall'esigenza di avere uno strumento agile che possa per lo meno identificare quali sono gli algoritmi che, probabilmente, avranno delle basse prestazioni. Ad esempio, se $ D_{new} $ ha un numero elevato di nodi e si sa che un certo algoritmo su un dataset di test con lo stesso numero di nodi ha un tempo d'esecuzione intollerabile, allora il framework proporr\`a di evitare quell'algoritmo, almeno per le prime fasi di analisi di $ D_{new} 
$.\\
Infine, per semplificare ulteriormente il compito di Neosperience, abbiamo sviluppato un tool per attribuire un significato \virgolette{descrittivo} al risultato di clustering delineandole caratteristiche di ogni cluster, tramite media, varianza e lista di valori degli attributi.

Prima di concludere, c'\`e da specificare che quando si parla di algoritmo o soluzione \virgolette{migliore} lo si fa si sempre in relazione ad una indice di validit\`a che offre solamente una stima della \virgolette{qualit\`a} del risultato. Questa stima non vuole in nessun modo sostituire l'importante collaborazione tra l'analista dei dati e l'esperto di mercato: entrambi, in maniera sinergica, dovranno valutare le soluzioni e prendere spunto da esse per la definizione finale dei segmenti di mercato.

\subsubsection{Sviluppi futuri}
[Per semplicit\`a, questa parte verr\`a scritta in concomitanza con la scrittura dell'ultimo capitolo \virgolette{Conclusioni e sviluppi futuri}]



\section{Struttura della tesi}
La tesi \`e strutturata nel modo seguente.\\
Nel \autoref{capitolo2} analizzeremo la letteratura accademica sul clustering, con un particolare accento sul problema della dimensionalit\`a. Inoltre, discuteremo la problematica della pulizia dei dati e della valutazione dei dataset e dei risultati del clustering.\\
Nel \autoref{capitolo3} descriveremo il framework Engage di Neosperience, per il quale questo lavoro \`e stato svolto, e presentiamo la nostra soluzione, dalla raccolta dei dati fino alla esecuzione degli algoritmi.\\
Nel \autoref{capitolo4} discutiamo la procedura ed i risultati dell'analisi sperimentale.\\
Nel \autoref{capitolo5} esponiamo le conclusioni e i possibili sviluppi del lavoro.\\
