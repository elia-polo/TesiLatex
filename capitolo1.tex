\chapter{Introduzione}
\label{Introduzione}
\thispagestyle{empty}
I servizi informatici che utilizziamo quotidianamente---dalla posta elettronica ai motori di ricerca, dai \textit{social network} ai giochi online---sono sempre più spesso gratuiti e si sostengono grazie alla pubblicità: direttamente, esponendo contenuto promozionale, o indirettamente, dalla valorizzazione delle nostre informazioni personali a fini pubblicitari. Per massimizzare l'efficacia della pubblicità, ossia trasformare l'utente in cliente, è tuttavia essenziale identificare i gusti e gli interessi dell'individuo che si ha di fronte, e proporgli inserzioni personalizzate che rispondano o addirittura anticipino una sua reale esigenza. Nell'era odierna dei Big Data, una moltitudine di nuovi dati su masse di
utenti \`e prontamente accessibile: l'attivit\`a sui siti internet, il profilo sui
social network, le relazioni sociali e il sentiment (le opinioni, i gusti e l'affiliazione a idee, prodotti, marchi, individui).  Ci\`o ha portato a ridefinire l'idea stessa della segmentazione di mercato---la disaggregazione dei clienti in gruppi omogenei per esigenze e comportamento di acquisto---per cui la ripartizione in aree geografiche, demografiche e psicografiche \`e obsoleta o insufficiente. Queste realt\`a, la necessit\`a di profilare gli utenti e il diluvio di informazione grezza che ciascuno di noi divulga su internet, sono fortemente sinergiche, ma richiedono una laboriosa raffinazione per poter essere impiegate nel marketing.\\
La segmentazione di mercato può essere costruita a partire dalle divisioni identificate mediante tecniche di clustering, un processo che organizza una popolazione in gruppi formati da individui affini. Un esperto dovr\`a poi attribuire un significato (profiling) a tali gruppi, comporli per caratterizzare una tipologia di consumatori tramite le variabili usate per il clustering nonché le caratteristiche demografiche, geografiche e comportamentali degli individui, al fine di elaborare una strategia di marketing peculiare per ciascun segmento.\\
Pertanto, per avvalersi dell'informazione proveniente dai social network a fini commerciali, è necessario definire una metodologia per il clustering di grandi volumi di dati: profili utente con un elevato numero di attributi e talvolta arricchiti da una tela di relazioni sociali. Lo scopo della tesi è realizzare un framework per il clustering di vasti grafi con attributi, che sia di supporto al processo di segmentazione.\\
Questo lavoro è stato motivato dall'esigenza di Neosperience, una azienda che offre servizi di marketing e \textit{customer experience}, di valorizzare i dati degli utenti---sotto forma di profili Facebook, Twitter e Foursquare---raccolti tramite la propria piattaforma Engage. Il lavoro è iniziato dall'analisi degli algoritmi esistenti in letteratura che potessero adattarsi al tipo, dimensione e volume dei dati su cui avremmo operato. In seguito sono stati raccolti i dati, una ampia collezione di profili Facebook, composti da informazioni di profilo, preferenze (\textit{like}) e connessioni sociali (\textit{friend}). Una volta reperito il codice degli algoritmi, i dati sono stati ripuliti e preparati per le tecniche di clustering selezionate. Dall'esecuzione degli algoritmi su svariati campioni del dataset completo abbiamo potuto evidenziare i punti di forza e debolezza di ciascun approccio. Tramite la misurazione di indici di qualità, siamo in grado di suggerire qual è la miglior parametrizzazione di ciascun algoritmo su un dataset in input e di svolgere una analisi comparativa sulle prestazioni delle diverse tecniche.

\section{Contesto del lavoro}
\label{sec:contesto}
Il clustering è un processo che, partendo da una definizione di similarità e una popolazione, produce una suddivisione degli individui in gruppi o \textit{cluster}, al fine di massimizzare la similarità intra-cluster, ovvero l'omogeneità degli elementi raggruppati insieme, e la dissimilarità inter-cluster, ossia accentuare le differenze tra gruppi diversi. Si tratta di un procedimento \textit{data-driven}, inevitabilmente determinato dai dati e dalle esigenze del committente; \textit{clustering is in the eye of the beholder}, e non ha pertanto senso dettare quale algoritmo e scelta dei parametri siano sistematicamente ottimali. Nel caso in esame, tuttavia, la struttura dei dati è stabile e ciò permette di avanzare delle linee guida per selezionare preliminarmente un ventaglio di approcci di maggior successo.\\
Negli ultimi anni, lo sviluppo delle capacità di memorizzazione ed elaborazione dei dati, unitamente alla diffusione pervasiva di Internet, hanno prodotto una svolta nell'era digitale, nella quantità e qualità di informazione nascosta nei dati che una tecnica di clustering potrebbe svelare. Questa rivoluzione ha preso il nome di \textit{Big Data}. Big data indica la disponibilità di enormi masse di dati, strutturati e non strutturati: una miniera di informazione grezza che richiede tecniche innovative di elaborazione per capire profondamente la realtà in cui si opera e prendere decisioni migliori\footnote{``Big data is high volume, high velocity, and/or high variety information assets that demand cost-effective, innovative forms of information processing to enable enhanced decision making, insight discovery and process optimization'' - Gartner \cite{laney01}}.
Il paradigma dei Big Data è articolato in tre V: Volume Velocità Varietà.
\begin{description}
\item[Volume]
%Al crescere del volume dei dati, altri fattori oltre alla qualità del risultato assumono rilievo nella valutazione di un algoritmo: la complessità spaziale e temporale, ovvero quanta memoria e tempo di computazione sono necessari per produrre il clustering finale.
Al crescere del volume dei dati, nella valutazione di un algoritmo assumono rilievo non soltanto la qualità dei suoi risultati, ma anche la complessità spaziale e temporale. Inoltre, quando si parla di volume dei dati non ci si riferisce unicamente alla loro cardinalità, ma anche al numero di proprietà o dimensioni associate ad ogni individuo. L'effetto della \textit{dimensionalità} elevata sul clustering è duplice: da un lato, le dimensioni irrilevanti, quegli attributi rispetto ai quali non c'è aggregazione, costituiscono rumore per gli algoritmi; dall'altro, alcune definizioni di distanza (ad esempio quella euclidea) non riflettono più la reale entità delle differenze tra gli individui \cite{Aggarwal01,Beyer99}.
\item[Velocità e Variabilità] Una rete sociale è un oggetto dinamico, nella struttura e nei contenuti. Ogni giorno si sviluppano nuove connessioni, tanto tra individui---\textit{friend} in Facebook e \textit{follower} in Twitter---quanto verso entità---\textit{like} in Facebook e \textit{checkin} in Foursquare. Anche la velocità con cui la nostra impronta digitale evolve varia a seconda dei singoli dati che consideriamo: se da un lato il luogo o la data di nascita sono permanenti, al contrario le amicizie, le relazioni sentimentali e i gusti sono via via più volubili, e devono essere raccolti ed analizzati prima che diventino obsoleti.
\item[Varietà] I dati si manifestano in innumerevoli forme: pur avendo fissate le sorgenti di informazione---Facebook, Twitter e Foursquare---da esse si ricavano tabelle strutturate, flussi di messaggi ed ogni sorta di contenuto multimediale. La sfida è trovare forme adeguate di organizzazione dei dati e tecniche di analisi capaci di adattarsi alla varietà.
\end{description}
Alcune proprietà notevoli delle reti sociali contribuiscono tuttavia a dominare la variabilità nei dati. In una rete sociale ogni nodo è densamente connesso ai propri vicini, cioè esibisce un intrinseco clustering locale; per esempio, è frequente che i nostri amici siano anche amici tra loro. Inoltre, la distanza media tra una coppia arbitraria di nodi di una rete sociale è sorprendentemente bassa rispetto alla dimensione del grafo\footnote{Una rete è detta \textit{small-world network} se la distanza tra due individui scelti a caso è proporzionale a $log N$, dove $N$ è il numero di nodi}; tale proprietà è nota come \textit{teoria del mondo piccolo}. Da ultimo, la somiglianza genera connessioni; questo principio, l'\textit{omofilia}, spiega perché siamo spesso simili ai nostri amici, per età, esperienze passate e passioni. In conclusione, la varietà dei dati provenienti da reti sociali non è libera, ma esibisce delle regolarità, specialmente nella cerchia ristretta di ciascun individuo.\\
Prima di poter essere sottoposti al clustering, i dati grezzi devono tuttavia essere elaborati per elevarne la qualità. Questa fase preparatoria, detta \textit{preprocessing}, si sostanzia dei seguenti passi: pulizia, integrazione, riduzione e trasformazione dei dati. La \textit{pulizia} dei dati ha lo scopo di identificare le incongruenze, rimpiazzare i dati mancanti, attenuare il rumore e rimuovere le anomalie. Dall'\textit{integrazione}, le differenti sorgenti di dati confluiscono in un unico archivio, avendo cura di definire un formato unico, coerente e privo di ridondanza verso il quale convertire le sorgenti. Partendo da questo modello dei dati è possibile definire le caratteristiche dell'algoritmo di clustering ideale, che farà da guida nella nella selezione delle metodologie esistenti in letteratura. Poiché tecniche diverse richiedono specifiche proprietà degli input per offrire i migliori risultati, i due passi successivi sono necessari per adattare i dati all'algoritmo scelto. La \textit{riduzione} consiste nel comprimere la forma dei dati, in particolare il numero di attributi o il numero di individui, cercando di preservarne intatta la sostanza, l'informazione nascosta. In ultimo si esegue la \textit{trasformazione}, che agisce sulla scala, sul tipo e sulla granularità dei singoli attributi. Oltre ai requisiti imposti dai dati, una caratteristica desiderabile per un algoritmo applicato alle reti sociali è la capacità di individuare comunità parzialmente sovrapposte o addirittura annidate, nonché identificare cluster omogenei per diversi sottoinsiemi di attributi (\textit{subspace clustering}). Infine, l'algoritmo dovrebbe essere scalabile nelle dimensioni e nella cardinalità dei dati.\\
Durante l'analisi sperimentale, si porrà il problema di decidere quale scelta dei parametri dell'algoritmo---ad esempio il numero di comunità da individuare nella popolazione---produca il miglior risultato. Sebbene esistano svariati indici numerici di qualità, le proprietà desiderabili di un clustering sono la purezza e la sintesi. La purezza misura quanto ciascun gruppo è composto da individui simili, privo di elementi estranei che appartengono ad altri gruppi o sono atipici rispetto all'intera popolazione. La sintesi giudica quanta inutile complessità e ridondanza vi sia nel clustering, ed indirettamente la capacità del modello di essere generalizzato per descrivere il fenomeno da cui i dati sono estratti.
%Il nostro studio non ha individuato una metodologia che rispondesse simultaneamente a tutte queste esigenze, pertanto abbiamo selezionato un ampio ventaglio di candidati con caratteristiche diverse e che almeno in parte soddisfacessero il profilo ideale che abbiamo delineato. Questa eterogeneità rende difficile confrontare algoritmi di diversa natura, né lo scopo del lavoro è stilare una inverosimile classifica. Piuttosto, l'obiettivo è selezionare le tecniche che possono produrre un risultato di rilievo, definire degli indici che aiutino l'analista dei dati a separare i clustering soddisfacenti da quelli insignificanti, offrire delle procedure per unificare diversi clustering potenziali \cite{Strehl03}.\\

\section{Breve descrizione del lavoro}
L'esigenza principale di Neosperience è quella di disporre di uno strumento, il più possibile automatico, che possa valorizzare i dati grezzi dei propri clienti. Questi infatti possiedono dei grandi database di \textit{clienti finali} che, di per sé, non presentano nessun valore addizionale in quanto mancano di una semantica associata ad essi. Una delle aree più importanti del marketing è la \textit{segmentazione di mercato}, che si prefigge lo scopo di trovare regolarità (di esigenze e comportamenti d'acquisto) all'interno di un numeroso insieme di individui. È stato proprio questo l'obiettivo di Neosperience: dare un significato ai dati tratteggiando le diverse tipologie di clienti che ne emergono. Da un lato, la segmentazione è considerata un compito creativo basato sull'intuizione del dirigente aziendale nell'individuare aree potenziali di mercato, dall'altro necessita, sicuramente, di una metodologia precisa e scrupolosa che consente di determinare almeno i confini teorici entro i quali poter prendere delle decisioni ponderate. La cluster analysis viene in aiuto in questo senso, mettendo a disposizioni metodi automatici per identificare gruppi omogenei di individui secondo una definizione di similarità tra questi. 
Delineiamo ora quali sono stati i passi fondamentali del lavoro di tesi.

\subsubsection{Analisi della letteratura}
Come in ogni campo della scienza, per un determinato problema non esiste un'unica soluzione ma sempre è possibile scegliere o progettare una vasta gamma di strumenti che propongono dei risultati diversi e con una qualità che varia a seconda delle loro caratteristiche. Ciò porta a vantaggi immediati: a causa della varietà delle basi di dati e della loro mutevolezza nel tempo, è necessario affidarsi ad un ventaglio di soluzioni in modo tale da poter scegliere quella che, per un certo input, risulti essere ottimale. Di conseguenza, l'analisi della letteratura non è stata focalizzata sulla ricerca di un solo algoritmo capace di risolvere qualsiasi problema; si è cercato, infatti, di avvalersi di un insieme di algoritmi in cui ognuno potesse intervenire sui dati in input a seconda dei propri meriti: come, ad esempio, la capacità di gestire grafi completi o con componenti isolate, di affrontare dataset con un elevato numero di attributi o con valori mancanti, oppure di essere scalabile in funzione della dimensione dell'input e delle performance dell'hardware.\\
Infine, oltre agli algoritmi di clustering, sono state studiate metodologie di \textit{data preprocessing} e pacchetti software (R\footnote{\url{http://www.r-project.org/}}, Matlab\footnote{\url{http://www.mathworks.it/}}, Gremlin\footnote{\url{https://github.com/tinkerpop/gremlin/wiki}}) per la raccolta e l'analisi di dati.

\subsubsection{Raccolta e analisi dati}
Col fine di ottenere una base di dati su cui eseguire e valutare gli algoritmi selezionati abbiamo sviluppato un' applicazione PHP che recupera il profilo Facebook di un utente e dei suoi amici. In particolare, per ogni utente dell'applicazione, si raccolgono le informazioni di profilo (genere, città natale, città di residenza, istruzione), le preferenze (le pagine su cui si è cliccato \textit{like}), i legami d'amicizia e, di nuovo, informazioni di profilo e preferenze degli amici. In questa maniera, da ogni utente si ricava una rete personale (\textit{ego-network}) che, composta con le altre, ha permesso la creazione di un unico grafo con attributi.\\
% La richiesta fondamentale di Neosperience è che il framework potesse essere in grado di gestire un generico dataset in input in quanto, al momento della commissione, non possedevano ancora una base di dati completa. Ciò ha introdotto un importante ordine di problemi: da un lato c'è stata la necessità di raccogliere un insieme di utenti di test su cui eseguire e valutare gli algoritmi, dall'altro si è cercato di creare, da questi, un ventaglio di sottoinsiemi di input che potessero modellare il generico dataset in ingresso.
Il primo passo è stato quello di ripulire il grafo da utenti con un'alta percentuale di informazioni mancanti (sia relazionali che di profilo); successivamente si è svolta, tramite considerazioni statistiche, l'imputazione dei \textit{missing value} e la selezione di attributi rilevanti per disporre di informazioni complete e significative su ogni utente. Infine, è stato sviluppato un algoritmo di campionamento che, da un lato, riuscisse a creare sottografi significativi dal punto di vista topologico e degli attributi, dall'altro, che presentasse una forte componente casuale per modellare la varietà dei dati in input che potrebbero essere usati in seguito. Conclusa questa fase abbiamo quindi ottenuto un insieme di sottografi su cui è possibile eseguire e valutare gli algoritmi di clustering. Infine, ogni sottografo è stato etichettato con una serie di misurazioni topologiche che lo \virgolette{identificano}.

\subsubsection{Valutazione e confronto degli algoritmi}
Ogni algoritmo presenta una serie di parametri di partenza che devono essere impostati accuratamente per elevare la qualità dei risultati: è quasi sempre possibile specificare, ad esempio, il numero di cluster da identificare, oppure il numero massimo di attributi che caratterizzeranno un cluster. %Al fine di disporre di uno strumento il più possibile automatico e che possa essere usato con relativa semplicità, uno degli obiettivi di questa fase è stato calibrare il valore dei parametri a seconda del dataset in ingresso. 
Tramite la misurazione dei tempi d'esecuzione e degli \textit{indici di validità} --- stime che valutano quanto sia appropriato un certo clustering in relazione alle caratteristiche degli individui --- si è raggiunto l'obiettivo \virgolette{zero} del lavoro: capire qual è la miglior parametrizzazione di un algoritmo su un dataset in input. La fase successiva è stata quella di confrontare e combinare questi risultati per ogni algoritmo a disposizione: in questo modo abbiamo raccolto tutte le informazioni necessarie per condurre delle analisi più complesse.
Fissati i dati sotto esame, è possibile, infatti, rispondere a domande come le seguenti:
\begin{itemize}
\item Come varia la stessa prestazione P (ad esempio il tempo d'esecuzione) di due algoritmi al variare dei loro parametri? È possibile decidere quale dei due algoritmi è mediamente il migliore in relazione a P?
\item Valutando l'andamento dei vari indici prestazionali, esiste un trend che permette di decidere qual è l'algoritmo, in generale, più efficace?
% \item Senza testare tutti gli algoritmi a disposizione, è possibile suggerire quale potrebbe essere il miglior algoritmo per un certo dataset?
\end{itemize}
%In particolare la risposta all'ultimo punto si ottiene tramite questo ragionamento: come già affermato, ogni dataset di test è stato etichettato da misurazioni topologiche. Quindi, calcolando le stesse misurazioni su un dataset in input $ D_{new} $ è possibile individuare il dataset di test $ D_{test} $ ad esso più simile e proporre per $ D_{new} $ le valutazioni ottenute su $ D_{test} $. In questo modo si ottiene una stima, da valutare attentamente, su come potrebbero comportarsi gli algoritmi sul nuovo input, senza eseguire effettivamente tutte le valutazioni del caso. Questa particolarità è dettata dall'esigenza di avere uno strumento agile che possa per lo meno identificare quali sono gli algoritmi che, probabilmente, avranno delle basse prestazioni. Ad esempio, se $ D_{new} $ ha un numero elevato di nodi e si sa che un certo algoritmo su un dataset di test con lo stesso numero di nodi ha un tempo d'esecuzione intollerabile, allora il framework proporrà di evitare quell'algoritmo, almeno per le prime fasi di analisi di $ D_{new} $.\\
Infine, abbiamo sviluppato un tool per attribuire un significato \virgolette{descrittivo} al risultato di clustering delineando le caratteristiche di ogni cluster, tramite media, varianza e lista di valori degli attributi.\\
Prima di concludere, c'è da specificare che quando si parla di algoritmo o soluzione \virgolette{migliore} lo si fa si sempre in relazione ad una indice di validità che offre solamente una stima della \virgolette{qualità} del risultato. Questa stima non vuole in nessun modo sostituire l'importante collaborazione tra l'analista dei dati e l'esperto di mercato: entrambi, in maniera sinergica, dovranno valutare le soluzioni e prendere spunto da esse per la definizione finale dei segmenti di mercato.

\section{Struttura della tesi}
La tesi è strutturata nel modo seguente.\\
Nel \autoref{capitolo2} analizzeremo la letteratura accademica sul clustering, con un particolare accento sul problema della dimensionalità. Inoltre, discuteremo la problematica della pulizia dei dati e della valutazione dei dataset e dei risultati del clustering.\\
Nel \autoref{capitolo3} descriveremo il framework Engage di Neosperience, per il quale questo lavoro è stato svolto, e presenteremo la nostra soluzione, dalla raccolta dei dati fino alla esecuzione degli algoritmi \\
Nel \autoref{capitolo4} discutiamo la procedura ed i risultati dell'analisi sperimentale.\\
Nel \autoref{capitolo5} esponiamo le conclusioni e i possibili sviluppi del lavoro.\\
