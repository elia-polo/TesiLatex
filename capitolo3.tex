\chapter{Architettura del framework}
\label{capitolo3}
\thispagestyle{empty}
In questo capitolo descriviamo l'architettura del framework sviluppato durante il lavoro di tesi. Inizialmente presentiamo la piattaforma \textit{Neosperience} specificando dove si colloca il nostro lavoro al suo interno e poi mostriamo quali sono i moduli progettati e le soluzioni proposte per ogni fase della data analysis. Vengono affrontate quindi le sfide di raccolta, preparazione e conversione dei dati, fino all’esecuzione degli algoritmi di clustering. Infine, presentiamo un \textit{workflow} modulare dell'intera architettura, utile per una visione d'insieme degli strumenti disponibili.
\section{Neosperience RTP}
Neosperience è una azienda che sviluppa strumenti software per la \textit{Digital Customer Experience}. In un mercato globale \mbox{iper-competitivo} e \mbox{iper-connesso}, per contrastare la crescente dispersione degli utenti, un marchio deve stabilire un legame individuale con il cliente. Customer experience descrive la percezione che il cliente matura---razionalmente ed emotivamente---della relazione con il marchio. Di conseguenza, la gestione dell'\textit{esperienza cliente} \`e la pianificazione e la risposta alle interazioni con il consumatore, al fine di raggiungerne ed eccederne le aspettative ed in questo modo aumentare la soddisfazione, la lealt\`a e il sostegno del cliente al marchio\footnote{``The practice of designing and reacting to customer interactions to meet or exceed customer expectations and, thus, increase customer satisfaction, loyalty and advocacy'' - Gartner}: in una parola, coinvolgimento o \textit{engagement}. L'esperienza cliente \textit{digitale} apre nuove vie di contatto con le persone: non solo portali internet, ma applicazioni per social network e dispositivi mobili, tecnologie 3D e negozi virtuali, realtà aumentata e \textit{gamification}. Il ciclo vitale di un cliente rispetto ad un marchio inizia con la scoperta di un prodotto o un servizio; dopo questo primo contatto, seguono diversi \textit{moment of truth}, le occasioni in cui il cliente interagisce con l'azienda e si forma una opinione, consapevole ed inconscia. Il mantenimento del cliente dipende dalla capacit\`a del marchio di influire positivamente in ciascuno di questi istanti: la valutazione del prodotto, la decisione dell'acquisto e l'esperienza d'uso. Questo non \`e solo qualit\`a del servizio, ma arrivare ad una comprensione cos\`i profonda dell'utente da poter offrire una esperienza---contenuti e benefici---tanto personalizzata ed appagante da indurlo non solo a restare leale al marchio ma a convincere altri ad avvicinarsi. Per raggiungere questo grado di conoscenza \`e necessario estrarre indizi da ogni punto di contatto con il cliente, sfruttando l'immensa mole di informazione nella Rete.\\
Recentemente, Neosperience ha aggiunto alla propria piattaforma di customer experience il modulo Right-Time Personalization (RTP), per offrire a ciascun cliente contenuti personalizzati, scelti e presentati in base ai suoi interessi e intenzioni di acquisto. Questo modulo potrebbe essere integrato in un negozio virtuale o una galleria di contenuti, laddove ad ogni prodotto può essere associato un target ovvero il destinatario più appropriato, definito mediante variabili demografiche (età, genere, luogo di nascita, istruzione), interessi e contesto (ad esempio la distanza dal negozio fisico). La piattaforma Neosperience, sottostante all'applicazione, recupera i dati personali dell'utente (da Facebook, Twitter e Foursquare) delineando quindi un profilo unico che lo contraddistingue. Partendo da questo profilo, RTP misura, per ogni oggetto nel catalogo, la somiglianza tra il target dell'oggetto e l'utente dell'applicazione, in modo tale che i prodotti vengano riordinati e visualizzati in ordine di significatività.\\
Il nostro lavoro è stato motivato dall'esigenza di Neosperience di valorizzare i dati degli utenti raccolti tramite il proprio modulo RTP. Questi dati infatti racchiudono il potenziale per un diverso approccio alla segmentazione della clientela, facendo leva su informazioni variegate, semi-strutturate e non strutturate, ma nuove e complementari rispetto alle tradizionali categorie sociali e geografiche. L'opportunità offerta da questi dati è la conoscenza puntuale degli interessi, delle opinioni, del \textit{mood} di ciascun cliente, e la possibilità di rispondervi in tempo reale.
%, limitati soltanto dalla capacità di elaborare e assimilare questa informazione.
\subsection{Il modello dei dati}
\label{section_modello_dati}
Nel progetto originale, il modulo RTP è alimentato da tre sorgenti di dati: Facebook, Twitter e Foursquare.\\
\textbf{Facebook}, con oltre un miliardo di utenti mensili, è il più diffuso e noto social network al mondo. In Facebook, un utente può pubblicare la propria storia personale, interessi, esperienze, foto, lavoro e perfino stati d'animo. D'altronde, il fulcro dei social network è tessere relazioni, e Facebook non fa eccezione: ogni utente può stringere amicizia con altri utenti, condividere con essi contenuti, conversare, giocare, organizzare eventi ed essere informato di ogni cambiamento nella propria rete sociale. Fra le tre sorgenti, Facebook contribuisce la porzione dominante di informazione: il profilo utente, i like, le amicizie. Come riportato in \autoref{fig:modello_dati}, la frazione osservabile del profilo include il nome, il genere, la data e il luogo di nascita, la città di residenza, l'istruzione (scuola superiore, università e formazione specialistica), lo stato sentimentale e l'orientamento sessuale. Al pari di ogni altro nodo del grafo sociale di Facebook, i like sono identificati univocamente e organizzati in oltre 200 macrocategorie, talvolta ulteriormente frazionate in sottogruppi: questo ha permesso di ridurre notevolmente la dimensionalità del problema.

\textbf{Twitter} è un social network e una piattaforma di microblogging. Al cuore di Twitter vi sono i \textit{tweet}, brevi messaggi in 140 caratteri che un utente pubblica e la piattaforma notifica a tutti i suoi contatti, chiamati \textit{follower}. Un utente riceve i tweet delle persone che segue sulla propria bacheca o \textit{home timeline}, da cui può rispondere, inserendosi nel flusso di tweet esistente, o ripubblicare (\textit{retweet}), propagando il tweet ai propri follower. Per organizzare tematicamente le conversazioni, i tweet sono spesso etichettati con un \textit{hashtag}, una parola chiave preceduta da un cancelletto, o \textit{hash} in inglese. Questa convenzione nacque spontaneamente tra gli utenti di Twitter e fu raccolta ed integrata nella piattaforma, che oggi pubblica in tempo reale la lista dei \textit{trending topics}, le parole o argomenti di discussione che compaiono più frequentemente negli ultimi tweet.

\textbf{Foursquare} \`e tanto un social network focalizzato sulla posizione dell'utente quanto un gioco, il cui successo è indubitabilmente connesso al dilagare di dispositivi mobili e intelligenti, sempre più comunemente provvisti di una antenna GPS. Foursquare non solo rende pubblica la posizione dell'utente, permettendo di trovare gli altri utenti nella medesima zona, ma invita gli iscritti a registrarsi (\textit{check in}) ai luoghi di incontro che visitano---un negozio, un bar, un museo---e attribuisce punti e distintivi (\textit{badges}) per premiare la frequenza con cui vi si accede o la scoperta di nuovi luoghi prima dei propri amici.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{pictures/modello_dati.pdf}
    \caption{Modello dei dati}
    \label{fig:modello_dati}
\end{figure}
\subsection{Problematiche di integrazione fra le sorgenti}
L'obiettivo finale è definire un modello unico dei dati, come esposto in \autoref{fig:modello_dati}, per integrare tre sorgenti eterogenee.\\
Una evidente differenza fra Twitter e Facebook è la natura delle interazioni fra gli utenti: Twitter realizza un modello asimmetrico di relazione, in cui la discrepanza tra il numero di persone che ci seguono e quelle che seguiamo determina la reputazione dell'individuo all'interno della comunità; viceversa in Facebook, una volta stretta amicizia, il rapporto tra due amici è paritario e simmetrico. Questa peculiarità ha sensibili implicazioni sul modello dei dati, giacché il grafo degli utenti deve adesso contenere archi orientati.\\
Una ulteriore peculiarità risiede nel confronto tra \textit{like} e \textit{hashtag}. Mentre la relazione tra un utente Facebook e un like è binaria---o l'utente ha espresso il proprio gradimento di un oggetto o non lo ha fatto---nel caso di Twitter un utente può non adoperare mai un hashtag o usarlo con frequenza. Anche in questo caso il modello dei dati richiede una estensione, tramite l'introduzione di pesi sugli archi utente-hashtag. A differenza del caso precedente, dove un arco non orientato può essere equivalentemente modellato da una coppia di archi orientati, conciliare gli archi privi di peso di Facebook con quelli pesati di Twitter richiede una normalizzazione arbitraria dei pesi. In aggiunta, il significato di un hashtag è tutt'altro che analogo ad un like, poiché, avendo fissato l'argomento, un tweet può esprimere tanto apprezzamento quanto avversione, discussione o ``inutile chiacchiera''\footnote{\url{http://web.archive.org/web/20110715062407/www.pearanalytics.com/blog/wp-content/uploads/2010/05/Twitter-Study-August-2009.pdf}}. Per amalgamare semanticamente i tweet ai like sarebbe necessario discriminare i tweet in semplici categorie---\textit{like} e \textit{dislike}---che tuttavia è un problema di elaborazione del linguaggio naturale e clustering in sé. Infine, la vita media di un hashtag è di gran lunga più breve rispetto ad un like. In termini assoluti, mentre un like è sostenuto da un contenuto---una pagina o una realtà esterna a Facebook---la creazione di un hashtag richiede al più uno sforzo di immaginazione; la controparte della proliferazione degli hashtag è proprio la loro volatilità. In termini relativi, il carattere episodico e conversevole del tweet richiede tempi di elaborazione ed azione più stringenti, in quanto l'associazione e il coinvolgimento dell'utente rispetto ad un hashtag può rapidamente consumarsi. In sintesi, l'analisi dei tweet richiede algoritmi tolleranti di una elevatissima dimensionalità, coerente con l'elaborazione del linguaggio naturale, possibilmente basati su grafi e che garantiscano una bassa complessità temporale anche a scapito della qualità.\\
In ultimo, Foursquare è un servizio unico nel suo genere e, contribuendo informazione nuova e non ridondante, non richiede sforzi addizionali di assimilazione nel modello.\\
Una volta definiti i criteri di integrazione, l'ultimo passo sarebbe la \textit{entity resolution} ovvero l'identificazione dei profili che, benché apparentemente scorrelati e provenienti da sorgenti diverse, appartengono al medesimo individuo reale.\\
La quantità di dati a disposizione di Neosperience si è rivelata, comunque, molto al di sotto del volume prospettato a regime, sul quale gli algoritmi e le tecniche di data mining avrebbero dovuto essere messe alla prova. Pertanto, il lavoro si è articolato nei seguenti passi:
\begin{itemize}
\item \hyperref[raccolta_dati]{raccolta dei dati}
\item \hyperref[selezione_algoritmi]{selezione degli algoritmi}
\item \hyperref[preparazione_dati]{preparazione dei dati}
\item \hyperref[esecuzione_algoritmi]{esecuzione degli algoritmi}
\item \hyperref[capitolo4]{analisi dei risultati}
\end{itemize}
\section{Raccolta dei dati}
\label{raccolta_dati}
Le \mbox{\textit{Graph API}}\footnote{\url{https://developers.facebook.com/docs/graph-api}} sono lo strumento principale che permette di leggere e scrivere sul grafo sociale di Facebook. Il grafo sociale è una rappresentazione delle informazioni presenti nel social network composto da:
\begin{itemize}
\item \textbf{nodi}: entità di vario tipo come utenti, foto, eventi, pagine o commenti, etichettati da un identificatore unico
\item \textbf{archi}: connessioni esistenti tra le entità: relazioni d'amicizia tra utenti, foto in una pagina, commento su certo un evento, etc.
\item \textbf{campi}: attributi delle entità come il compleanno di un utente o il nome di una pagina.
\end{itemize}
La maggior parte delle richieste fatte con le Graph API necessita di un \textit{access token}\footnote{\url{www.oauth.org}}, che consente un accesso temporaneo e sicuro alle informazioni di profilo di un utente. In particolare, l'access token deve essere composto con una serie di permessi a seconda di quale attributo si vuole leggere o scrivere: ad esempio, se si è interessati al nome, al genere e agli amici di un certo individuo, esso deve considerare, simultaneamente, i permessi \texttt{user\_about\_me}, \texttt{user\_gender} e \texttt{user\_friends}.\\
Per recuperare queste informazioni è sufficiente una chiamata al metodo \texttt{HTTP GET}:
\begin{lstlisting}[basicstyle=\normalfont\ttfamily\scriptsize,backgroundcolor=\color{background}]
GET https://graph.facebook.com/me?fields=name,gender,friends
\end{lstlisting}
che restituisce il seguente file JSON\footnote{JavaScript Object Notation, \url{http://www.json.org}}:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "name": "Nicola P.", 
  "gender": "male",
  "friends": {
    "data": [
      { "name": "Linda",  "id": "987654321" },
      {	"name": "Matteo", "id": "123123123" }
    ]
   }
}
\end{lstlisting}

\subsection{Applicazione per la raccolta dei dati}
Mediante l'SDK per PHP\footnote{\url{https://developers.facebook.com/docs/reference/php/4.0.0}} abbiamo creato un'applicazione che ha ci permesso di recuperare da un utente le seguenti tipologie d'informazione: \textit{profilo} (data di nascita, genere, orientamento sessuale, stato sentimentale, città natale, città di residenza  e istruzione), \textit{like} (tutte le pagine su cui si è indicato \textit{like}) e \textit{amici} (profilo e like degli amici).\\
L'applicazione è stata utilizzata da 87 utenti (che abbiamo indicato con il termine \textit{DAU} o \textit{Direct User Application}) ottenendo, in totale, circa 25 mila profili Facebook in formato JSON.\\
Il passo successivo è stato quello di unificare tutti i profili in un unico file GML\footnote{Il formato GML (Graph Modelling Language) permette la rappresentazione dettagliata di un grafo con attributi}. Dal file JSON di ogni DAU sono stati recuperati gli identificatori degli amici e quindi i file JSON ad essi corrispondenti: in questa maniera è stato possibile costruire la rete personale del DAU (o \textit{ego-network}) arricchita dalle informazioni di profilo e dai like di ogni nodo. La composizione incrementale di queste reti ha portato alla creazione del grafo finale.

%\subsubsection{Pulizia preliminare dei nodi}
%Il grafo è stato ripulito da un insieme di nodi problematici sotto due aspetti. Un utente, infatti, viene escluso se: (1) il numero di attributi di profilo mancanti è maggiore della metà, e (2) se presenta un valore mancante per un certo attributo e la percentuale dei suoi vicini che, per lo stesso attributo hanno valore mancante, è maggiore del cinquanta percento. Il primo punto è di immediata comprensione, il secondo è giustificato dal fatto che l'inferenza di un certo attributo di un utente è basata sui valori che quell'attributo possiede nei vicini dell'utente stesso: se più della metà dei vicini hanno un valore mancante allora è lecito pensare che l'inferenza può addurre risultati molto approssimativi.

\subsection{Creazione dei sottografi}
\label{sampler}
Come già accennato in precedenza, abbiamo sviluppato uno strumento\footnote{Il linguaggio utilizzato per la creazione di questo tool è Gremlin (\url{https://github.com/tinkerpop/gremlin/wiki}) che permette di comporre query su grafi} per il campionamento di grafi che, da un lato, riuscisse a creare sottografi significativi dal punto di vista topologico e degli attributi; dall'altro, che presentasse una forte componente casuale per modellare la varietà dei dati in input che potrebbero essere utilizzati successivamente.\\
Il procedimento adottato è il seguente:
\begin{enumerate}
\item si sceglie casualmente un DAU $ X $ non ancora considerato
\item si recuperano i vicini di $ X $ non ancora considerati
\item con un certa probabilità $ p $ si sceglie un vicino $ Y $ di $ X $; altrimenti si torna a 1, scegliendo casualmente un altro DAU
\item se si è scelto il vicino $ Y $ di $ X $ si scala la probabilità $ p $ di un fattore $ s < 1 $, si pone $ X = Y $ e si torna ad 1.
\end{enumerate}
Questo procedimento viene ripetuto fino a quando non si raggiunge una certa percentuale $ \pi $ di utenti considerati.\\
Un DAU è un nodo ricco di informazioni, quindi è fondamentale che esso venga scelto come punto di partenza del campionamento. Con una probabilità che scala progressivamente, si considerano i suoi vicini e i vicini dei vicini, costruendo così una rete collegata al DAU iniziale. Successivamente, scegliendo un nuovo DAU (cioè partendo di nuovo dal punto 1) si ha la possibilità di costruire una rete ulteriore che, a seconda dei casi, sarà collegata o meno alla prima. Questo procedimento iterativo porta, infine, alla selezione di un insieme di nodi su cui è possibile indurre il grafo campionato.\\
Variando i tre parametri ($ p $, $ s $ e $ \pi $) siamo stati in grado di generare una vasta gamma di sottografi che presentano caratteristiche topologiche e di attributi molto variegate. 

\section{Selezione degli algoritmi}
\label{selezione_algoritmi}
In considerazione della varietà dei dataset su cui avremmo operato, abbiamo selezionato una vasta gamma di algoritmi, cercando di differenziarli per modello dei dati utilizzato, tecnica di clustering (\ref{sec:classificazione_algoritmi}), approccio alla dimensionalità (\ref{subsec:subspace_clustering}) e prestazioni.\\
Limitatamente al modello dei dati, gli algoritmi possono essere raggruppati in tre categorie:
\begin{itemize}
\item clustering su attributi: NetClus \cite{netclus}, MOC \cite{moc}, LAC \cite{lac}, ORCLUS \cite{orclus}
\item clustering su grafo: Fast Unfolding \cite{blondel2008fuc}
\item clustering su attributi e grafo: LatentNet \cite{handcock07}, CESNA \cite{cesna}, Inc-Cluster \cite{inc_cluster}, DB-CSC \cite{db_csc}, BAGC \cite{bagc}
\end{itemize}
\subsection{Descrizione degli algoritmi}
Di seguito mostreremo, per il lettore interessato, la descrizione teorica dei vari algoritmi a nostra disposizione.
\begin{description}
\item[NetClus] è un algoritmo per il clustering di reti composte da entità eterogenee e conformazione a stella (\textit{star network schema}). In assenza di connessioni tra utenti, il modello dei dati descritto in \autoref{fig:modello_dati} riflette esattamente questo scenario: al centro della stella vi è l'utente, da cui scaturiscono archi verso gli attributi di profilo, like, hashtag e check-in, ognuno dei quali forma un nuovo tipo di entità. Il pregio di NetClus è che non si limita ad attribuire una etichetta di cluster a ciascun utente, ma produce un ordinamento dei valori di ciascuna entità in ognuno dei cluster ottenuti, semplificando notevolmente l'interpretazione dei risultati.
%Ad esempio, per ogni cluster sarebbero evidenziati i valori di età, luogo di nascita, grado di istruzione, like, check-in\dots più rappresentativi per la popolazione raccolta nel cluster.
In secondo luogo, l'approccio di NetClus riduce fortemente la dimensionalità del problema rispetto agli altri algoritmi su attributi, dove ogni nuovo like o hashtag aggiunge una dimensione al dataset. Un ultimo punto di forza è che non richiede la definizione di una misura di similarità, ma costruisce un modello probabilistico generativo sotto l'ipotesi che la rete esibisca assortatività e \textit{preferential attachment}, come nel caso delle reti sociali. Dato il numero di cluster, l'idea generale di NetClus si compone dei seguenti passi: generare una partizione iniziale degli oggetti ed indurre i \textit{net-cluster} $\{C_{k}^{0}\}_{k=1}^{K}$ dalla rete originale sulla base di queste partizioni; costruire, per ciascun net-cluster, un modello probabilistico generativo $\{P(x|C_{k}^{t})\}_{k=1}^{K}$; quindi calcolare la probabilità a posteriori $P(C_{k}^{t}|x)$ per ciascun oggetto e ricalcolare l'assegnamento degli oggetti ai cluster. I due passi precedenti sono reiterati finché i cluster non raggiungono una condizione stazionaria. Al termine, dalla probabilità a posteriori si ottiene un ordinamento degli oggetti all'interno dei cluster di appartenenza. L'algoritmo ha complessità lineare nel numero di archi, ovvero per reti sparse è approssimativamente lineare nel numero di nodi.

\item[MOC] è un algoritmo probabilistico generativo per l'identificazione di cluster non disgiunti o \textit{overlapping}. L'algoritmo generalizza e semplifica un precedente studio sul clustering sovrapposto di espressioni genetiche \cite{SegalBK03}; con una scelta oculata del modello di probabilità e definizione di distanza---la famiglia esponenziale e la divergenza di Bregman rispettivamente---gli autori sostengono di poter applicare la tecnica a dati sparsi e con numerose dimensioni, laddove il modello gaussiano e la distanza euclidea, usati più comunemente, sono noti offrire scarsi risultati. A testimonianza della formulazione originale, i dati o matrice di espressione $X$ sono modellati come la manifestazione di due fattori, l'appartenenza $M$ e l'attivazione $A$. L'appartenenza descrive quanto i cluster nascosti nei dati si manifestano in ciascun individuo; dato che il modello è \textit{overlapping}, ogni individuo può appartenere simultaneamente a più cluster. L'attivazione indica il peso di ciascuna dimensione dei dati in ognuno dei cluster, ossia quali caratteristiche di un individuo sono determinate più significativamente dall'appartenenza al cluster. I dati sono quindi ricostruiti come il prodotto $X'=M \times A$ tra la matrice di appartenenza e la matrice di attivazione. Seguendo questa intuizione, l'algoritmo costruisce una stima iterativa di $M$ e $A$. Il vettore di appartenenza $M_i$ di ciascun individuo è ottenuto con un approccio \textit{greedy}, volto a minimizzare la distanza tra l'individuo e la sua approssimazione $M_i \times A$: fissata la matrice di attivazione $A$, MOC `accende' con ogni iterazione un nuovo cluster, fino a quando per nessuna scelta è possibile ridurre ulteriormente la funzione obiettivo. La complessità temporale di questa fase è $O(k^3)$ nel numero di cluster $k$. La matrice di attivazione può essere costruita dalla minimizzazione della medesima funzione obiettivo, fissata $M$; seppure sia definita una diversa formula per ogni modello nella famiglia esponenziale e relativa divergenza, la complessità temporale di questa fase può essere stimata in $O(d^3)$.
\item[LAC] è un algoritmo \textit{centroid-based} di \textit{subspace clustering}. Come discusso in \ref{subsec:metodi_partitivi}, le prestazioni degli algoritmi basati sulla distanza euclidea degradano rapidamente al crescere del numero di dimensioni. Per ovviare a questo noto limite, LAC trasforma lo spazio in cui ciascun cluster è immerso, associando ad ogni cluster un vettore di pesi affinché maggiore importanza sia conferita alle dimensioni lungo le quali c'è aggregazione dei punti, all'interno del cluster. I pesi indicano quindi il grado di partecipazione di ciascun attributo al cluster; se i punti sono fortemente raggruppati rispetto a quell'attributo, il peso sarà alto, e basso altrimenti. Questi coefficienti sono appresi iterativamente con la ottimizzazione della somma degli errori quadrati (SSE), ovvero, per ciascun cluster, le distanze tra il centroide di riferimento e tutti i punti appartenenti al cluster. In assenza di contromisure, tutto il peso sarebbe concentrato sulla dimensione a minima varianza e l'algoritmo scoprirebbe unicamente cluster monodimensionali; per controllare la dimensione dello sottospazio in cui i cluster sono valutati, l'utente deve fissare un termine di penalità che è inserito nella funzione obiettivo. Sebbene l'esistenza di parametri astratti come questo sia un aspetto in genere indesiderabile, gli autori hanno definito una metodologia \textit{ensemble} per la determinazione del valore ottimo attraverso esecuzioni multiple della procedura di clustering. Ogni iterazione dell'algoritmo ha complessità $O(kDN)$, dove $k$ è il numero di cluster, $N$ il numero di elementi del dataset e $D$ il numero di dimensioni; il tempo di esecuzione è quindi governato dalla rapidità con cui l'algoritmo converge ad una soluzione.
\item[ORCLUS] è un algoritmo di clustering proiettato (\textit{projected clustering}) basato su \textit{K}-medoids. Dati due parametri specificati dall'utente, il numero di gruppi e il numero di dimensioni dei gruppi, ORCLUS identifica un insieme di cluster arbitrariamente allineati---non solo paralleli agli assi, ma ottenuti dalla combinazione lineare delle dimensioni originali---ciascuno in un sottospazio della dimensione specificata. ORCLUS è molto simile all'algoritmo k-means, tranne per il fatto che, invece di misurare le distanze nello spazio completo, queste sono calcolate nei sottospazi di ciascun cluster. Preliminarmente, ORCLUS sceglie casualmente dei `semi' o potenziali medoidi. L'algoritmo richiede quindi un processo iterativo: in ogni iterazione ciascun cluster è progressivamente raffinato, rimuovendo le dimensioni prive di aggregazione e riducendo il numero di cluster dall'accorpamento di quelli più simili. I sottospazi sono formati tramite PCA, ovvero calcolando la matrice di covarianza per ciascun cluster e selezionando gli autovettori con la minore diffusione nel sottospazio del cluster, ovvero quelli associati agli autovalori più piccoli. Quando due cluster sono vicini e hanno simili direzioni degli autovettori sono fusi assieme. A chiusura di ogni iterazione è eseguita la fase di assegnamento: ogni punto è associato al seme più vicino e i semi sono ricalcolati come i centroidi dei cluster appena formati. L'algoritmo è computazionalmente intensivo $O(d^3)$ nella dimensione dei dati, in conseguenza del calcolo della matrice di covarianza e degli autovettori di ciascun cluster. Oltretutto, richiede la specificazione non solo del numero di cluster ma anche della dimensione del sottospazio in cui ciascun cluster è costruito, un parametro difficile da stimare correttamente a priori.
\item[LatentNet] è un algoritmo di clustering probabilistico, basato su un modello generativo. Partendo da un grafo di relazioni binarie, il modello assume che ogni nodo possieda una posizione non osservabile in uno spazio sociale \mbox{\textit{d}-dimensionale}, euclideo e latente. Con questa premessa, la presenza o l'assenza di una connessione tra due individui è indipendente da ogni altra informazione, se sono note le posizioni dei due individui nello spazio sociale. Questo modello esprime in maniera innata tanto la transitività quanto l'omofilia negli attributi osservati. Per introdurre la proprietà di clustering, si assume che le posizioni nello spazio latente siano estratte da un modello misto (\textit{mixture model}) di distribuzioni Gaussiane multivariate: ogni distribuzione è caratterizzata da una propria media e varianza, e rappresenta un diverso gruppo di individui o cluster. Per la stima delle posizioni nello spazio latente, gli autori propongono due metodi. Il primo è un metodo in due fasi: inizialmente calcola la stima a massima verosimiglianza del modello dello spazio latente---che spiega la transitività e l'omofilia---e determina le posizioni degli attori nello spazio sociale; la seconda fase stima i parametri del modello misto, basandosi sulle posizioni nello spazio latente ottenute nella prima fase. Il secondo metodo è pienamente Bayesiano e usa \textit{MCMC sampling}: il vantaggio è che stima simultaneamente le posizioni latenti e il modello di clustering, ma è computazionalmente più esigente.

\item [Fast Unfolding] è un algoritmo euristico per il clustering di grafi basato sull'ottimizzazione della modularità. Il metodo è diviso in due fasi che vengono ripetute ciclicamente. All'inizio, si considerano tante comunità quanti sono i nodi della rete. Successivamente, per ogni nodo $ i $ si considerano i suoi vicini $ j $ e si valuta il guadagno della modularità che si avrebbe se si spostasse il nodo dalla sua comunità alla comunità di un nodo $ j $: il nodo $ i $ viene effettivamente spostato in quella comunità che genera il massimo guadagno positivo. Se ciò non è possibile il nodo resta nella sua comunità. Questa prima fase viene ripetuta sequenzialmente per ogni nodo, finché non è possibile nessun ulteriore miglioramento. La seconda fase consiste nel costruire un nuovo grafo i cui nodi sono le comunità trovate durante la prima fase: gli archi di ogni nuovo \virgolette{nodo-comunità} sono l'unione degli archi dei nodi atomici che lo compongono. Al termine di questa seconda fase si può applicare di nuovo la prima, in un processo iterativo che termina quando non è più possibile spostare un nodo in una comunità diversa ed avere un guadagno positivo della modularità.
\item [Inc-Cluster] è un algoritmo di clustering per grafi con attributi. Per ogni coppia attributo-valore $ (a, v) $ del dataset viene aggiunto, nel grafo originale, un nodo $ n_{a,v} $ che sarà collegato a tutti i vertici che presentano il valore $ v $ per l'attributo $ a $. In questo modo viene creato un grafo \textit{aumentato} su cui è possibile definire una funzione di distanza (la \textit{random walk distance}) che tenga conto sia dell'informazione topologica sia del valore degli attributi dei nodi: infatti la funzione di distanza considera \textit{simili} due individui se ci sono numerosi percorsi che li collegano. Poiché nel grafo aumentato i percorsi tra i nodi sono possibili anche grazie alla presenza degli attributi che condividono e non solo per le relazioni d'amicizia, allora è facile immaginare in che senso la funzione distanza riesce a tener conto, contemporaneamente, della ego-network dei due nodi e della similarità tra i loro attributi. L'algoritmo utilizza una tecnica molto simile a quella del K-Medoids iterando l'assegnamento di un nodo ai vari \virgolette{centroidi} fino a alla convergenza della funzione obiettivo. Purtroppo il calcolo della similarità degli utenti è computazionalmente inefficiente, $ O(n^{2.807}) $ con $ n $ numero di nodi del grafo. Infine, Inc-Cluster richiede di specificare non solo il numero di cluster, ma anche probabilità di \textit{restart} e la lunghezza massima del percorsi nelle random walk.
\item [DB-CSC] è un algoritmo di clustering per grafi con attributi, basato sul concetto di densità: esso individua, infatti, regioni dense sia nel grafo che nello spazio degli attributi. In particolare, per ogni nodo $ v $ vengono definiti la $ \epsilon \mdash neighborhood $, cioè l'insieme dei vicini che distano al massimo $ \epsilon $ da $ v $, e la $ k \mdash neighborhood $ cioè l'insieme dei vicini che raggiungono $ v $ in  $ k $ passi al massimo. L'intersezione tra $ \epsilon \mdash neighborhood $ e $ k \mdash neighborhood $ rappresenta il \textit{vicinato locale} di un nodo. L'algoritmo definisce un cluster come un insieme di vertici $ O $ tali per cui, (1) ogni vertice $ v \in O $ ha un vicinato locale molto numeroso (cioè la cui cardinalità è maggiore di $ minPts $, definito dall'utente) e (2) per ogni coppia di punti in $ O $ esiste sempre un cammino che li collega.
Successivamente, viene costruito un grafo \textit{arricchito} in questa maniera: se due nodi del grafo originale hanno attributi simili e sono connessi da $ k $ archi al massimo (ovvero, se appartengono allo stesso vicinato locale) allora verranno collegati nel grafo arricchito. Dal grafo arricchito si calcolano tutti i $ (minPts-1) \mdash core$\footnote{Dato un grafo $ G $, un $ c \mdash core $  è un sottografo di G connesso e massimale in cui tutti i vertici hanno un grado maggiore o uguale a $ c $}, che rappresentano delle comunità potenziali: se il grafo arricchito contiene un unico $ (minPts-1) \mdash core $ che copre ogni nodo allora esso è un cluster valido secondo la definizione sopracitata; altrimenti bisogna ripetere questa considerazione per ognuno dei $ (minPts-1) \mdash core $ presenti nel grafo arricchito. Ricorsivamente si individuano i grafi arricchiti indotti dai nodi di ogni $ (minPts-1) \mdash core $ e su questi si calcolano, ancora una volta, i $ (minPts-1) \mdash core $: se ogni grafo arricchito è composto da un singolo $ (minPts-1) \mdash core $ che copre tutti i suoi nodi allora esso è una comunità.
Questa procedura viene, infine, generalizzata per individuare i cluster in ogni sottospazio degli attributi originali.

\item[CESNA] è un algoritmo probabilistico generativo per l'identificazione di comunità non disgiunte (\textit{overlapping}) su grafi con attributi. Per la costruzione del modello probabilistico vengono individuate due variabili osservabili, il grafo $ G $ e l'insieme di attributi $ X $, e una variabile latente da inferire, la funzione di \textit{membership} $ F $ che indica, per ogni nodo, qual è il grado di appartenenza ad una certa comunità. A questo punto è possibile definire due sotto-modelli generativi. Il \textit{modello degli archi della rete}, stabilisce qual è la probabilità che due nodi sono collegati tra loro in funzione della loro membership: essi avranno un'alta probabilità di essere collegati se condividono un elevato numero di comunità.\\
CESNA è l'unico algoritmo, tra quelli selezionati, che supporta i valori mancanti (\textit{missing value}): per ogni coppia attributo-valore $ (a, v) $ si crea una \textit{proprietà} $ k $ che viene associata a tutti e soli gli utenti che presentano il valore $ v $ per l'attributo $ a $. Quindi, il \textit{modello degli attributi dei nodi}, stabilisce qual è la probabilità che un certo nodo $ u $  esponga la proprietà $ k $. Successivamente, si introduce un'altra variabile latente (anch'essa da inferire): il peso $ W $ di una proprietà all'interno di un certo cluster. Se il nodo $ x $ partecipa a un certo numero di cluster per cui il peso di $ k $ è alto, allora è molto probabile che $ x $ possieda $ k $. Questo modello fa in modo che i nodi che appartengono alla stessa comunità condividono, probabilmente, le stesse proprietà.\\
Infine, viene costruito il modello \textit{combinato} che, in estrema sintesi, rappresenta la probabilità di avere un certo grafo $ G $ e un insieme di attributi $ X $ conoscendo la funzione di membership $ F $ e il set di pesi $ W $: $ P(G, X | F, W) $. Si suppone ora che questo modello generi i dati in input all'algoritmo e quindi, per spiegarli e giustificarli al meglio, si deve determinare la coppia $ \hat{F} $ e $ \hat{W} $ che massimizza la verosimiglianza $ log P(G,X | F,W) $. Se il grado di appartenenza di un nodo ad una certa comunità supera una soglia $ \delta $ allora il nodo appartiene a quella comunità; se un nodo presenta, per tutte le comunità, una membership minore di $ \delta $ allora viene considerato un outlier.\\
La complessità computazionale è lineare nel numero dei nodi $N$, degli archi $E$ e nel numero di attributi $K$: $ O (E + NK) $.

\item[BAGC] è un algoritmo probabilistico generativo per il clustering di grafi con attributi. Idealmente, BAGC attribuisce ad ogni possibile clustering sui dati una probabilità, cosicché il miglior clustering è semplicemente quello più verosimile, che massimizza la probabilità. Nonostante la semplicità concettuale di tale approccio, esplorare interamente lo spazio dei clustering possibili è impraticabile, e l'algoritmo propone una soluzione approssimata. Il primo passo è individuare una distribuzione parametrica che approssimi la probabilità congiunta tra il grafo considerato e i possibili clustering, modellando separatamente l'etichetta di cluster, gli attributi e gli archi per ciascun vertice. Avendo espresso il clustering come un problema di ottimizzazione---trovare il massimo a posteriori della divisione in cluster condizionatamente al grafo e agli attributi dati---devono essere caratterizzate le condizioni di stazionarietà, per potersi arrestare nella ricerca iterativa del valore ottimo. La soluzione approssimata è quindi raggiunta con un metodo variazionale.

\item[ROCK] è un tecnica gerarchica (bottom-up) per il clustering overlapping su attributi categorici. Per raggruppare gli oggetti del dataset, l'algoritmo non utilizza una semplice funzione di similarità (come l'indice Jaccard) poiché questa può rivelarsi poco conveniente in molte situazioni. Si pensi, in maniera intuitiva, al caso in cui due punti sono \textit{distanti} tra loro ma presentano un insieme di \virgolette{vicini} molto simili: una misura standard non considererà quest'ultimo aspetto e un algoritmo gerarchico non deciderà mai di raggruppare insieme i due punti. Per ovviare a questo problema, gli autori propongono una misura \textit{globale} che consideri non solo i punti in sé ma anche la distribuzione del loro \textit{vicinato}. Si definisce, quindi, il numero di \textit{link} tra due oggetti come il numero di vicini che hanno in comune;\footnote{In generale il numero di link è calcolato su due insiemi di punti ed indica, semplicemente, il numero di vicini comuni a tutti gli elementi degli insiemi} inoltre, due oggetti sono detti \textit{vicini} se la loro distanza non supera una certa soglia definita dall'utente. Come ogni algoritmo gerarchico bottom-up, ROCK inizia col considerare tanti cluster quanti sono gli oggetti del dataset per poi agglomerare, iterativamente, le coppie di cluster che massimizzano una misura di \textit{bontà} proporzionale al numero di link. La complessità computazione è $ O(m_{a}n^{2}) $ dove $ m_{a} $ è il numero medio dei vicini di un punto.

\end{description}
\section{Preparazione dei dati}
\label{preparazione_dati}
I profili utente si sono rivelati spesso inaccurati, disseminati di dati inverosimili e omissioni. Purtroppo, la cattiva qualità è una caratteristica dei dati reali, in cui il rumore, le imprecisioni della rilevazione e l'informazione mancante pregiudicano il successo del data mining. Per questo motivo, la qualità del clustering dipende tanto dalla adeguatezza della tecnica impiegata quanto dall'abilità dell'analista di preparare i dati per l'analisi.
\subsection{Rimozione degli outlier}
%Un outlier è un individuo che devia significativamente dal resto della popolazione; contrariamente al rumore, che esprime le naturali differenze tra le persone, l'anomalia è generata da un meccanismo e risponde a regole radicalmente diversi da quelle valide per gli altri individui.
La ricerca degli outlier è spesso un fine in sé---come l'identificazione delle intrusioni informatiche e delle frodi bancarie---ma in questo lavoro siamo interessati unicamente a prevenire le distorsioni numeriche che un outlier produce. Identificare un individuo atipico nel contesto delle reti sociali non è semplice, poiché la numerosità delle dimensioni rende insignificanti la distanza e la densità, che sono alla base delle procedure canoniche per l'identificazione delle anomalie (\autoref{subsubsec:outlier}). Non è neppure sufficiente basarsi sulla topologia del grafo delle amicizie, e dichiarare anomali gli individui debolmente connessi alla popolazione. Poiché abbiamo una visione parziale della rete, le connessioni che potrebbero contribuire ad integrare un individuo sono spesso inosservate, e questo limite si accentua ulteriormente nei campioni estratti dal dataset completo. Infine, anche i like possono essere fuorvianti, poiché risultano empiricamente molto sparsi: come discuteremo in \autoref{subsubsec:discretizzazione}, la popolarità media di un singolo like è estremamente bassa, meno di 5 utenti su oltre 25000. Ci siamo pertanto limitati a ricercare gli utenti che avessero un profilo platealmente abnorme. A tal fine, abbiamo misurato tramite la distanza di Mahalanobis la dissimilarità di ciascun individuo rispetto alla media della popolazione, trasformando così un problema multivariato---in considerazione del numero di attributi di ciascun individuo---in uno univariato. A questo punto è possibile applicare al vettore di distanze una tecnica arbitraria di identificazione degli outlier: test Chi-quadro, gaussiano, Local-Outlier-Factor etc. Partendo da questa selezione, abbiamo poi ispezionato individualmente i candidati e rimosso quelli che, a nostro giudizio, fossero effettivamente incompatibili con la popolazione.

\subsection{Inferire i valori mancanti}
A ciascun utente sono associati due tipi di proprietà: il profilo e i like. I like sono ovviamente variabili da un individuo all'altro, né ha senso trattare l'assenza di un \textit{mi piace} su un certo contenuto come valore mancante. In generale, parliamo quindi di valori mancanti solo per i dati di profilo, dove sono raccolte le informazioni biografiche dell'utente. Una ulteriore ragione per dare maggior peso all'assenza di informazione di profilo rispetto ai like è il ruolo che questi dati rivestiranno nell'analisi. Mentre i like sono fondamentali per produrre il clustering, gli attributi di profilo sono decisivi per interpretarlo. Una volta individuati i cluster, attraverso i like siamo in grado di capire quali interessi accomunano le persone; tuttavia, assumendo di non avere altre fonti di informazione, senza i dati del profilo non è possibile caratterizzare questi gruppi e descrivere le persone che ne fanno parte con le variabili della segmentazione tradizionale.\\
Tra gli algoritmi che abbiamo selezionato, solo CESNA \cite{cesna} tollera i valori mancanti; per tutti gli altri è stato necessario produrre una versione completa dei dati. In questa fase abbiamo fatto ricorso a tre soluzioni: rimuovere l'individuo con valori mancanti (la riga del dataset), rimuovere integralmente l'attributo (la colonna del dataset) o inferire il valore laddove assente. La prima soluzione è consigliabile quando l'individuo è così povero di informazione da non produrre alcun beneficio all'analisi. La seconda soluzione, eliminare integralmente una dimensione, è necessaria quando l'attributo è assente per una significativa porzione della popolazione: in questo scenario, rimuovere gli individui privi dell'attributo sarebbe un spreco di dati e non è nemmeno disponibile sufficiente informazione reale per inferire i valori mancanti. Dalle statistiche in \autoref{table:missing_values} si nota che gli attributi \textit{orientamento sessuale} e \textit{stato sentimentale} sono specificati da una estrema minoranza di utenti e sono stati pertanto rimossi dal dataset.
\begin{table}[h]
\resizebox{0.9\textwidth}{!}{\begin{minipage}{\textwidth}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | }
\multicolumn{1}{c}{genere}
 &  \multicolumn{1}{c}{età}
 & \multicolumn{1}{c}{\specialcell[t]{luogo di\\nascita}}
 & \multicolumn{1}{c}{\specialcell[t]{luogo di\\residenza}}
 & \multicolumn{1}{c}{educazione}
 & \multicolumn{1}{c}{\specialcell[t]{orientamento\\sessuale}}
 & \multicolumn{1}{c}{\specialcell[t]{stato\\sentimentale}}
 \tabularnewline
\cline{1-7}
0.78\% & 32.53\% & 28.02\% & 23.64\% & 24.29\% & 99.13\% & 99.84\%\tabularnewline
\cline{1-7}
\end{tabular}
\caption{Distribuzione dei valori mancanti per attributo}
\label{table:missing_values}
\end{minipage} }
\end{table}\\
In ultimo, gli attributi non specificati possono essere dedotti tramite imputazione, ma la procedura da adottare dipende dalla natura del valore mancante. Alla radice dell'informazione incompleta vi sono cause disparate: un profilo sostanzialmente inattivo, l'irrilevanza dell'informazione per l'utente o semplicemente il desiderio di privacy. Richiamando la terminologia introdotta in \autoref{subsec:missing_value}, inattività e irrilevanza ricadono nella tipologia MCAR, in quanto non vi è alcuna connessione tra il valore dell'attributo mancante e il fatto che non sia stato inserito. La riservatezza invece è generalmente MNAR, poiché dobbiamo conservativamente assumere che il non pubblicare una informazione possa dipendere dal valore dell'attributo per l'utente considerato. I valori mancanti nel nostro dataset sono quindi anche MNAR, e per essi non esiste una procedura univoca di inferenza. Abbiamo deciso pertanto di applicare una tecnica di imputazione ad hoc che faccia leva sull'omofilia nella rete, cioè sul fatto che gli amici di un individuo sono naturalmente selezionati tra i più simili ad esso all'interno della popolazione. Per ciascun individuo con valori mancanti, abbiamo considerato gli utenti a cui è connesso nel grafo delle amicizie e stimato da questi il valore dell'attributo mancante. A questo punto, la stima può avvenire con uno dei metodi definiti in letteratura: media, hot-deck, regressione etc. Tuttavia, quando un individuo non ha un vicinato sufficientemente ampio, o i vicini a loro volta non possiedono l'attributo mancante nel soggetto, l'imputazione non è affidabile. Pertanto, abbiamo deciso di escludere dal dataset un utente se: (1) il numero di attributi di profilo mancanti è maggiore della metà, e (2) se presenta un valore mancante per un certo attributo che, a sua volta, non è specificato da oltre la metà dei suoi vicini. Con le due fasi di rimozione dei dati impuri, la dimensione del dataset è drasticamente calata; abbiamo quantificato la perdita di informazione in \autoref{table:cleaned_dataset_statistics}.
\begin{table}[h]
\small
\centering
\begin{tabular}{l | c | c | c |}
 \multicolumn{1}{c}{} 
 & \multicolumn{1}{c}{nodi}
 & \multicolumn{1}{c}{archi \textit{friend}}
 & \multicolumn{1}{c}{archi \textit{like}}
 \tabularnewline
\cline{2-4}
originale (con MV) & 25853 & 975\hspace{2pt}382 & 4\hspace{2pt}391\hspace{2pt}494 \tabularnewline
\cline{2-4}
completo (senza MV) & 16336 & 710\hspace{2pt}711 & 3\hspace{2pt}131\hspace{2pt}019 \tabularnewline
\cline{1-4}
decremento assoluto & 9517 & 264\hspace{2pt}671 & 1\hspace{2pt}260\hspace{2pt}475 \tabularnewline
\cline{2-4}
decremento percentuale & 36.81\% & 27.14\% & 28.7\% \tabularnewline
\cline{2-4}
\end{tabular}
\caption{Perdita di informazione durante l'imputazione dei valori mancanti (MV)}
\label{table:cleaned_dataset_statistics}
\end{table}\\
%Missingness by attribute
%interested_in 0.9913379
%gender 0.007874636
%hometown 0.280652
%location 0.2375384
%birthday 0.3262855
%relationship_status 0.9983857
%education 0.2437594
%missingness for education
%HS			College		GR Sc		
%0.3231751 	0.4175526 	0.9296795
%True missing
%HS			College		GR Sc
%0.0794157	0.01204819	0
Si noti che anche la trasformazione degli attributi può produrre valori mancanti: nel passare dai nomi dei luoghi alle coordinate geografiche, non è stato talvolta possibile identificare città e paesi inseriti dagli utenti, per i quali si è dovuto inferire un valore categorico riconoscibile e convertire quest'ultimo.

\subsection{Selezione degli attributi}
Come già anticipato nei paragrafi precedenti (\autoref{data_prepro}), le tecniche di feature selection portano dei vantaggi immediati alla cluster analysis: oltre a velocizzare gli algoritmi di mining, a ridurre la dimensione del dataset e a facilitare la presentazione dei risultati, migliorano significativamente la qualità e la granularità del clustering. Infatti, la suddivisione dei dati in gruppi omogenei è molto più semplice e nitida in uno spazio con poche dimensioni; inoltre, di frequente, i cluster emergono solamente in un sottoinsieme di attributi restando \virgolette{nascosti} in uno spazio ad elevata dimensionalità (ad esempio, un gruppo di individui potrebbe essere simile per età, genere e luogo di nascita ma non per religione e città di residenza).\\
In questo lavoro abbiamo utilizzato il tool \texttt{FSFS}\footnote{Feature Selection using Feature Similarity, \url{http://cse.iitkgp.ac.in/~pabitra/paper/fsfs.tar.gz}} che permette di identificare gli attributi più significativi all'interno di un dataset in input. È una tecnica non supervisionata che si basa sul concetto di similarità tra le feature: queste, infatti, vengono raggruppate in vari cluster omogenei per poi selezionare, da ogni cluster, l'attributo più significativo. L'algoritmo richiede all'utente di specificare il numero di proprietà $ h $ da rimuovere: è necessario, quindi, condurre degli esperimenti per la regolazione di questo parametro. Tramite la definizione di \textit{entropia di rappresentazione} \cite{Mitra2002} è possibile capire quando un certo sottoinsieme di feature è indicativo o meno: l'entropia di rappresentazione assume valori bassi quando la maggior parte dell'informazione significativa di un dataset è contenuta lungo poche dimensioni, mentre assume valori alti quando l'informazione è equamente distribuita fra tutte le feature. Quindi, studiando l'andamento dell'entropia di rappresentazione del dataset al variare del numero di attributi selezionati, è possibile individuare il miglior valore di $ h $.

\subsection{Trasformazione dei dati}
Come anticipato in \autoref{sec:contesto}, ciascun algoritmo richiede un preciso formato degli input per poter essere applicabile, e i dati, quando non soddisfano tale formulazione, devono essere convertiti. Una frequente ragione di incompatibilità fra dati e tecniche di clustering è il tipo degli attributi---numerico o categorico---ed il nostro grafo li contiene entrambi. Anche per questo motivo, si è reso necessario scrivere dei convertitori che eseguono la trasformazione del grafo nella formulazione specifica di ciascun algoritmo. Limitatamente al tipo di dato supportato, LAC, MOC e ORCLUS sono numerici, mentre NetClus, LatentNet, CESNA, BAGC e ROCK sono categorici.
\subsubsection{Conversione numerica dei like}
\label{subsubsec:conversione_like}
Per convertire una variabile categorica in una numerica, l'approccio più consueto è generare tanti attributi booleani quanti sono i possibili valori nel dominio della variabile. A ciascun like nel dataset è stata quindi associata una variabile che, per ciascun utente, assume valore \textit{vero} o \textit{1} se l'utente ha espresso \textit{mi piace} su quel contenuto e \textit{falso} o \textit{0} altrimenti. \`E chiaro che la scelta dei valori numerici corrispondenti a \textit{vero} e \textit{falso} è completamente arbitraria ma non ininfluente, come discuteremo nel prossimo paragrafo.
\subsubsection{Normalizzazione}
\label{subsubsec:normalizzazione}
Gli algoritmi numerici, tanto quelli probabilistici quanto quelli basati sulla distanza, fanno spesso ricorso alla distanza euclidea per confrontare gli individui. Quando gli attributi possiedono intervalli di valori non comparabili---per esempio uno molto ampio, come l'età, ed uno molto compatto, come i like precedentemente trasformati---la differenza tra due individui, limitatamente al primo attributo, sarà determinante nel calcolo della distanza complessiva. Per questa ragione sono essenziali le tecniche di normalizzazione, come \mbox{\textit{min-max}} e \mbox{\textit{z-score}} che abbiamo utilizzato in questo lavoro. Min-Max normalizza le variabili nell'intervallo $[0,1]$ mentre Z-score trasforma i dati affinché possiedano media nulla e varianza unitaria. Non esiste un approccio preferibile in ogni circostanza e la scelta dipende dai dati da normalizzare, ma influisce certamente sulla qualità dei risultati del clustering \cite{visalakshi2009}. Tuttavia, abbiamo riscontrato che la presenza di outlier incide notevolmente nella normalizzazione min-max, che tende a comprimere i dati non anomali in un intervallo ristretto di valori. Quindi la scelta è ricaduta sulla normalizzazione \textit{z-score}, con l'eccezione degli algoritmi (come MOC) che accettano un dataset in formato sparso. In questi ultimi, la possibilità di comprimere la dimensione del dataset dipende dal numero di elementi nulli nella matrice dei dati, in massima parte i like, la cui trasformazione tramite \textit{z-score} vanificherebbe la sparsità dei dati. Le tecniche di normalizzazione considerate hanno imposto la rimozione degli attributi a varianza nulla, ovvero quelli che assumono lo stesso valore sull'intera popolazione; poiché tali attributi non contribuiscono al raggruppamento, la loro eliminazione non pregiudica la qualità del clustering.
\subsubsection{Conversione numerica dei luoghi}
\label{subsec:conversione_luoghi}
La trasformazione numerica applicata ai like è valida, ma ha lo svantaggio di aumentare la dimensionalità dei dati ed in alcuni casi specifici è possibile fare leva sulla semantica degli attributi per ottenere migliori risultati. Il \textit{geocoding} è il processo che porta alla determinazione delle coordinate geografiche di un luogo---espresse in longitudine e latitudine---a partire da altre informazioni, come l'indirizzo o il codice di avviamento postale. Dal nome della città di nascita e residenza, contenuto nel profilo Facebook, siamo arrivati alle sue coordinate appoggiandoci alle \textit{Geocoding API} di Google. A questo punto, però, è essenziale un'ulteriore trasformazione: infatti un qualsiasi algoritmo di clustering considererà le due coordinate separatamente anche se queste rappresentano uno stesso attributo. È stato, quindi, necessario far sì che la latitudine e la longitudine venissero trasformate in un unico numero (che abbiamo chiamato \textit{ODC}, \textit{One-Dimensional Coordinates}) preservando le distanze originali tra i luoghi. A questo proposito, abbiamo utilizzato \textit{lo scaling multidimensionale}, ovvero una tecnica di analisi statistica che, partendo dalla matrice delle distanze tra oggetti $ D $ dimensionali, assegna ad ogni oggetto una coordinata a $ \tilde{D} < D $ dimensioni, in modo tale che venga approssimata la distanza euclidea originale. Quindi, abbiamo calcolato la matrice delle distanze tra tutte le città del dataset e, tramite lo Statistics ToolBox di Matlab, ponendo $ \tilde{D} = 1 $ siamo riusciti ad ottenere l'ODC per ogni luogo del dataset.
\subsubsection{Discretizzazione}
\label{subsubsec:discretizzazione}
La trasformazione dei like da variabile categorica a numerica ha sensibilmente cambiato la struttura del problema. Poiché il numero di like unici è di poco inferiore al milione di elementi e ciascuno di essi formerà una nuova dimensione, la complessità del problema esplode rapidamente e pochissimi algoritmi sono capaci di operare in uno spazio così sparso. Un altro dato di interesse è la popolarità dei singoli like: benché esistano oggetti largamente condivisi (oltre 200 \textit{mi piace} sono comuni a più di 1000 utenti), in media un singolo like è stato espresso da meno di 5 individui; pertanto il potere di aggregazione dei like in questa formulazione è prevedibilmente basso. Analogamente alla discretizzazione delle variabili numeriche, per le variabili categoriche abbiamo costruito una gerarchia concettuale, tramite la quale è possibile ridurre la dimensionalità del problema risalendo dai concetti più precisi, i like, a quelli più generali, le categorie tematiche in cui Facebook classifica i like. L'uso delle categorie è necessario per tutti gli algoritmi che non accettano e non sfruttano internamente una formulazione sparsa dei dati (ad esempio LAC). In quei casi infatti non è possibile trarre vantaggio dal gran numero di elementi nulli---i like che un utente non ha mai condiviso---all'interno del dataset, la cui dimensione su disco ed in memoria non è tollerabile. In generale, le categorie sono estremamente vantaggiose per una prima analisi dei dati, con la quale identificare approssimativamente i fattori di aggregazione degli utenti. A tal punto, le categorie più rilevanti possono essere nuovamente dettagliate nei singoli like, ed il clustering ripetuto in uno sottospazio del dataset originale a granularità più fine.\\
Per gli algoritmi categorici si è resa necessaria una discretizzazione delle variabili numeriche: in particolare, per l'età sono state utilizzate tecniche standard di discretizzazione \textit{equal-width} mentre le città (identificate dall'\mbox{ODC} negli algoritmi numerici) sono state raggruppate per provincia grazie alle Geocoding API di Google.
\section{Esecuzione degli algoritmi}
\label{esecuzione_algoritmi}
Il primo passo dell'analisi sperimentale è stato la generazione di campioni del grafo degli utenti progressivamente più grandi: 1500, 3000, 6000 e 11000 nodi. Di ciascun campione abbiamo misurato gli indici topologici descritti in \autoref{topo_metrics}, che serviranno a studiare l'effetto della struttura e del volume del grafo sulle prestazioni degli algoritmi. I grafi così costruiti sono stati dapprima sottoposti ai convertitori scritti per ciascun algoritmo, e, laddove necessario, è stata applicata la normalizzazione, la discretizzazione e l'imputazione dei valori mancanti. Infine sono stati eseguiti gli algoritmi.\\
Descriviamo adesso per ciascuna tecnica i passi necessari a produrre i risultati sperimentali e le complicazioni che sono emerse.
\begin{description}
\item[LAC] è una tecnica per dati numerici, e pertanto si è resa necessaria la trasformazione di ogni variabile categorica: genere, luogo di nascita e residenza, orientamento sessuale, stato sentimentale, like. Come descritto in \autoref{subsec:conversione_luoghi}, per i luoghi abbiamo utilizzato le coordinate monodimensionali; per i like e per il genere, che nel nostro dataset assume due soli valori, abbiamo invece adottato una trasformazione in attributi binari.
%Un'ultima peculiarità, il dataset per LAC deve essere orlato di una colonna contenente l'etichetta di classe, se disponibile, per ciascun elemento; in assenza di \textit{ground-truth}, la colonna deve comunque essere aggiunta ma può contenere dati arbitrari. Questa informazione sarà utilizzata per presentare all'utente la matrice di confusione, tramite la quale valutare l'accuratezza del clustering.
L'algoritmo è scritto in linguaggio C e legge da file una tabella che sarà poi interamente materializzata in memoria, a prescindere dalla sparsità dei dati. Questa implementazione preclude dataset con numerose dimensioni, per cui la tecnica è limitata all'uso delle categorie di like o al più poche migliaia di attributi. Oltre ai dati, l'algoritmo richiede due parametri: il numero \textit{k} di cluster nella soluzione e l'indicatore \textit{h} del numero di attributi che formeranno il sottospazio di ciascun cluster. Tra i due, \textit{h} è chiaramente di difficile stima, poiché influisce indirettamente sul risultato attraverso la funzione obiettivo. Un inconveniente pratico dell'algoritmo è l'impossibilità di eseguire agevolmente e automaticamente il clustering su un intervallo di valori dei parametri. A causa della rappresentazione in memoria dei dati, cambiare \textit{k} o \textit{h} richiede infatti la ricompilazione del programma\footnote{Poiché in C le matrici sono allocate staticamente, variare a tempo di esecuzione \textit{k} e \textit{h} impone la riscrittura del codice con l'allocazione dinamica delle matrici. In alternativa, gli array di lunghezza variabile (VLA) sono stati introdotti in C99, ma alcuni compilatori di rilievo (Visual Studio) non supportano lo standard.}.
%[Questo forse non è necessario anticiparlo adesso] Secondariamente, nonostante gli autori abbiano offerto le prove teoriche della convergenza dell'approccio adottato \cite{lac}, LAC esibisce tempi di esecuzione estremamente scostanti, pur lavorando sullo stesso dataset e con minime variazioni della parametrizzazione.
% 1286844 byte - 1500 rows
% 2691312 byte - 3000 rows
\item[ORCLUS] similmente a LAC è un algoritmo per dati numerici, ed ha richiesto le medesime operazioni sul dataset. L'implementazione che abbiamo utilizzato per l'analisi sperimentale è contenuta nel package \textit{orclus} per R. Anche in questo caso, la procedura deve ricevere in input una matrice densa\footnote{Siamo tuttavia a conoscenza di un'altra implementazione pubblica nel framework Elki, che supporta dataset in formato sparso.\\ \url{http://elki.dbs.ifi.lmu.de}}. ORCLUS richiede tre parametri all'utente: il numero \textit{k} di cluster, la dimensione \textit{l} del sottospazio specifico di ciascun cluster e il numero \textit{k0} di cluster iniziali, che vengono progressivamente accorpati fino a raggiungere una soluzione della dimensione specificata \textit{k}. In ogni caso, la formulazione binaria dei like genera una matrice estremamente sparsa, che risulta spesso matematicamente incompatibile con l'implementazione dell'algoritmo al crescere del parametro \textit{k0}.
\item[MOC] come nei casi precedenti opera su dati numerici, ma, giacché realizzato in Matlab, è possibile utilizzare un formato sparso per la matrice dei dati sia su file sia in memoria. Le trasformazioni applicate sui dati sono le medesime dei due algoritmi precedenti, ma in questo caso sarebbe possibile codificare direttamente i like senza ricorrere alle categorie. L'algoritmo richiede all'utente due parametri: il numero \textit{k} di cluster ed una stima iniziale dell'appartenenza di ciascun elemento ai cluster della soluzione. Questa matrice di appartenenza approssimata può essere ottenuta dall'esecuzione di un altro algoritmo di clustering, come k-means o clustering gerarchico.
\item[LatentNet] è stato distribuito dagli autori nel package \textit{latentnet} per R. LatentNet opera su grafi con attributi e, per poter essere sottoposto all'algoritmo, il dataset deve essere convertito in un oggetto di tipo \textit{network}. Abbiamo però incontrato da subito uno scoglio invalicabile: sebbene i modelli descritti dagli autori siano potenti, tanto l'impronta in memoria quanto il tempo di computazione richiesto per produrre una soluzione crescono brutalmente con la dimensione del campione, al punto da rendere la tecnica inutilizzabile nel nostro scenario.
\item[NetClus] esige l'esistenza di un arco tra ogni utente e ciascun tipo di entità: per gli attributi di profilo, questo si traduce nell'assenza di valori mancanti. Nonostante l'algoritmo sia calzante per il problema, l'implementazione che abbiamo ottenuto dagli autori è risultata inutilizzabile: le funzioni di ordinamento, usate nel calcolo della probabilità a posteriori del modello costruito da NetClus, sono purtroppo specifiche per il clustering di risorse bibliografiche, ed assumono un modello dei dati composto da articoli, autori, conferenze e aree di ricerca. Cambiare tali funzioni, identificare dei sostituti adeguati e scriverne il codice non avrebbe comunque garantito il raggiungimento dei risultati mostrati nell'articolo di ricerca. Pertanto abbiamo dovuto scartare NetClus.

\item [Fast Unfolding] è stato implementato del package \textit{igraph} di R. Il metodo richiede in input un oggetto \textit{graph} che può essere costruito a partire da un file GML; restituisce le comunità identificate e il valore della modularità del partizionamento.
\item [CESNA] fa parte della piattaforma SNAP\footnote{\url{http://snap.stanford.edu/}} che mette a disposizione una serie di tool e librerie C++ per l'analisi e la manipolazione di reti di grandi dimensioni. Richiede in input tre file: (1) il grafo, (2) una tabella che descrive quali sono gli identificatori degli attributi di ogni utente, (3) una mappa che associa ad ogni identificatore la coppia $ (attributo, valore) $. L'algoritmo permette di specificare una vasta gamma di parametri: il numero di comunità da individuare, il numero minimo e massimo di comunità nel caso si decida di identificarle automaticamente, il peso da dare agli attributi e alla rete del grafo e, infine, il numero di thread per una versione parallela. È l'unico, tra gli algoritmi selezionati, che consente la presenza dei valori mancanti e, quindi, risulta essere efficiente anche con i singoli like di un utente. Poiché utilizza dati categorici è stato necessario discretizzare gli attributi come l'età e le città di nascita e di residenza.
\item[BAGC] similmente a CESNA opera su grafi con attributi categorici, quindi ha richiesto le medesime trasformazioni sul dataset. L'algoritmo richiede all'utente il numero di cluster da individuare e due file in input: il grafo e una matrice che specifica per ogni utente il set di attributi che lo qualifica. Poiché non sono ammessi i valori mancanti, per evitare una dimensionalità troppo elevata, è stato necessario l'utilizzo delle categorie invece dei singoli like.
\item[DB-CSC] riceve in input un grafo con attributi in formato GraphML. Richiede all'utente di impostare i parametri $ \epsilon $, $ k $ e $ minPts $ descritti nella \autoref{selezione_algoritmi}. Come molti altri algoritmi, non consente la presenza di valori mancanti e, quindi, anche in questo caso è stato necessario utilizzare le categorie dei like. Purtroppo, con i dataset a disposizione, l'implementazione JAVA messa a disposizione dagli autori si è rivelata essere davvero inefficiente dal punto di vista computazionale. Per tanto DB-CSC è stato escluso dall'analisi sperimentale.
\item [Inc-Cluster] richiede all'utente di specificare il numero di cluster, la probabilità di restart e la lunghezza massima dei random walk. Come input necessita della matrice di transizione (in formato sparso) del grafo e di una tabella che specifica, per ogni individuo, gli attributi e i corrispettivi valori. L'implementazione MATLAB messa a disposizione dagli autori è sviluppata appositamente per un dataset particolare: ciò avrebbe comportato di dover reimplementare l'algoritmo e renderlo capace di accettare un qualunque input. Spinti da questa circostanza e dalle basse performance teoriche\footnote{$ O(n^{2.807}) $}, abbiamo deciso di non utilizzarlo nel nostro framework.
\item [ROCK] è distribuito nel package \textit{cba} di R; si è mostrato in generale molto efficace con i dataset a nostra disposizione, individuando cluster eccessivamente numerosi e pertanto non significativi. Per questa ragione, seppure l'algoritmo è presente nel nostro framework, si è deciso di non utilizzarlo nell'analisi sperimentale.
\end{description}

\section{Descrizione modulare dell'architettura}
Per avere una visione d'insieme delle varie tecniche e funzionalità appena esposte, in quest'ultimo paragrafo mostriamo in maniera descrittiva quali sono i moduli teorici che compongono il nostro framework e in quale ordine sono stati utilizzati (\autoref{fig:framework_flow}).\\
Il modulo \texttt{Data To Graph} permette di ottenere un unico grafo con attributi, in formato GML, partendo da un insieme di utenti Facebook, in formato JSON.\\
Questo grafo può essere campionato tramite il modulo \texttt{Graph Sampler} (paragrafo \ref{sampler}) che restituirà un insieme di sottografi con caratteristiche topologiche e di attributi molto variegate. Lo strumento può essere utilizzando anche per selezionare un gruppo di utenti che abbiano certe caratteristiche: ad esempio, si può avere la necessità di condurre una segmentazione di mercato solo sui profili Facebook che siano al di sotto di una certa età e con un particolare livello di istruzione.\\
Ogni grafo GML può essere sottoposto al modulo \texttt{Graph Preprocessing} (paragrafo \ref{preparazione_dati}) che è composto a sua volta dai tre strumenti per l'identificazione degli outlier, la selezione degli attributi rilevanti e l'inferenza dei valori mancanti.\\
Se si necessita di una valutazione topologica del grafo è allora possibile usare il modulo \texttt{Topological Metrics} (paragrafo \ref{topo_metrics}): il risultato sarà composto da una serie di metriche che indicano il numero di nodi e di relazioni d'amicizia, le componenti connesse del grafo, o altre informazioni più complesse come la robustezza o l'entropia del grafo.\\
Il modulo \texttt{Algorithms} (paragrafo \ref{esecuzione_algoritmi}) invece, presenta, oltre a tutti gli algoritmi descritti nei paragrafi precedenti, anche un insieme di strumenti di conversione che, partendo dal grafo GML da analizzare, consente di ottenere i file di input nella rappresentazione interna di un algoritmo.\\
Il modulo \texttt{Communities Description} serve ad avere una rappresentazione immediata delle varie comunità: tramite un file XML, viene indicata la media, la varianza e la lista dei valori di ogni attributo del cluster ordinata per rilevanza.\\
Infine, il modulo \texttt{Clustering Evaluation} (le cui funzionalità verranno descritte nel prossimo capitolo insieme all'analisi sperimentale) permette di valutare la bontà di un risultato di clustering, confrontare le prestazioni dei diversi algoritmi e suggerire la tecnica migliore per un certo dataset in input. \\
\newpage
\begin{sidewaysfigure}
 \centering
    \includegraphics[width=1\textheight]{pictures/workflow.pdf}
    \caption{Workflow modulare dell'architettura}
    \label{fig:framework_flow}
\end{sidewaysfigure}
