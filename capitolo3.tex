\chapter{La piattaforma Neosperience Engage}
\label{capitolo3}
\thispagestyle{empty}

Neosperience è una azienda italiana che offre servizi di marketing e \textit{customer experience}. Customer experience è la percezione che il cliente matura---razionalmente e emotivamente---della relazione con un marchio, un fornitore di beni o servizi. In un mercato globale \mbox{iper-competitivo} e \mbox{iper-connesso}, per contrastare la crescente dispersione degli utenti, il marchio deve stabilire un legame individuale con il cliente. La gestione dell'esperienza cliente \`e la pianificazione e la reazione alle interazioni del cliente al fine di raggiungerne ed eccederne le aspettative ed in questo modo aumentare la soddisfazione, la lealt\`a e il sostegno del cliente al marchio\footnote{``The practice of designing and reacting to customer interactions to meet or exceed customer expectations and, thus, increase customer satisfaction, loyalty and advocacy'' - Gartner}: in una parola, coinvolgimento o \textit{engagement}. L'esperienza cliente digitale apre nuove vie di contatto con il cliente: non solo portali internet, ma applicazioni per social network e dispositivi mobili, tecnologie 3D e negozi virtuali, realtà aumentata, \textit{gamification}. Il ciclo vitale di un cliente rispetto ad un marchio inizia con il coinvolgimento, la scoperta di un prodotto o un servizio; dopo questo primo contatto, seguono diversi \textit{moment of truth}, le occasioni in cui il cliente interagisce con l'azienda e si forma una opinione, consapevole ed inconscia. Il mantenimento del cliente dipende dalla capacit\`a di influire positivamente in ciascuno di questi istanti: la valutazione del prodotto, la decisione dell'acquisto e l'esperienza d'uso. Questo non \`e solo qualit\`a del servizio, ma arrivare ad una comprensione cos\`i profonda del cliente da poter offrire una esperienza---contenuti e benefici---tanto personalizzata ed appagante da indurlo non solo a restare leale al marchio ma a convincere altri ad avvicinarsi. Per raggiungere questo grado di conoscenza \`e necessario estrarre indizi da ogni punto di contatto con il cliente, sfruttando l'immensa mole di informazione nella rete.\\
Recentemente, Neosperience ha sviluppato Engage, una collezione gratuita di librerie per presentare ad un utente il miglior ordinamento degli oggetti di un catalogo, secondo un criterio di pertinenza, basandosi sul profilo dell'utente delineato dalla sua impronta sui social network.
Engage offre una interfaccia (API) o uno strumento di sviluppo (SDK) per arricchire o creare applicazioni per dispositivi mobili. Una applicazione costruita con Engage è una galleria di contenuti; ciascun prodotto del catalogo o \textit{deck} è presentato individualmente su una \textit{card}, un volantino digitale, che racchiude una intestazione, una immagine e una breve descrizione testuale. Attraverso le API Engage è inoltre facilmente possibile offrire funzionalità di \mbox{e-commerce} e condivisione della card su social network. Ad ogni oggetto nella vetrina deve essere infine associato un \textit{target} o utente ideale, colui che l'autore dei contenuti ritiene essere più interessato all'oggetto o propenso ad acquistarlo. Il \textit{target} è definito mediante variabili demografiche---età, genere, luogo di nascita, educazione---, interessi, avversioni e geolocazione---la distanza dell'utente da un luogo specificato. Quando l'utente installa e avvia l'applicazione può connettere i propri profili sui principali social network---Facebook Twitter e Foursquare---permettendo alla piattaforma Engage sottostante di tratteggiare un profilo dell'utente avvalendosi dei dati personali, dei \textit{like} e \textit{hashtag} sia dell'utente sia degli amici sul social network. Partendo da questo profilo, Engage misura per ogni oggetto nel catalogo dell'applicazione la somiglianza tra il \textit{target} dell'oggetto e l'utente; gli oggetti sono quindi riordinati e visualizzati dal più appropriato, quello per cui l'utente è massimamente simile al \textit{target}, al più distante.\\
Questo lavoro è stato motivato dall'esigenza di Neosperience di valorizzare i dati degli utenti raccolti tramite la propria piattaforma Engage. Questi dati difatti racchiudono il potenziale per un diverso approccio alla segmentazione della clientela, facendo leva su informazioni variegate, semi-strutturate e non strutturate, ma nuove e complementari rispetto alle tradizionali categorie sociali e geografiche. L'opportunità offerta da questi dati è la conoscenza puntuale degli interessi, delle opinioni, del mood di ciascun cliente, e poter rispondere in tempo reale, limitati soltanto dalla capacità di elaborare e assimilare questa informazione. Tuttavia, complice la giovinezza della piattaforma, la quantità di dati a disposizione di Neosperience si è rivelata molto al di sotto del volume prospettato a regime, sul quale gli algoritmi e le tecniche di data mining avrebbero dovuto essere messe alla prova. Pertanto, il lavoro si è articolato nei seguenti passi:
\begin{itemize}
\item \hyperref[capitolo2]{studio della letteratura}
\item \hyperref[raccolta_dati]{raccolta dei dati}
\item \hyperref[selezione_algoritmi]{selezione degli algoritmi}
\item \hyperref[preparazione_dati]{preparazione dei dati}
\item \hyperref[esecuzione_algoritmi]{esecuzione degli algoritmi}
\item \hyperref[capitolo4]{analisi dei risultati}
\end{itemize}
\section{Studio della letteratura}
\label{studio_letteratura}
\section{Raccolta dei dati}
\label{raccolta_dati}
Nel progetto originale, il framework è alimentato da tre sorgenti di dati: Facebook, Twitter e Foursquare.
\subsection{Il modello dei dati}
\textbf{Facebook}, con oltre un miliardo di utenti mensili, è il più diffuso e noto social network al mondo. In Facebook, un utente può pubblicare la propria storia personale, interessi, esperienze, foto, lavoro e perfino stati d'animo. D'altronde, il fulcro dei social network è tessere relazioni, e Facebook non fa eccezione: ogni utente può stringere amicizia con altri utenti, condividere con essi contenuti, conversare, giocare, organizzare eventi ed essere informato di ogni cambiamento nella propria rete sociale. In conclusione, Facebook contribuisce la porzione dominante di informazione: il profilo utente, i like, le amicizie. Come riportato in \autoref{fig:modello_dati}, la frazione osservabile del profilo include il nome, che abbiamo rimosso dai dati per irrilevanza e riservatezza, il genere, la data e il luogo di nascita, la città di residenza, l'educazione---scuola superiore, università e formazione specialistica---, lo stato sociale o sentimentale, l'orientamento sessuale. Al pari di ogni altro nodo del grafo sociale di facebook, i like sono identificati univocamente ma anche già organizzati in oltre 200 macrocategorie, talvolta ulteriormente frazionate in sottogruppi: questo ha permesso di ridurre notevolmente la dimensionalità del problema.

\textbf{Twitter} è un social network e una piattaforma di microblogging. Al cuore di Twitter vi sono i \textit{tweet}, brevi messaggi in 140 caratteri che un utente pubblica e la piattaforma notifica a tutti i suoi contatti, chiamati \textit{follower}. Un utente riceve i tweet delle persone che segue sulla propria bacheca o \textit{home timeline}, da cui può rispondere, inserendosi nel flusso di tweet esistente, o ripubblicare (\textit{retweet}), propagando il tweet ai propri follower. Per organizzare tematicamente le conversazioni, i tweet sono spesso etichettati con un \textit{hashtag}, una parola chiave preceduta da un cancelletto, o \textit{hash} in inglese. Questa convenzione nacque spontaneamente tra gli utenti di Twitter e fu raccolta ed integrata nella piattaforma, che oggi pubblica in tempo reale la lista dei \textit{trending topics}, le parole o argomenti di discussione che compaiono più frequentemente negli ultimi tweet.

\textbf{Foursquare} \`e tanto un social network focalizzato sulla posizione dell'utente quanto un gioco, il cui successo è indubitabilmente connesso al dilagare di dispositivi mobili e intelligenti, sempre più comunemente provvisti di una antenna GPS. Foursquare non solo rende pubblico dove l'utente si trova, permettendo di trovarsi fra amici e di incontrare gli altri utenti nella medesima zona, ma invita gli utenti a registrarsi (\textit{check in}) ai luoghi di incontro che visitano---un negozio, un bar, un museo---e attribuisce punti e distintivi (\textit{badges}) per premiare la frequenza con cui vi si accede o la scoperta di nuovi luoghi prima dei propri amici. Ad esempio, l'utente che ha avuto il maggior numero di check-in in un dato luogo nell'arco di 60 giorni ne viene incoronato \textit{mayor}, letteralmente sindaco, ma deve successivamente difendere il titolo dagli altri utenti.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{pictures/modello_dati.pdf}
    \caption{Modello dei dati}
    \label{fig:modello_dati}
\end{figure}
\subsubsection{Problematiche di integrazione fra le sorgenti}
L'obiettivo finale di questa fase è definire un modello unico dei dati, come esposto in \autoref{fig:modello_dati}, per integrare sorgenti eterogenee.\\
Una evidente differenza fra Twitter e Facebook è la natura e lo scopo delle interazioni tra gli utenti. Facebook è solitamente usato per stare in contatto o ritrovare persone che conosciamo realmente o abbiamo frequentato nel passato. In particolare, una volta stretta amicizia il rapporto tra due amici è paritario e simmetrico: possono scambiarsi liberamente messaggi ed ogni contenuto pubblicato da una parte viene notificato all'altra. Al contrario, Twitter è principalmente adoperato per comunicare con altre persone con le quali, pur non frequentandole nella vita reale, condividiamo interessi o argomenti di discussione. Questo legame più superficiale incide sul grado di omofilia nella rete: individui connessi come \textit{friend} saranno in genere più simili di individui connessi come \textit{follower}, tanto più se la relazione non è reciproca. Twitter realizza un modello asimmetrico di relazione, in cui la discrepanza tra il numero di persone che ci seguono e quelle che seguiamo determina la nostra reputazione all'interno della comunità. Nella pratica, l'asimmetria limita i privilegi di colui che segue: ad esempio, il \textit{follower} non può contattare unilateralmente le persone che segue, a meno che non sia stato esplicitamente autorizzato o la relazione sia stata ricambiata. Questa peculiarità ha sensibili implicazioni sul modello dei dati, giacché il grafo degli utenti deve adesso contenere archi orientati.\\
Una ulteriore peculiarità risiede nel confronto tra \textit{like} e \textit{hashtag}. Mentre la relazione tra un utente Facebook e un like è binaria, o l'utente ha espresso il proprio gradimento di un oggetto o non lo ha fatto, nel caso di Twitter un utente può non adoperare mai un hashtag o usarlo con frequenza. Anche in questo caso il modello dei dati richiede una estensione, tramite l'introduzione di pesi sugli archi utente-hashtag. A differenza del caso precedente, dove un arco non orientato può essere equivalentemente modellato da una coppia di archi orientati, conciliare gli archi privi di peso di Facebook con quelli pesati di Twitter richiede una normalizzazione arbitraria dei pesi. In aggiunta, il significato di un hashtag è tutt'altro che analogo ad un like, poiché, avendo fissato l'argomento, un tweet può esprimere tanto apprezzamento quanto avversione, discussione o ``inutile chiacchiera''\footnote{\url{http://web.archive.org/web/20110715062407/www.pearanalytics.com/blog/wp-content/uploads/2010/05/Twitter-Study-August-2009.pdf}}. Per amalgamare semanticamente i tweet ai like sarebbe necessario discriminare i tweet in semplici categorie, \textit{like} e \textit{dislike}, che tuttavia è un problema di elaborazione del linguaggio naturale e clustering in sé. Infine, la vita media di un hashtag è di gran lunga più breve rispetto ad un like. In termini assoluti, mentre un like è sostenuto da un contenuto---una pagina o una realtà esterna a Facebook---ed esprimendo il like l'utente si connette a tale contenuto nel grafo sociale, la creazione di un hashtag richiede al più uno sforzo di immaginazione; la controparte della proliferazione di hashtag è proprio la loro volatilità. In termini relativi, il carattere episodico e conversevole del tweet richiede tempi di elaborazione ed azione più stringenti, in quanto l'associazione e il coinvolgimento dell'utente rispetto ad un hashtag può rapidamente consumarsi. In sintesi, l'analisi dei tweet richiede algoritmi tolleranti di una elevatissima dimensionalità, coerente con l'elaborazione del linguaggio naturale, possibilmente basati su grafi e che garantiscano una bassa complessità temporale, anche a scapito della qualità.\\
Nel 2010 Facebook lanciò Luoghi, un nuovo componente dell'applicazione ed un clone di Foursquare, di cui imitava la terminologia e le funzionalità. Il servizio conobbe poco successo e fu abbandonato un anno dopo, mantenendo unicamente la possibilità di comunicare la propria posizione quando si pubblica un post. Per questa ragione, Foursquare è un servizio unico nel suo genere e, contribuendo informazione nuova e non ridondante, non richiede sforzi addizionali di assimilazione nel modello.\\
Una volta definiti i criteri di integrazione, l'ultimo passo sarebbe la \textit{entity resolution}, ovvero l'identificazione dei profili che, benché apparentemente scorrelati e provenienti da sorgenti diverse, pertengono al medesimo individuo reale. Questa fase non risulta tuttavia applicabile ai dati a nostra disposizione, che provengono unicamente da Facebook.\\
\subsection{La costruzione del dataset}
Le \mbox{Graph API}\footnote{https://developers.facebook.com/docs/graph-api} sono lo strumento principale che permette di leggere e scrivere sul grafo sociale di Facebook --- una rappresentazione delle informazioni in esso presenti composto da:
\begin{itemize}
\item \textbf{nodi}: entità di vario tipo come utenti, foto, pagine o commenti; sono etichettati da un identificatore unico.
\item \textbf{archi}: connessioni esistenti tra le entità: relazioni d'amicizia tra utenti, foto in una pagina, commento di un certo utente, etc.
\item \textbf{campi}: attributi delle entità come il compleanno di un utente o il nome di una pagina.
\end{itemize}
La maggior parte delle richieste fatte con le Graph API necessitano di un \textit{access token} che consente un accesso temporaneo e sicuro alle informazioni di profilo di un utente. In particolare, l'access token deve essere composto con una serie di permessi a seconda di quale attributo si vuole leggere o scrivere: ad esempio, se si è interessati al nome, alla religione e agli amici di un certo individuo, il token di accesso deve essere composto, simultaneamente, con i permessi \texttt{user\_about\_me}, \texttt{user\_religion\_politics} e \texttt{user\_friends}.\\
Per recuperare queste informazioni è sufficiente una chiamata al metodo \texttt{HTTP GET}:
\begin{lstlisting}[basicstyle=\normalfont\ttfamily\scriptsize,backgroundcolor=\color{background}]
GET https://graph.facebook.com/me?fields=name,religion,friends
\end{lstlisting}
che darà come output il seguente file JSon\footnote{JavaScript Object Notation, http://www.json.org}:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "name": "Nicola Padovano", 
  "religion": "Gnosticism",
  "friends": {
    "data": [
      { "name": "Linda",  "id": "987654321" },
      {	"name": "Matteo", "id": "123123123" }
    ]
   }
}
\end{lstlisting}

\subsubsection{Applicazione per la raccolta dei dati}

Mediante l'SDK per PHP\footnote{https://developers.facebook.com/docs/reference/php/4.0.0} abbiamo creato un'applicazione che ha ci permesso di recuperare le seguenti tipologie d'informazioni: \textit{profilo} (data di nascita, genere, orientamento sessuale, stato sentimentale, città natale, città di residenza  e istruzione), \textit{like} (tutte le pagine su cui si è indicato \textit{like}) e \textit{amici} (profilo e like degli amici).\\
L'applicazione è stata utilizzata da 87 utenti (che abbiamo indicato con il termine \textit{DAU} o \textit{Direct User Application}) ottenendo, in totale, circa 25 mila profili Facebook.\\
Il passo successivo è stato quello di unificare i tutti i profili in un unico file GML\footnote{Il formato GML (Graph Modelling Language) permette la rappresentazione dettagliata di un grafo con attributi}. Dal file JSon di ogni DAU sono stati recuperati i JSon degli amici: in questa maniera è stato possibile costruire la rete personale del DAU (o \textit{ego-network}) arricchita dalle informazioni di profilo e dai like di ogni nodo. La composizione incrementale di queste reti ha portato alla creazione del grafo finale.

\subsubsection{Pulizia preliminare dei nodi}
Il grafo è stato ripulito da un insieme di nodi problematici sotto due aspetti. Un utente, infatti, viene escluso se: (1) il numero di attributi di profilo mancanti è maggiore della metà, e (2) se presenta un valore mancante per un certo attributo e la percentuale dei suoi vicini che, per lo stesso attributo hanno valore mancante, è maggiore del cinquanta percento. Il primo punto è di immediata comprensione, il secondo è giustificato dal fatto che l'inferenza di un certo attributo di un utente è fatta basandosi sui valori che quell'attributo possiede nei vicini dell'utente stesso: se più della metà dei vicino hanno un valore mancante allora l'inferenza può portare a risultati molto approssimativi.

\subsubsection{Creazione dei sottografi}
Come già accennato in precedenza, abbiamo sviluppato un tool\footnote{Il linguaggio utilizzato per la creazione di questo tool è Gremlin (https://github.com/tinkerpop/gremlin/wiki) che permette di comporre delle query su grafi} per il campionamento di grafi che, da un lato, riuscisse a create sotto-grafi significativi dal punto di vista topologico e degli attributi, dall'altro, che presentasse una forte componente casuale per modellare la varietà dei dati in input che potrebbero essere utilizzati successivamente.\\
Il procedimento adottato è il seguente:
\begin{enumerate}
\item si sceglie casualmente un DAU $ X $ non ancora considerato
\item si recuperano i vicini di $ X $ non ancora considerati
\item con un certa probabilità $ p $ si sceglie un vicino $ Y $ di $ X $; altrimenti si torna a 1, scegliendo casualmente un altro DAU
\item se si è scelto il vicino $ Y $ di $ X $ si scala la probabilità $ p $ di un fattore $ s < 1 $, si pone $ X = Y $ e si torna ad 1
\end{enumerate}
Questo procedimento viene iterato fino a quando non si raggiunge una certa percentuale $ \pi $ di utenti considerati.\\
Un DAU è un nodo ricco di informazioni (in quanto ha utilizzato l'applicazione direttamente sul suo profilo) quindi, durante il campionamento, è fondamentale che esso venga scelto come punto di partenza dell'algoritmo. Con una probabilità che scala in maniera incrementale, si considerano i suoi vicini e i vicini dei vicini costruendo così una rete collegata al DAU iniziale. Successivamente, scegliendo un nuovo DAU (cioè partendo di nuovo dal punto 1) si ha la possibilità di costruire una rete ulteriore che, a seconda dei casi, sarà collegata o meno alla prima. Questo procedimento iterativo porta ad avere alla selezione di un insieme dei nodi su cui è possibile indurre il grafo campionato.\\
Infine, variando i tre parametri ($ p $, $ s $ e $ \pi $) siamo stati in grado di generare una vasta gamma di sotto-grafi che presentano caratteristiche (topologiche e di attributi) molto variegate. 
\section{Selezione degli algoritmi}
\label{selezione_algoritmi}
In considerazione della varietà dei dataset su cui avremmo operato, abbiamo selezionato una vasta gamma di algoritmi, cercando di differenziarli per modello dei dati, tecnica di clustering (\ref{sec:classificazione_algoritmi}), approccio alla dimensionalità (\ref{subsec:subspace_clustering}), prestazioni.\\
Limitatamente al modello dei dati, gli algoritmi possono essere raggruppati in tre categorie:
\begin{itemize}
\item clustering su attributi: NetClus \cite{netclus}, MOC \cite{moc}, LAC \cite{lac}, ORCLUS \cite{orclus}
\item clustering su grafo: Blondel et al. \cite{blondel2008fuc}
\item clustering su attributi e grafo: LatentNet \cite{handcock07}, CESNA \cite{cesna}, Inc-Cluster \cite{inc_cluster}, DB-CSC \cite{db_csc}, BAGC \cite{bagc}
\end{itemize}
\subsection{Descrizione degli algoritmi}
\begin{description}
\item[NetClus] è un algoritmo per il clustering di reti composte da entità eterogenee e conformazione a stella (\textit{star network schema}). In assenza di connessioni tra utenti, il modello dei dati descritto in \autoref{fig:modello_dati} riflette esattamente questo scenario: al centro della stella vi è l'utente, da cui scaturiscono archi verso gli attributi di profilo, like, hashtag e check-in, ognuno dei quali forma un nuovo tipo di entità. Il pregio di NetClus è che non si limita ad attribuire una etichetta di cluster a ciascun utente, ma produce un ordinamento dei valori di ciascuna entità in ognuno dei cluster ottenuti, semplificando grandemente l'interpretazione dei risultati.
%Ad esempio, per ogni cluster sarebbero evidenziati i valori di età, luogo di nascita, grado di istruzione, like, check-in\dots più rappresentativi per la popolazione raccolta nel cluster.
In secondo luogo, l'approccio di NetClus riduce fortemente la dimensionalità del problema rispetto agli altri algoritmi su attributi, dove ogni nuovo like o hashtag aggiunge una dimensione al dataset. Un ultimo punto di forza è che non richiede la definizione di una misura di similarità, ma costruisce un modello probabilistico generativo sotto l'ipotesi che la rete esibisca assortatività e \textit{preferential attachment}, come nel caso delle reti sociali. Dato il numero di cluster, l'idea generale di NetClus si compone dei seguenti passi: generare una partizione iniziale degli oggetti ed indurre i \textit{net-cluster} $\{C_{k}^{0}\}_{k=1}^{K}$ dalla rete originale sulla base di queste partizioni; costruire, per ciascun net-cluster, un modello probabilistico generativo $\{P(x|C_{k}^{t})\}_{k=1}^{K}$; quindi calcolare la probabilità a posteriori $P(C_{k}^{t}|x)$ per ciascun oggetto e ricalcolare l'assegnamento degli oggetti ai cluster. I due passi precedenti sono reiterati finché i cluster non raggiungono una condizione stazionaria. Al termine, dalla probabilità a posteriori si ottiene un ordinamento degli oggetti all'interno dei cluster di appartenenza. L'algoritmo ha complessità lineare nel numero di archi, ovvero per reti sparse è approssimativamente lineare nel numero di nodi.
\item[MOC] è un algoritmo probabilistico generativo per l'identificazione di cluster non disgiunti o \textit{overlapping}. L'algoritmo generalizza e semplifica un precedente studio sul clustering sovrapposto di espressioni genetiche \cite{SegalBK03}; con una scelta oculata del modello di probabilità e definizione di distanza---la famiglia esponenziale e la divergenza di Bregman rispettivamente---gli autori sostengono di poter applicare la tecnica a dati sparsi e con numerose dimensioni, laddove il modello gaussiano e la distanza euclidea, usati più comunemente, sono noti offrire scarsi risultati. A testimonianza della formulazione originale, i dati o matrice di espressione $X$ sono modellati come la manifestazione di due fattori, l'appartenenza $M$ e l'attivazione $A$. L'appartenenza descrive quanto i cluster nascosti nei dati si manifestano in ciascun individuo; dato che il modello è \textit{overlapping}, ogni individuo può appartenere simultaneamente a più cluster. L'attivazione indica il peso di ciascuna dimensione dei dati in ognuno dei cluster, ossia quali caratteristiche di un individuo sono determinate più significativamente dall'appartenenza al cluster. I dati sono quindi ricostruiti come il prodotto $X'=M \times A$ tra la matrice di appartenenza e la matrice di attivazione. Seguendo questa intuizione, l'algoritmo costruisce una stima iterativa di $M$ e $A$. Il vettore di appartenenza $M_i$ di ciascun individuo è ottenuto con un approccio \textit{greedy}, volto a minimizzare la distanza tra l'individuo e la sua approssimazione $M_i \times A$: fissata la matrice di attivazione $A$, MOC `accende' con ogni iterazione un nuovo cluster, fino a quando per nessuna scelta è possibile ridurre ulteriormente la funzione obiettivo. La complessità temporale di questa fase è $O(k^3)$ nel numero di cluster $k$. La matrice di attivazione può essere costruita dalla minimizzazione della medesima funzione obiettivo, fissata $M$; seppure sia definita una diversa formula per ogni modello nella famiglia esponenziale e relativa divergenza, la complessità temporale di questa fase può essere stimata in $O(d^3)$.
\item[LAC] è un algoritmo \textit{centroid-based} di \textit{subspace clustering}. Come discusso in \ref{subsec:metodi_partitivi}, le prestazioni degli algoritmi basati sulla distanza euclidea degradano rapidamente al crescere del numero di dimensioni. Per ovviare a questo noto limite, LAC trasforma lo spazio in cui ciascun cluster è immerso: associa ad ogni cluster un vettore di pesi affinché maggiore importanza sia conferita alle dimensioni lungo le quali c'è aggregazione dei punti, all'interno del cluster. I pesi indicano quindi il grado di partecipazione di ciascun attributo al cluster; se i punti sono fortemente raggruppati rispetto a quell'attributo, il peso sarà alto, e basso altrimenti. I pesi sono appresi iterativamente con la ottimizzazione della somma degli errori quadrati (SSE), ovvero, per ciascun cluster, le distanze tra il centroide di riferimento e tutti i punti appartenenti. In assenza di contromisure, tutto il peso sarebbe concentrato sulla dimensione a minima varianza e l'algoritmo scoprirebbe unicamente cluster monodimensionali; per controllare la dimensione dello sottospazio in cui i cluster sono valutati, l'utente deve fissare un termine di penalità che è inserito nella funzione obiettivo. Sebbene l'esistenza di parametri astratti come questo sia un aspetto in genere indesiderabile, gli autori hanno definito una metodologia \textit{ensemble} per la determinazione del valore ottimo attraverso esecuzioni multiple della procedura di clustering.
\item[ORCLUS] è un algoritmo di clustering proiettato (\textit{projected clustering}) basato su \textit{K}-medoids. Dati due parametri specificati dall'utente, il numero di cluster e il numero di dimensioni dei cluster, ORCLUS identifica un insieme di cluster arbitrariamente allineati---non solo paralleli agli assi, ma ottenuti dalla combinazione lineare delle dimensioni originali---ciascuno in un sottospazio della dimensione specificata. ORCLUS è molto simile all'algoritmo k-means, tranne per il fatto che, invece di misurare le distanze nello spazio completo, queste sono calcolate nei sottospazi di ciascun cluster. Preliminarmente, ORCLUS sceglie casualmente dei `semi' o potenziali medoidi. L'algoritmo richiede quindi un processo iterativo: in ogni iterazione ciascun cluster è progressivamente raffinato, rimuovendo le dimensioni prive di aggregazione e riducendo il numero di cluster dall'accorpamento di quelli più simili. I sottospazi sono formati tramite PCA, ovvero calcolando la matrice di covarianza per ciascun cluster e selezionando gli autovettori con la minore diffusione nel sottospazio del cluster, ovvero quelli associati agli autovalori più piccoli. Quando due cluster sono vicini e hanno simili direzioni degli autovettori sono fusi assieme. A chiusura di ogni iterazione è eseguita la fase di assegnamento: ogni punto è associato al seme più vicino e i semi sono ricalcolati come i centroidi dei cluster appena formati. L'algoritmo è computazionalmente intensivo $O(d^3)$ nella dimensione dei dati, in conseguenza del calcolo della matrice di covarianza e degli autovettori di ciascun cluster. Oltretutto, richiede la specificazione non solo del numero di cluster ma anche della dimensione del sottospazio in cui ciascun cluster è costruito, un parametro difficile da stimare correttamente a priori.
\item[LatentNet] è un algoritmo di clustering probabilistico, basato su un modello generativo. Partendo da un grafo di relazioni binarie, il modello assume che ogni nodo possieda una posizione non osservabile in uno spazio sociale \mbox{\textit{d}-dimensionale}, euclideo e latente. Con questa premessa, la presenza o l'assenza di una connessione tra due individui è indipendente da ogni altra informazione, se sono note le posizioni dei due individui nello spazio sociale. Questo modello esprime in maniera innata tanto la transitività quanto l'omofilia negli attributi osservati. Per introdurre la proprietà di clustering, si assume che le posizioni nello spazio latente siano estratte da un modello misto (\textit{mixture model}) di distribuzioni Gaussiane multivariate: ogni distribuzione è caratterizzata da una propria media e varianza, e rappresenta un diverso gruppo di individui o cluster. Per la stima delle posizioni nello spazio latente, gli autori propongono due metodi. Il primo è un metodo in due fasi: inizialmente calcola la stima a massima verosimiglianza del modello dello spazio latente---che spiega la transitività e l'omofilia---e determina le posizioni degli attori nello spazio sociale; la seconda fase stima i parametri del modello misto, basandosi sulle posizione nello spazio latente ottenute nella prima fase. Il secondo metodo è pienamente Bayesiano e usa \textit{MCMC sampling}: il vantaggio è che stima simultaneamente le posizioni latenti e il modello di clustering, ma è computazionalmente più esigente.
\item [Fast Unfolding] è un algoritmo euristico per il clustering di grafi basato sull'ottimizzazione della modularità. Il metodo è diviso in due fasi che vengono ripetute iterativamente. Per prima cosa, ad ogni nodo della rete si assegna una comunità diversa: quindi la partizione iniziale è costituita da tante comunità quanti sono i nodi del grafo. Successivamente, per ogni nodo $ i $ si considerano i suoi vicini $ j $ e si valuta il guadagno della modularità che si avrebbe se si spostasse il nodo dalla sua comunità alla comunità di un nodo $ j $: il nodo $ i $ viene effettivamente spostato in quella comunità che genera il massimo guadagno positivo. Se ciò non è possibile il nodo resta nella sua comunità. Questa prima fase viene ripetuta sequenzialmente per ogni nodo finché non è possibile nessun ulteriore miglioramento. La seconda fase consiste nel costruire un nuovo grafo i cui nodi sono le comunità trovate durante la prima fase: gli archi di ogni nuovo nodo-comunità sono l'unione degli archi dei nodi atomici che lo compongono. Al termine di questa seconda fase si può applicare di nuovo la prima, in un processo iterativo che termina quando non è più possibile spostare un nodo in una comunità diversa ed avere un guadagno positivo di modularità.

\item [Inc-Cluster] è un algoritmo di clustering per grafi con attributi. Per ogni coppia attributo-valore $ (a, v) $ del dataset viene aggiunto, nel grafo originale, un nodo $ n_{a,v} $ che sarà collegato a tutti i vertici che presentano il valore $ v $ per l'attributo $ a $. In questo modo viene creato un grafo \textit{aumentato} su cui è possibile definire una funzione di distanza (la \textit{random walk distance}) che tenga conto sia dell'informazione topologica sia del valore degli attributi dei nodi: infatti la funzione di distanza considera \textit{simili} due individui se ci sono numerosi percorsi che li collegano. Poiché nel grafo aumentato i percorsi tra i nodi sono possibili anche grazie alla presenza degli attributi che condividono e non solo per le relazioni d'amicizia, allora è facile immaginare in che senso la funzione distanza riesce a tener conto, contemporaneamente, della similarità degli attributi e della ego-network dei due nodi.
Definita la similarità tra due utenti, l'algoritmo utilizza una tecnica molto simile a quella del K-Medoids iterando l'assegnamento di un nodo ai vari \virgolette{centroidi} fino a alla convergenza della funzione obiettivo. Purtroppo il calcolo della di similarità tra gli utenti tramite i random walk è computazionalmente inefficiente ($ O(n^{3}) $ con $ n $ numero di nodi) poiché impiega moltiplicazioni matriciali; con tecniche innovative si riesce ad arrivare anche ad una complessità dell'ordine di $ O(n^{2.807}) $. L'algoritmo inoltre richiede di specificare non solo il numero di cluster, ma anche probabilità di \textit{restart} e la lunghezza massima del percorsi nelle random walk.

\item [DB-CSC] è un algoritmo di clustering di grafi con attributi, basato sul concetto di densità: esso individua, infatti, regioni dense sia nel grafo che nello spazio degli attributi. In particolare, per ogni nodo $ v $ vengono definiti la $ \epsilon \mdash neighborhood $, cioè l'insieme dei vicini che distano al massimo $ \epsilon $ da $ v $, e la $ k\-neighborhood $ cioè l'insieme dei vicini che raggiungono $ v $ in  $ k $ passi al massimo. L'intersezione tra $ \epsilon \mdash neighborhood $ e $ k \mdash neighborhood $ definisce il \textit{vicinato locale} di un nodo. Queste definizioni sono essenziali perché l'algoritmo considera un cluster come un insieme di vertici $ O $ tali per cui, (1) ogni vertice $ v \in O $ ha un vicinato locale molto numeroso (cioè la cui cardinalità è maggiore di $ minPts $, definito dall'utente) e (2) per ogni coppia di punti in $ O $ esiste un cammino che li collega.
Successivamente viene costruito un grafo \textit{arricchito} in questa maniera: se due nodi del grafo originale hanno attributi simili e sono connessi da $ k $ archi al massimo (ovvero, se appartengono allo stesso vicinato locale) allora verranno collegati nel grafo arricchito. Se una componente connessa del grafo arricchito contiene un singolo $ (minPts-1) \mdash core$\footnote{Dato un grafo $ G $, un $ c \mdash core $  è un sottografo di G connesso e massimale in cui tutti i vertici hanno un grado maggiore o uguale a $ c $} che copre ognuno dei suoi nodi, allora si dimostra che quella componente è una comunità secondo la definizione sopracitata; se, invece, una componente è composta da più core lo stesso procedimento si applica ad ognuno di essi in maniera ricorsiva: quando un core contiene al suo interno un unico $ (minPts-1) \mdash core $ che copre tutti i suoi nodi allora esso è una comunità.
Questa procedura viene generalizzata per individuare i cluster in ogni sottospazio degli attributi originali.

\item[CESNA] è un algoritmo probabilistico generativo per l'identificazione di comunità non disgiunte (\textit{overlapping}) su grafi con attributi. Le tre variabili principali sono il grafo $ G $, l'insieme di attributi $ X $ e la funzione di membership $ F $ che indica, per ogni nodo, qual è il grado di appartenenza ad una certa comunità. Le prime due ($ G $ e $ X $) sono le variabili osservate, mentre la $ F $ è la variabile latente, da inferire. Una volta individuate le variabili, vengono definiti due modelli generativi in funzione di $ F $. Il \textit{modello degli archi della rete}, stabilisce qual è la probabilità che due nodi sono collegati tra loro in funzione della loro membership: essi avranno un'alta probabilità di essere collegati se condividono un elevato numero di comunità. Il \textit{modello dei nodi degli attributi}, stabilisce qual è la probabilità che un certo nodo $ u $  possieda un attributo $ k $\footnote{CESNA utilizza attributi binari}. A questo punto viene introdotta un'altra variabile latente (anch'essa da inferire): il peso $ W $ che l'attributo $ k $ presenta all'interno di una comunità. Se il nodo $ x $ partecipa a un certo numero di cluster  per cui la significatività di $ k $ è alta, allora è molto probabile che $ x $ abbia quell'attributo. Questo modello fa in modo che i nodi che appartengono alla stessa comunità probabilmente condividono gli stessi attributi.\\
Infine, viene costruito il modello \textit{combinato} che, in estrema sintesi, rappresenta la probabilità di avere un certo grafo $ G $ e un insieme di attributi $ X $ conoscendo la funzione di membership $ F $ e il set di pesi $ W $: $ P(G, X | F, W) $. Si suppone ora che questo modello generi i dati in input all'algoritmo e quindi, per spiegarli e giustificarli al meglio, si deve determinare la coppia $ \hat{F} $ e $ \hat{W} $ che massimizza il $ log P(G,X | F,W) $.\\
A questo punto poiché ogni nodo è etichettato da un vettore di gradi di appartenenza alle varie comunità: se ogni elemento del vettore supera un certa soglia $ \delta $ allora esso appartiene alla comunità corrispondente.\\
La complessità computazionale è lineare nel numero dei nodi $N$, degli archi $E$ e nel numero di attributi $K$: $ O (E + NK) $.

\item[BAGC] è un algoritmo probabilistico generativo per il clustering di grafi con attributi. Idealmente, BAGC attribuisce ad ogni possibile clustering sui dati una probabilità, cosicché il miglior clustering è semplicemente quello più verosimile, che massimizza la probabilità. Nonostante la semplicità concettuale di tale approccio, esplorare interamente lo spazio dei clustering possibili è impraticabile, e l'algoritmo propone una soluzione approssimata. Il primo passo è individuare una distribuzione parametrica che approssimi la probabilità congiunta tra il grafo considerato e i possibili clustering, modellando separatamente l'etichetta di cluster, gli attributi e gli archi per ciascun vertice. Avendo espresso il clustering come un problema di ottimizzazione---trovare il massimo a posteriori della divisione in cluster condizionatamente al grafo e agli attributi dati---devono essere caratterizzate le condizioni di stazionarietà, per potersi arrestare nella ricerca iterativa del valore ottimo. La soluzione approssimata è quindi raggiunta con un metodo variazionale.

\item[ROCK] è un tecnica gerarchica (bottom-up) per il clustering overlapping su attributi categorici. Per raggruppare gli oggetti del dataset, l'algoritmo non utilizza una semplice funzione di similarità (come l'indice Jaccard) poiché questa può rivelarsi non conveniente in molte situazioni. Intuitivamente, si pensi al caso in cui due punti sono \textit{distanti} tra loro ma presentano un insieme di \virgolette{vicini} molto simili: una misura standard non considererà quest'ultimo aspetto e un algoritmo gerarchico non deciderà mai di raggruppare insieme i due punti. Per ovviare a questo problema, gli autori propongono una misura \textit{globale} che consideri non solo i punti in sé ma la distribuzione del loro \textit{vicinato}. Si definisce, quindi, il numero di \textit{link} tra due oggetti come il numero di vicini che hanno in comune\footnote{In generale il numero di link è calcolato su due insiemi di punti ed indica, semplicemente, il numero di vicini comuni a tutti gli elementi degli insiemi}; inoltre, due oggetti sono detti \textit{vicini} se la loro distanza non supera una certa soglia definita dall'utente. In questa maniera le decisioni sul raggruppamento dei due punti verranno prese in relazione ad una misura di \textit{bontà} che è proporzionale al numero di link. Come in ogni algoritmo gerarchico bottom-up, ROCK inizia col considerare tanti cluster quanti sono gli oggetti del dataset per poi agglomerare i cluster  Successivamente, per ogni cluster $ i $, calcola uno \textit{heap locale} $ q[i] $ che memorizza tutti i cluster $ j $ tali per cui il numero di link tra $ i $ e $ j $ è maggiore di zero; inoltre viene utilizzato anche uno \textit{heap globale} $ Q $ che conserva tutti i cluster ottenuti fino a quel momento. Come ogni algoritmo di clustering bottom-up, genera inizialmente tanti cluster quanti sono i punti del dataset e, ad ogni iterazione, raggruppa i due cluster che massimizzano la misura di bontà. La complessità computazione è $ O(m_{a}n^{2}) $ dove $ m_{a} $ è il numero medio dei vicini di un punto.


\end{description}
\section{Preparazione dei dati}
\label{preparazione_dati}
I profili utente da noi raccolti si sono rivelati sorprendentemente inaccurati, disseminati di dati inverosimili e omissioni. Purtroppo, la cattiva qualità è una caratteristica dei dati reali, in cui il rumore, le imprecisioni della rilevazione e l'informazione mancante pregiudicano il successo del data mining. Per questo motivo, la qualità del clustering dipende tanto dalla adeguatezza della tecnica impiegata quanto dall'abilità dell'analista di preparare i dati per l'analisi.
\subsection{Rimozione degli outlier}
Un outlier è un individuo che devia significativamente dal resto della popolazione; contrariamente al rumore, che esprime le naturali differenze tra le persone, l'anomalia è generata da un meccanismo e risponde a regole radicalmente diverse da quelle valide per gli altri individui. La ricerca degli outlier è spesso un fine in sé, ma in questo lavoro siamo interessati unicamente a prevenire le distorsioni numeriche che un outlier produce. Identificare un individuo atipico nel contesto delle reti sociali non è semplice, poiché la numerosità delle dimensioni rende insignificanti la distanza e la densità, che sono alla base delle procedure canoniche per l'identificazione delle anomalie (\autoref{subsubsec:outlier}). Inoltre, non è sufficiente basarsi sulla topologia del grafo delle amicizie, giacché abbiamo una visione parziale della rete e le connessioni che potrebbero contribuire ad integrare un individuo sono spesso inosservate; questo limite si accentua ulteriormente nei campioni estratti dal dataset completo. Infine, anche i like possono essere fuorvianti, poiché risultano empiricamente molto sparsi: come discuteremo in \autoref{subsubsec:discretizzazione}, la popolarità media di un singolo like è estremamente bassa, meno di 4 utenti su oltre 25000. Ci siamo pertanto limitati a ricercare gli utenti che avessero un profilo platealmente abnorme. A tal fine, abbiamo studiato la dissimilarità di ciascun individuo dalla media della popolazione, trasformando così un problema multivariato, rispetto al numero di attributi di ciascun individuo, in uno univariato. A questo punto è possibile applicare al vettore di distanze una tecnica arbitraria di identificazione degli outlier: test Chi-quadro, gaussiano, Local-Outlier-Factor etc. Partendo da questa selezione, abbiamo poi ispezionato individualmente i candidati e rimosso quelli che, a nostro giudizio, fossero effettivamente incompatibili con la popolazione.
\subsection{Imputazione dei missing values}
A ciascun utente sono associati due tipi di proprietà: il profilo e i like. I like sono ovviamente variabili da un individuo all'altro, né a senso trattare l'assenza di un \textit{mi piace} su un certo contenuto come valore mancante. In generale, parliamo quindi di valori mancanti solo per i dati di profilo, dove sono raccolte le informazioni biografiche dell'utente. Una ulteriore ragione per dare maggior peso all'assenza di informazione di profilo rispetto ai like è il ruolo che questi dati rivestiranno nell'analisi. Mentre i like sono fondamentali per produrre il clustering, gli attributi di profilo sono decisivi per interpretarlo. Una volta tracciati i cluster, attraverso i like siamo in grado di capire quali interessi accomunano le persone; ma, assumendo di non avere altre fonti di informazione, senza i dati del profilo non è possibile caratterizzare questi gruppi, descrivere con le variabili della segmentazione tradizionale le persone che ne fanno parte, ed il risultato perde significativamente valore.\\
Tra gli algoritmi che abbiamo selezionato, solo Cesna \cite{cesna} è insensibile ai missing value; per tutti gli altri è stato necessario produrre una versione completa dei dati. In questa fase, abbiamo fatto ricorso a tre soluzioni: rimuovere l'individuo con valori mancanti (la riga del dataset), rimuovere integralmente l'attributo (la colonna del dataset) o inferire il valore laddove assente. La prima soluzione è consigliabile quando l'individuo è così povero di informazione da non produrre alcun beneficio all'analisi. La seconda soluzione, rimuovere integralmente una dimensione, è necessaria quando l'attributo è mancante per una significativa porzione della popolazione: in questo scenario, rimuovere gli individui privi dell'attributo sarebbe un spreco di dati e non è nemmeno disponibile sufficiente informazione reale per inferire i valori mancanti. Dalle statistiche in \autoref{table:missing_values} si nota che gli attributi \textit{orientamento sessuale} e \textit{stato sentimentale} sono specificati da una estrema minoranza di utenti, certamente troppo pochi per applicare una procedura di imputazione, e sono stati pertanto rimossi dal dataset.
\begin{table}[h]
\resizebox{0.9\textwidth}{!}{\begin{minipage}{\textwidth}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | }
\multicolumn{1}{c}{genere}
 &  \multicolumn{1}{c}{età}
 & \multicolumn{1}{c}{\specialcell[t]{luogo di\\nascita}}
 & \multicolumn{1}{c}{\specialcell[t]{luogo di\\residenza}}
 & \multicolumn{1}{c}{educazione}
 & \multicolumn{1}{c}{\specialcell[t]{orientamento\\sessuale}}
 & \multicolumn{1}{c}{\specialcell[t]{stato\\sentimentale}}
 \tabularnewline
\cline{1-7}
0.78\% & 32.53\% & 28.02\% & 23.64\% & 24.29\% & 99.13\% & 99.84\%\tabularnewline
\cline{1-7}
\end{tabular}
\caption{Distribuzione dei valori mancanti per attributo}
\label{table:missing_values}
\end{minipage} }
\end{table}\\
In ultimo, gli attributi non specificati possono essere dedotti tramite imputazione, ma la procedura da adottare dipende dalla natura del missing value. Alla radice dell'informazione incompleta vi sono cause disparate: un profilo sostanzialmente inattivo, l'irrilevanza dell'informazione per l'utente o semplicemente il desiderio di privacy. Richiamando la terminologia introdotta in \autoref{subsec:missing_value}, inattività e irrilevanza ricadono nella tipologia MCAR, in quanto non vi è alcuna connessione tra il valore dell'attributo mancante e il fatto che non sia stato inserito. La riservatezza invece è generalmente MNAR, poiché dobbiamo conservativamente assumere che il non pubblicare una informazione possa dipendere dal valore dell'attributo per l'utente considerato. I missing value nel nostro dataset sono quindi anche MNAR, e per essi non esiste una procedura univoca di inferenza. Abbiamo deciso pertanto di applicare una tecnica di imputazione ad hoc che faccia leva sull'omofilia nella rete, sul fatto che gli amici di un individuo sono naturalmente selezionati tra i più simili ad esso all'interno della popolazione. Per ciascun individuo con missing value, abbiamo considerato gli utenti a cui è connesso nel grafo delle amicizie e stimato il valore dell'attributo mancante da quello dei suoi vicini. A questo punto, la stima può avvenire con uno dei metodi definiti in letteratura: media, hot-deck, regressione etc. Tuttavia, quando un individuo non ha un vicinato sufficientemente ampio, o i vicini a loro volta non possiedono l'attributo mancante nel soggetto, l'imputazione non è affidabile. Per questa ragione, abbiamo deciso di escludere dal dataset un utente se: (1) il numero di attributi di profilo mancanti è maggiore della metà, e (2) se presenta un valore mancante per un certo attributo che, a sua volta, non è specificato da oltre la metà dei suoi vicini. Con le due fasi di rimozione dei dati impuri, la dimensione del dataset è drasticamente calata; abbiamo quantificato la perdita di informazione in \autoref{table:cleaned_dataset_statistics}.
\begin{table}[h]
\centering
\begin{tabular}{l | c | c | c |}
 \multicolumn{1}{c}{} 
 & \multicolumn{1}{c}{nodi}
 & \multicolumn{1}{c}{archi \textit{friend}}
 & \multicolumn{1}{c}{archi \textit{like}}
 \tabularnewline
\cline{2-4}
originale (con MV) & 25853 & 975\hspace{2pt}382 & 4\hspace{2pt}391\hspace{2pt}494 \tabularnewline
\cline{2-4}
completo (senza MV) & 16336 & 710\hspace{2pt}711 & 3\hspace{2pt}131\hspace{2pt}019 \tabularnewline
\cline{1-4}
decremento assoluto & 9517 & 264\hspace{2pt}671 & 1\hspace{2pt}260\hspace{2pt}475 \tabularnewline
\cline{2-4}
decremento percentuale & 36.81\% & 27.14\% & 28.7\% \tabularnewline
\cline{2-4}
\end{tabular}
\caption{Perdita di informazione durante l'imputazione dei valori mancanti}
\label{table:cleaned_dataset_statistics}
\end{table}\\
%Missingness by attribute
%interested_in 0.9913379
%gender 0.007874636
%hometown 0.280652
%location 0.2375384
%birthday 0.3262855
%relationship_status 0.9983857
%education 0.2437594
%missingness for education
%HS			College		GR Sc		
%0.3231751 	0.4175526 	0.9296795
%True missing
%HS			College		GR Sc
%0.0794157	0.01204819	0
Si noti che anche la trasformazione degli attributi può produrre missing value. Nel passare dai nomi dei luoghi alle coordinate geografiche, non è stato talvolta possibile identificare città e paesi inseriti dagli utenti, per i quali si è dovuto inferire un valore categorico riconoscibile e convertire quest'ultimo.
\subsection{Trasformazione dei dati}
Come anticipato in \autoref{sec:contesto}, ciascun algoritmo richiede una precisa formulazione dei dati per poter essere applicabile; quando i dati non soddisfano tale formulazione, devono essere trasformati. Una frequente ragione di incompatibilità fra dati e tecniche di clustering è il tipo di dato: numerico o categorico. Ad esempio non sarebbe possibile sottoporre dati categorici a \textit{k-means}, in quanto su di essi non è definita la distanza euclidea, che k-means usa internamente. Per questo motivo, abbiamo scritto dei convertitori che eseguono la trasformazione del dataset, dal formato a grafo in cui i dati sono conservati alla formulazione specifica di ciascun algoritmo. Limitatamente al tipo di dato supportato, LAC, MOC e ORCLUS sono numerici, mentre NetClus, LatentNet, CESNA, e BAGC sono categorici [DA COMPLETARE]. Poiché il dataset contiene entrambi i tipi di dato, si è resa in ogni caso necessaria una conversione. 
\subsubsection{Conversione numerica dei like}
\label{subsubsec:conversione_like}
Per convertire una variabile categorica in numerica, l'approccio più consueto è generare tanti attributi booleani quanti sono i possibili valori nel dominio della variabile. Si consideri l'attributo \textit{genere}, il cui dominio ammette due valori: maschio e femmina. Per passare ad una codifica numerica dovremmo definire due nuovi attributi, che sostituiscano quello esistente: \textit{M} e \textit{F}. A questo punto, un individuo di sesso maschile avrà \textit{vero} o 1 nella dimensione \textit{M} e \textit{falso} o 0 in \textit{F}. Questa codifica è chiaramente ridondante e può essere semplificata, come è accaduto per i like. A ciascun like, o meglio ciascun contenuto su cui si è fatto like, è stata associata una singola variabile booleana che risponde alla domanda ``Ha l'utente espresso \textit{mi piace} su questo contenuto?''. \`E chiaro che la scelta dei valori numerici corrispondenti a \textit{vero} e \textit{falso} è completamente arbitraria ma non ininfluente, come discuteremo nel prossimo paragrafo.
\subsubsection{Normalizzazione}
\label{subsubsec:normalizzazione}
Gli algoritmi numerici, tanto quelli probabilistici quanto quelli basati sulla distanza, fanno spesso ricorso alla distanza euclidea per confrontare gli individui. Quando gli attributi possiedono intervalli di valori non comparabili---per esempio uno molto ampio, come l'età, ed uno molto compatto, come i like precedentemente trasformati---la differenza tra due individui limitatamente al primo attributo sarà determinante nel calcolo della distanza complessiva. Per questa ragione sono essenziali le tecniche di normalizzazione, e i metodi considerati in questo lavoro sono la normalizzazione \mbox{\textit{min-max}} e \mbox{\textit{z-score}}. Non esiste un approccio preferibile in ogni circostanza e la scelta dipende dai dati da normalizzare, ma influisce certamente sulla qualità dei risultati del clustering \cite{visalakshi2009}. Abbiamo riscontrato tuttavia che la presenza di outlier incide notevolmente nella normalizzazione min-max, che tende a comprimere i dati non anomali in un intervallo ristretto di valori. Per questa ragione abbiamo applicato la normalizzazione z-score, con l'eccezione degli algoritmi come MOC che accettano un dataset in formato sparso. Per questi ultimi, la possibilità di comprimere la dimensione del dataset dipende dal numero di elementi nulli nella matrice dei dati, la cui trasformazione tramite z-score renderebbe addirittura preferibile la rappresentazione densa. Le tecniche di normalizzazione considerate hanno imposto la rimozione degli attributi a varianza nulla, ovvero quelli che assumono lo stesso valore sull'intera popolazione; poiché tali attributi non contribuiscono al raggruppamento, la loro eliminazione non pregiudica la qualità del clustering.
\subsubsection{Conversione numerica dei luoghi}
\label{subsec:conversione_luoghi}
La trasformazione numerica applicata ai like è valida, ma ha lo svantaggio di aumentare la dimensionalità dei dati ed in alcuni casi specifici è possibile fare leva sulla semantica degli attributi per ottenere migliori risultati. Il principale limite delle variabili categoriche è che non esiste alcuna sfumatura nel confrontarle: o sono esattamente uguali o sono diverse. Questa proprietà, applicata ai luoghi fisici, è controintuitiva, giacché siamo naturalmente portati a misurare la dissimilarità tra due luoghi in funzione della distanza che li separa. Il \textit{geocoding} è il processo che porta alla determinazione delle coordinate geografiche di un luogo---espresse in longitudine e latitudine---a partire da altre informazioni geografiche, come l'indirizzo o il codice di avviamento postale. Dal nome del luogo di nascita e residenza, contenuto nel profilo Facebook, siamo arrivati alle sue coordinate appoggiandoci alle Geocoding API di Google. A questo punto, però, è necessaria un ulteriore trasformazione: infatti un qualsiasi algoritmo di cluster considererà le due variabili separatamente anche se queste rappresentano uno stesso attributo. È stato, quindi, necessario fare in modo che la latitudine e la longitudine venissero trasformate in un unico numero (che abbiamo chiamato \textit{ODC}, \textit{One-Dimensional Coordinates}) con una tecnica che preservi le distanze originali tra i luoghi. A questo proposito, abbiamo utilizzato lo scaling multidimensionale [cit]: una tecnica di analisi statistica che, partendo dalla matrice delle distanze tra oggetti $ D $ dimensionali, assegna, ad ogni oggetto, una coordinata a $ \tilde{D} < D $ dimensioni in modo tale che venga preservata --- approssimativamente --- la distanza euclidea originale. Quindi, abbiamo calcolato la matrice delle distanze tra tutte le città del dataset, utilizzando le funzionalità dello Statistics ToolBox di MATLAB per i metodi di scaling (metrico) multidimensionale: ponendo $ \tilde{D} = 1 $ siamo riusciti ad ottenere l'ODC per ogni luogo del dataset.
\subsubsection{Discretizzazione}
\label{subsubsec:discretizzazione}
La trasformazione dei like da variabile categorica a numerica ha sensibilmente cambiato la struttura del problema. Il numero di like unici nel dataset completo è di poco inferiore al milione di elementi; poiché ciascun like formerà una nuova dimensione, la complessità del problema esplode rapidamente e pochissimi algoritmi sono capaci di operare in uno spazio così sparso. Un altro dato di interesse è la popolarità dei singoli like: benché esistano oggetti largamente condivisi (oltre 200 like sono condivisi da più di mille utenti), in media un singolo like è stato espresso da meno di 5 utenti; pertanto il potere di aggregazione dei like in questa formulazione è prevedibilmente basso. Analogamente alla discretizzazione delle variabili numeriche, per le variabili categoriche esiste la gerarchia concettuale, tramite la quale è possibile ridurre la dimensionalità del problema risalendo dai concetti più precisi, i like, a quelli più generali, le categorie tematiche in cui i like possono essere classificati. Facebook organizza i like in una fra oltre 200 categorie, che è poi inclusa nella descrizione del like nel file Json del profilo utente. L'uso delle categorie è necessario per tutti gli algoritmi che non accettano e non sfruttano internamente una formulazione sparsa dei dati (ad esempio LAC). In quei casi infatti non è possibile trarre vantaggio dal gran numero di elementi nulli---i like che un utente non ha mai condiviso---all'interno del dataset, la cui dimensione su disco ed in memoria non è tollerabile. In generale, le categorie sono estremamente vantaggiose per una prima analisi dei dati, con la quale identificare approssimativamente i fattori di aggregazione degli utenti. A tal punto, le categorie più rilevanti possono essere nuovamente dettagliate nei singoli like, ed il clustering ripetuto in uno sottospazio del dataset originale a granularità più fine.\\
Per gli algoritmi categorici si è resa necessaria una discretizzazione delle variabili numeriche, come l'età. [NICOLA]
\section{Esecuzione degli algoritmi}
\label{esecuzione_algoritmi}
Il primo passo dell'analisi sperimentale è stato la generazione dei campioni del grafo degli utenti. Per studiare le prestazioni degli algoritmi al variare del volume dei dati, abbiamo creato dataset progressivamente più grandi: 1500, 3000, 6000 e 11000 nodi. Di ciascun campione abbiamo misurato gli indici topologici descritti in \autoref{section:indici_e_misure}, che serviranno a studiare l'effetto della struttura del grafo sulle prestazioni degli algoritmi. I grafi così costruiti sono stati sottoposti ai convertitori scritti per ciascun algoritmo; laddove necessario, è stata applicata la normalizzazione, la discretizzazione e l'imputazione dei valori mancanti. Infine sono stati eseguiti gli algoritmi.\\
Descriviamo adesso per ciascun algoritmo i passi necessari a produrre i risultati sperimentali e le complicazioni che sono emerse.
\begin{description}
\item[LAC] è una tecnica per dati numerici, e pertanto si è resa necessaria la trasformazione di ogni variabile categorica: genere, luogo di nascita e residenza, orientamento sessuale, stato sentimentale, like. Come descritto in \autoref{subsec:conversione_luoghi}, per i luoghi abbiamo utilizzato le coordinate monodimensionali; per i like e per il genere, che nel nostro dataset assume due soli valori, abbiamo invece adottato una trasformazione in attributi binari.
%Un'ultima peculiarità, il dataset per LAC deve essere orlato di una colonna contenente l'etichetta di classe, se disponibile, per ciascun elemento; in assenza di \textit{ground-truth}, la colonna deve comunque essere aggiunta ma può contenere dati arbitrari. Questa informazione sarà utilizzata per presentare all'utente la matrice di confusione, tramite la quale valutare l'accuratezza del clustering.
L'algoritmo è scritto in linguaggio C e legge da file una tabella che sarà poi interamente materializzata in memoria, a prescindere dalla sparsità dei dati. Questa implementazione preclude dataset con numerose dimensioni, per cui la tecnica è limitata all'uso delle categorie di like o al più poche migliaia di attributi. Oltre ai dati, l'algoritmo richiede due parametri: il numero \textit{k} di cluster nella soluzione e l'indicatore \textit{h} del numero di attributi che formeranno il sottospazio di ciascun cluster. Tra i due, \textit{h} è chiaramente di difficile stima, poiché influisce indirettamente sul risultato attraverso la funzione obiettivo. Un inconveniente pratico dell'algoritmo è l'impossibilità di eseguire agevolmente e automaticamente il clustering su un intervallo di valori dei parametri. A causa della rappresentazione in memoria dei dati, cambiare \textit{k} o \textit{h} richiede infatti la ricompilazione del programma\footnote{Poiché in C le matrici sono allocate staticamente, variare a tempo di esecuzione \textit{k} e \textit{h} impone la riscrittura del codice con l'allocazione dinamica delle matrici. In alternativa, gli array di lunghezza variabile (VLA) sono stati introdotti in C99, ma alcuni compilatori di rilievo (Visual Studio) non supportano lo standard.}.
%[Questo forse non è necessario anticiparlo adesso] Secondariamente, nonostante gli autori abbiano offerto le prove teoriche della convergenza dell'approccio adottato \cite{lac}, LAC esibisce tempi di esecuzione estremamente scostanti, pur lavorando sullo stesso dataset e con minime variazioni della parametrizzazione.
% 1286844 byte - 1500 rows
% 2691312 byte - 3000 rows
\item[ORCLUS] similmente a LAC è un algoritmo per dati numerici, ed ha richiesto le medesime operazioni sul dataset. L'implementazione che abbiamo utilizzato per l'analisi sperimentale è contenuta nel package \textit{orclus} per R. Anche in questo caso, la procedura richiede in input una matrice densa\footnote{Siamo tuttavia a conoscenza di un'altra implementazione pubblica nel framework Elki, che supporta dataset in formato sparso.\\ \url{http://elki.dbs.ifi.lmu.de}}. ORCLUS richiede tre parametri all'utente: il numero \textit{k} di cluster, la dimensione \textit{l} del sottospazio specifico di ciascun cluster, il numero \textit{k0} di cluster iniziali, che vengono progressivamente accorpati fino a raggiungere una soluzione della dimensione specificata \textit{k}. In ogni caso, la formulazione binaria dei like genera una matrice estremamente sparsa, che risulta spesso matematicamente incompatibile con il funzionamento dell'algoritmo.
\item[MOC] come nei casi precedenti opera su dati numerici, ma, giacché realizzato in Matlab, è possibile utilizzare un formato sparso per la matrice dei dati sia su file sia in memoria. Le trasformazioni applicate sui dati sono le medesime dei due algoritmi precedenti, ma in questo caso sarebbe possibile codificare direttamente i like senza ricorrere alle categorie. L'algoritmo richiede all'utente due parametri: il numero \textit{k} di cluster ed una stima iniziale dell'appartenenza di ciascun elemento ai cluster della soluzione. Questa matrice di appartenenza approssimata può essere ottenuta dall'esecuzione di un altro algoritmo di clustering, come k-means o clustering gerarchico.
\item[LatentNet] è stato distribuito dagli autori nel package \textit{latentnet} per R. LatentNet opera su grafi con attributi e, per poter essere sottoposto all'algoritmo, il dataset deve essere convertito in un oggetto di tipo \textit{network}. Abbiamo però incontrato da subito uno scoglio invalicabile: sebbene i modelli descritti dagli autori siano potenti, tanto l'impronta in memoria quanto il tempo di computazione richiesto per produrre una soluzione crescono brutalmente con la dimensione del campione, al punto da rendere la tecnica inutilizzabile nel nostro scenario.
\item[NetClus] esige l'esistenza di un arco tra ogni utente e ciascun tipo di entità; per gli attributi di profilo, questo si traduce nell'assenza di missing value. Nonostante l'algoritmo sia calzante per il problema, l'implementazione che abbiamo ottenuto dagli autori è risultata inutilizzabile: le funzioni di ordinamento, usate nel calcolo della probabilità a posteriori del modello costruito da NetClus, sono purtroppo specifiche per il clustering di risorse bibliografiche, ed assumono un modello dei dati composto da articoli, autori, conferenze e aree di ricerca. Cambiare tali funzioni, identificare dei sostituti adeguati e scriverne il codice non avrebbe comunque garantito il raggiungimento dei risultati mostrati nell'articolo di ricerca. Pertanto abbiamo dovuto scartare NetClus.
\item [Fast Unfolding] è un algoritmo su grafi, puramente topologico. L'implementazione che abbiamo utilizzato per l'analisi sperimentale è contenuta nel package \textit{igraph} per R. Non ci sono state complicazioni di nessun tipo, in quanto è necessario disporre solamente del grafo in formato GML.

\item [CESNA] è una tecnica per il clustering overlapping di grafi con attributi categorici. Fa parte della piattaforma SNAP\footnote{http://snap.stanford.edu/}  che mette a disposizione una serie di tool e librerie C++ per l'analisi e la manipolazione di reti di grandi dimensioni. Richiede in input tre file: (1) il grafo, (2) una tabella che descrive quali sono gli identificatori degli attributi di ogni utente, (3) una mappa che associa ad ogni identificatore la coppia $ (attributo, valore) $. L'algoritmo permette di specificare una vasta gamma di parametri: il numero di comunità da individuare, il numero minimo e massimo di comunità nel caso si decida di identificarle automaticamente, il peso da dare agli attributi e alla rete del grafo e, infine, il numero di thread per una versione parallela. È l'unico, tra gli algoritmi selezionati, che consente la presenza dei missing value e, quindi, risulta essere efficiente anche utilizzando i singoli like di un utente. Poiché utilizza dati categorici è stato necessario categorizzare attributi come l'età e le città di nascita e di residenza.  CESNA, infine, consente l'individuazione degli outlier del dataset che vengono, automaticamente, esclusi dal clustering finale.

\item[BAGC] similmente a CESNA opera su grafi con attributi categorici, quindi ha richiesto le medesime trasformazioni sul dataset. L'algoritmo richiede all'utente il numero di cluster da individuare e due file in input: il grafo e una matrice che specifica per ogni utente il set di attributi che lo qualifica. Poiché non sono ammessi i missing value è stato necessario utilizzare le categorie invece dei singoli like di ogni utente. 

\item[DB-CSC] è un algoritmo di clustering di grafi con attributi categorici. Il formato da utilizzare è il GraphML che esplicita sia l'insieme di attributi che le connessioni di ogni nodo. Richiede di specificare i parametri $ \epsilon $, $ k $ e $ minPts $ descritti nel paragrafo X.Y.Z. Come molti altri algoritmi, non consente la presenza di missing value e, quindi, anche in questo caso è stato necessario utilizzare le categorie dei like. Purtroppo l'implementazione JAVA messa a disposizione dagli autori si è rivelata essere davvero inefficiente dal punto di vista computazionale per i dataset a disposizione. Per tanto è stato escluso dall'analisi sperimentale.

\item [Inc-Cluster] è un algoritmo di clustering di grafi con attributi categorici. Richiede all'utente di specificare il numero di cluster, la probabilità di restart e la lunghezza massima dei random walk. Come input necessita della matrice di transizione (in formato sparso) del grafo e di una tabella che specifica, per ogni individuo, gli attributi e i corrispettivi valori. L'implementazione MATLAB messa a disposizione dagli autori è sviluppata appositamente per un dataset particolare: ciò comporta il dover reimplementare l'algoritmo per renderlo capace di accettare un qualunque input. Spinti da questa motivazione e dalle basse performance computazionali\footnote{$ O(n^{2.807}) $}, abbiamo deciso di non utilizzarlo nel nostro framework.

\item [ROCK] è distribuito nel package \textit{cba} di R; si è mostrato in generale molto inefficiente con i dataset a nostra disposizione, individuando cluster di dimensioni eccessivamente grandi. Per questa ragione, seppure l'algoritmo è presente nel nostro framework, abbiamo deciso di non utilizzarlo nell'analisi sperimentale.

\end{description}