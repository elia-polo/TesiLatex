\chapter{Stato dell'arte}
\label{capitolo2}
\thispagestyle{empty}

\noindent
Il termine \textit{clustering} si riferisce ad un insieme di tecniche che permettono la suddivisione di dati in gruppi (o \textit{cluster}) di oggetti simili. In questo capitolo verranno presentate alcune definizioni preliminari, analizzati i passi principali della cluster analysis e approfonditi gli specifici aspetti teorici che hanno portato al raggiungimento degli obiettivi del lavoro di tesi.

\section{Introduzione}
L'\textit{apprendimento automatico} (o \textit{machine learning}) \`e una delle aree pi\`u importanti dell'intelligenza artificiale. Esso si occupa della realizzazione di algoritmi che, partendo da un insieme di dati, portano alla sintesi di nuova conoscenza. Possiamo individuare due grandi tipologie di apprendimento automatico: l'\textit{apprendimento supervisionato} (o \textit{supervised learning}) e l'\textit{apprendimento non supervisionato} (o \textit{unsupervised learning}) \cite{Mitchell1997}. Il primo riguarda la progettazione di sistemi che generano una funzione capace di predire la \textit{classe} di appartenenza di un dato in input sulla base di una serie di esempi gi\`a classificati. Al contrario, nell'apprendimento non supervisionato si progettano algoritmi che riorganizzano le informazioni in ingresso per identificare strutture naturalmente emergenti da esse. L'oggetto di studio di questo lavoro \`e la cluster analysis, che si colloca tra le tecniche di apprendimento non supervisionato. 

\subsection{Cos'\`e la cluster analysis} 
Viviamo in mondo pieno di dati. Ogni giorno le persone s'imbattono in una grande quantit\`a di informazioni che vengono memorizzate per analisi successive. Uno dei modi principali per trattare questi dati \`e quello di classificarli in un insieme di categorie o cluster. La classificazione svolge da sempre un ruolo fondamentale per lo sviluppo scientifico dell'uomo che, per comprendere un nuovo fenomeno, cerca di confrontarlo con altri fenomeni noti basandosi sulla somiglianza tra questi. 
La cluster analysis, che ha come dominio di interesse i dati e non i fenomeni naturali, automatizza proprio questo processo, mettendo a disposizione una serie di strumenti per individuare informazioni nascoste o per dedurre modelli latenti all'interno di complesse strutture informative, che di per s\'e non mostrano nessuna regolarit\`a. 
L'obiettivo principale da raggiungere \`e quello di creare dei raggruppamenti che massimizzino la similarit\`a intra-cluster e, contemporaneamente, minimizzino quella inter-cluster, in modo tale che ogni gruppo esponga delle propriet\`a che lo qualificano in maniera determinante.\\
Le applicazioni della cluster analysis sono molto variegate. Ad esempio, in medicina \`e necessario identificare diversi tipi di tessuto nelle immagini PET \cite{PET}, nelle scienze sociali \`e utile localizzare aree metropolitane con un'alta incidenza di criminalit\`a \cite{crime}, mentre nell'analisi delle reti sociali \`e interessante riconoscere comunit\`a all'interno di un elevato numero di individui \cite{Mishra2007}. Di conseguenza, il concetto astratto di dato assume rispettivamente la forma di una immagine o di una sezione geografica o, ancora, di un nodo all'interno di un grafo.

\subsection{Definizioni preliminari}
Un dataset $ D $ indica un insieme di $ N $ dati (o \textit{tuple}, \textit{istanze}, \textit{oggetti}) eterogenei. Ogni oggetto $ x \in D $ \`e un vettore del tipo $ x = (x_{1},..., x_{m}) $ dove l'i-esimo elemento $ x_{i} $ indica un attributo (o \textit{dimensione}, \textit{propriet\`a}) di $ x $. Un attributo \`e \textit{nominale} (o \textit{categorico}) se ha un dominio finito i cui elementi rappresentano una categoria di appartenenza, oppure \`e detto \textit{numerico} se ha un dominio, in generale, infinito i cui valori sono quantit\`a numeriche \cite{Han2011}.\\
Su due oggetti del dataset \`e possibile definire una \textit{funzione di similarit\`a} che misura quanto sono simili tra loro analizzando il valore degli attributi. La \textit{matrice di prossimit\`a} \`e una matrice simmetrica $ N \times N $ in cui ogni elemento $ (i,j) $ rappresenta la similarit\`a tra gli oggetti $ i $ e $ j $.\\
Un \textit{cluster} \`e un sottoinsieme $ C \subseteq D $ i cui i suoi oggetti sono contemporaneamente simili tra loro e diversi dagli altri (secondo la definizione di similarit\`a). Un \textit{clustering} \`e un insieme di cluster. Pi\`u in particolare, nel \textit{crisp clustering} ogni oggetto appartiene ad uno e un solo cluster; nel \textit{fuzzy clustering}, invece, un oggetto pu\`o appartenere a diverse comunit\`a con un grado variabile mentre nell'\textit{overlapping clustering} un oggetto appartiene a diverse comunit\`a con lo stesso grado di appartenenza.\\
La \textit{matrice di incidenza} \`e una matrice simmetrica $ N \times N $ in cui ogni elemento $ (i, j) $ \`e uguale a 1 se $ i $ e $ j $ appartengono allo stesso cluster, oppure \`e pari a 0 se $ i $ e $ j $ appartengono a cluster diversi.\\
Un dataset pu\`o contenere oggetti, chiamati \textit{outlier}, che non rispettano il comportamento generale o il modello sottostante dei dati. Gli outlier possono falsare sensibilmente il risultato degli algoritmi di clustering tanto da dover intervenire con una loro eliminazione tramite tecniche statistiche o tramite metodologie basate sul concetto di distanza \cite{Han2011}.

\subsection{Procedura di clustering}
Per avere una idea d'insieme della cluster analysis analizziamone i quattro passi fondamentali.

\begin{description}

\item[Selezione o estrazione degli attributi] La \textit{feature selection} permette di selezionare un certo numero di propriet\`a rilevanti da un insieme di candidate, mentre la \textit{feature extraction} utilizza delle trasformazioni per generare nuovi attributi da quelli originali \cite{PatternAnalysis}. L'assunto fondamentale alla base di queste tecniche \`e che i dati possono contenere molte propriet\`a ridondanti o non rilevanti: le dimensioni ridondanti sono quelle che non aggiungono nessuna informazione in pi\`u rispetto a quelle gi\`a selezionate, le feature non rilevanti non forniscono nessuna informazione in qualunque contesto. Gli effetti benefici di queste tecniche sono immediati: oltre a velocizzare gli algoritmi di mining, a ridurre la dimensione del dataset e a facilitare la presentazione dei risultati, migliorano significativamente la qualit\`a e la granularit\`a del clustering. Infatti, la suddivisione dei dati in gruppi omogenei \`e molto pi\`u semplice e \virgolette{nitida} in uno spazio con poche dimensioni; inoltre, di frequente, i cluster emergono solamente in un sottoinsieme di attributi restando \virgolette{nascosti} in uno spazio ad elevata dimensionalit\`a (un gruppo di individui potrebbe essere simile per et\`a, genere e luogo di nascita ma non per religione e citt\`a di residenza).\\
%Entrambe le metodologie sono cruciali per l'efficacia dei risultati del clustering in quanto riducono notevolmente il carico del dataset e semplificano il processo di design/selezione degli algoritmi. 
Questo \`e inoltre un passaggio fondamentale per evitare la cosiddetta \textit{curse of dimensionality}, cio\`e una serie di complicazioni che si verificano con dati ad elevata dimensionalit\`a. Ci\`o introduce due ordini di problemi: il primo riguarda la presenza di attributi non rilevanti ai fini del clustering che eliminano ogni tendenza al raggruppamento; il secondo, invece, comporta il fatto che un qualsiasi punto del dataset risulta praticamente equidistante dal punto pi\`u vicino e da quello pi\`u lontano. Questo \`e un problema determinante perch\'e, senza una oggettiva funzione di distanza, si perde il metro di giudizio per identificare due oggetti simili e individuare cluster significativi. Per un approfondimento su queste e altre problematiche si veda \cite{Kriegel09}, \cite{Beyer99} e \cite{Aggarwal01}.
\item[Progettazione o selezione degli algoritmi di clustering] Questo passo \`e usualmente accompagnato dalla scelta di una misura di similarit\`a e di una funzione obiettivo: in particolare un algoritmo di clustering deve utilizzare la funzione di similarit\`a per giudicare simili o dissimili due oggetti e deve disporre di una funzione obiettivo per decidere qual \`e il miglior raggruppamento. Ci\`o rende il problema delle ricerca dei cluster un problema d'ottimizzazione, ben definito matematicamente e con variegate soluzioni in letteratura.
\item[Validazione del clustering] Su uno stesso dataset, ogni algoritmo di clustering pu\`o sempre generare un raggruppamento, indipendentemente dal fatto che questo sia effettivamente significativo. Inoltre, algoritmi differenti portano a soluzioni differenti e, persino per lo stesso algoritmo, una diversa scelta dei valori dei suoi parametri pu\`o influire sul risultato finale. Per questa ragione \`e utile disporre di strumenti (detti \textit{indici di validit\`a}) che permettano di giudicare oggettivamente il grado di confidenza dei risultati del clustering. Essi si dividono in esterni ed interni. I primi si basano sulla presenza di un clustering specificato a-priori (\textit{ground-truth}), che serve per valutare la qualit\`a dell'algoritmo misurando quanto la sua soluzione si discosta dalla ground-truth. Gli indici interni invece non si basano su nessuna informazione esterna, ma esaminano la struttura del clustering direttamente dai dati originali. Per un compendio approfondito sugli indici di validit\`a si faccia riferimento a \cite{Gordon99}, \cite{Jain88}.
\item[Interpretazione dei risultati] L'ultimo obiettivo della cluster analysis \`e fornire agli utenti una rappresentazione immediata e facilmente comprensibile del risultato ottenuto, in modo tale da poter procedere con una valutazione soggettiva dell'utente: infatti, il ricorso ai soli indici non \`e mai sufficiente per misurare la qualit\`a del clustering. L'affermazione \textit{clustering is in the eyes of the beholder} \cite{Moise2009} conferma la tesi secondo cui la decisione definitiva sulla coerenza del risultato deve essere presa dall'osservatore. Verranno quindi consultati esperti del dominio di interesse per interpretare, convalidare e, semmai, trasformare le soluzioni proposte dagli algoritmi.
\end{description}

\subsection{Segmentazione di mercato}
La \textit{segmentazione di mercato} \`e definita come quel processo che permette di partizionare un ampio mercato in piccoli gruppi di clienti (o \textit{segmenti}) caratterizzati da bisogni omogenei e simili comportamenti d'acquisto \cite{Myers96}, \cite{Smith56}. Conoscere questi segmenti \`e utile ai dirigenti aziendali non solo per creare soluzioni ad-hoc per ogni tipologia di cliente ma anche per definire con elevata precisione strategie competitive di mercato. I segmenti sono costruiti sulla base di caratteristiche demografiche e geografiche, considerando anche le vendite avute in passato e le informazioni provenienti da sondaggi sul prodotto \cite{Anderson03}, \cite{Aaker13}.\\
Con l'avvento dei Big Data si assiste a due fenomeni fondamentali: da un lato \`e molto pi\`u semplice individuare i propri clienti grazie all'utilizzo dei Social Network, dall'altro, ogni cliente \`e descritto da un elevato numero di attributi, come quelli recuperabili dal suo profilo Facebook. Questa esplosione di informazioni rende pi\`u appetibile e potenzialmente pi\`u proficuo il processo di segmentazione, ma aggiunge, d'altro canto, un ulteriore livello di difficolt\`a. Gestire un grande insieme di clienti e individuare informazioni nascoste al suo interno diventa davvero difficoltoso senza l'ausilio di un procedimento automatico. \`e proprio a questo punto che la cluster analysis interviene a favore della segmentazione di mercato, fornendo una serie di strumenti che agevolano l'identificazione dei caratteri emergenti dalla base di dati aziendale.\\
Vediamo ora, nello specifico, quali sono state le tecniche studiate e utilizzate per il raggiungimento degli obiettivi prefissati.

\section{Data Preprocessing}
Le basi di dati odierne, a causa della loro elevata dimensione, contengono spesso informazioni altamente rumorose, mancanti o incoerenti. Inevitabilmente, la bassa qualit\`a dei dati porta ad avere una bassa qualit\`a dei risultati di mining. Con il termine \textit{data preprocessing} s'intende una serie di tecniche e strumenti che aiutano a migliorare la qualit\`a dei dati: il \textit{data cleaning} (per rimuovere rumore o correggere inconsistenze), il \textit{data integration} (per combinare dati provenienti da fonti diverse e creare un'unica sorgente coerente), il \textit{data reduction}  (per ridurre la dimensione della base di dati aggregando o eliminando caratteristiche ridondanti) e il \textit{data transformation} (che consente di trasformare i dati da un formato di partenza ad uno di destinazione).
Tutte queste tecniche vengono usate in maniera congiunta per ottenere una base di dati coerente che possa essere usata con efficacia nel processo di clustering.\\
In particolare, tra queste tecniche spiccano tre fondamentali metodologie: la selezione di attributi rilevanti, il rilevamento e la rimozione di outlier e l'inferenza dei valori mancanti.

\subsubsection{Feature Selection}
Come gi\`a affermato nei paragrafi precedenti, la feature selection permette di selezionare un certo numero di propriet\`a rilevanti da un insieme di candidate.\footnote{In questo lavoro non abbiamo ritenuto vantaggioso utilizzare tecniche di feature extraction [PCA] [simili] in quanto queste comportano una trasformazione delle dimensioni originali e di conseguenza portano inevitabilmente alla perdita di informazioni necessarie alla caratterizzazione della semantica dei risultati.}
Gli algoritmi di feature selection possono essere classificati secondo la tipologia di output che generano: da un lato vediamo quelli che producono una lista di attributi ordinati in base ad un peso di rilevanza; dall'altro quelli che escludono gli attributi considerati insignificanti. In particolare, ogni algoritmo pu\`o essere supervisionato o non supervisionato a seconda del fatto che sia disponibile o meno una informazioni esterna: l'\textit{etichetta di classe}. Basandosi su questa informazione aggiuntiva, \`e possibile determinare quali sono le feature essenziali che spiegano al meglio i valori dell'etichetta di classe: se ci si accorge che i valori di una certa dimensione sono scorrelati da quelli dell'etichetta, allora quella dimensione verr\`a eliminata \cite{hall1999}, \cite{Molina2002}.\\
Tecniche non supervisionate che hanno riscontrato un buon successo teorico e pratico si basano sul clustering degli attributi: il loro output \`e costituito dalla selezione delle dimensioni pi\`u rappresentative di ogni cluster \cite{Mitra2002}. Altri approcci interessanti come \cite{Zhao2007} si basano invece sulla definizione preliminare della matrice di similarit\`a $ S $ del dataset. Una feature \`e detta \textit{coerente} se assegna valori simili alle istanze che sono vicine le une alle altre in  $ S $.
Per un compendio completo su tecniche di feature selection (sia supervisionate che non) si faccia riferimento a \cite{Molina2002}, \cite{Chandrashekar2014}.

\subsubsection{Outlier}
\label{subsubsec:outlier}
Le tecniche di outlier detection vengono utilizzate per identificare e rimuovere osservazioni anomale dai dati.
Le pi\`u semplici sono basate sul concetto di distanza e tentano di individuare gli outlier definendoli come quei punti sostanzialmente pi\`u \virgolette{lontani} dagli altri. Ad esempio, nel DB($\epsilon$, $\pi$), \cite{Knorr97} un punto $ p $ \`e considerato un outlier se, al massimo una percentuale $ \pi $ di punti ha una distanza minore di $\epsilon $ da $ p $.\\
Altri algoritmi lavorano sulla densit\`a \cite{Breunig2000,Breunig1999} (un outlier ha una densit\`a \virgolette{locale} molto pi\`u bassa della densit\`a della maggior parte degli altri punti); altri ancora ricostruiscono preliminarmente un clustering sui dati e definiscono outlier quei punti che appartengono a gruppi di piccole dimensioni \cite{Hodge2004}, \cite{Chandola07}.

\subsubsection{Missing Value}
\label{subsec:missing_value}
In una collezione di dati \`e molto probabile che il valore di certi attributi sia mancante; ci\`o comporta inevitabilmente la necessit\`a di dover adottare una metodologia che gestisca l'assenza di questi valori. Le tecniche di inferenza statistica pi\`u semplici prevedono di sostituire il valore mancante di una variabile con la media totale degli altri valori. Spesso queste tecniche producono un errore di stima aggiungendo un \textit{bias} alla base di dati. Si pensi, infatti, all'inferenza dell'et\`a di un individuo tramite l'et\`a media dei suoi amici: se questo attributo possiede un'alta varianza o \`e mancante in un gran numero di individui allora il valor medio non \`e significativo \cite{rubin1987}. Per ovviare a questi e altri inconvenienti si utilizzano delle tecniche pi\`u complesse che si basano sulla tipologia dei dati mancanti \cite{Donders2006}. Quando la probabilit\`a che una osservazione sia mancante \`e indipendente da ogni altra caratteristica dell'individuo si parla di dati \textit{Missing Completely At Random} (MCAR). In questo caso si riesce a produrre una inferenza oggettiva anche tramite tecniche basilari. Se, invece, la probabilit\`a che una osservazione sia mancante dipende da informazioni non osservate, come il valore dell'osservazione stessa, i dati vengono detti \textit{Missing Not At Random} (MNAR). In questo caso, quindi, la mancanza di una osservazione non \`e completamente casuale ma  dipende da una caratteristica non osservata dell'individuo: ad esempio, in un questionario, un uomo ricco potrebbe non rispondere a domande riguardanti il suo salario. Qui, poich\'e l'informazione mancante (il salario) dipende da qualche variabile non osservata (la ricchezza), \`e molto difficile non introdurre errori di stima con qualsiasi tecnica di inferenza. Infine, quando la probabilit\`a che un valore non sia specificato dipende da altre variabili osservate (ma non dal valore stesso) si parla di dati \textit{Missing At Random} (MAR). In questo caso le tecniche triviali di inferenza spesso producono risultati scorretti \cite{Greenland1995}, \cite{Vach1994}, quindi \`e necessario utilizzare metodologie pi\`u sofisticate. Un esempio \`e dato dall'algoritmo del donatore di minima distanza, in cui si sceglie di sostituire il dato mancante in un individuo con quello presente nell'individuo ad esso pi\`u simile.
Altre metodologie si avvalgono di potenti strumenti statistici (regressione, algoritmi di \textit{expectation-maximization}, inferenza multipla). Per una analisi approfondita si rimanda a \cite{Little1986}, \cite{Schafer1997}, \cite{rubin1987} e a \cite{Kossinets2003} per l'inferenza di dati in reti sociali.


\section{Classificazione degli algoritmi di clustering}
\label{sec:classificazione_algoritmi}
La categorizzazione degli algoritmi di clustering non \`e univoca, e gli algoritmi possono travalicare i confini tra approcci diversi; in questa sezione presentiamo la divisione canonica delle soluzioni al clustering, in base alla natura dei cluster generati e alle tecniche per ottenerli.
\subsection{Metodi gerarchici}
Il clustering gerarchico costruisce una gerarchia di cluster, un albero binario detto \textit{dendrogram} avente alla radice l'intera popolazione e alle foglie i singoli individui. Un nodo intermedio costituisce pertanto un cluster dei propri discendenti, e tagliando l'albero a varie altezze si ottengono soluzioni di diversa dimensione al problema del clustering, con un numero crescente di gruppi man mano che ci si allontana dalla radice.
%Questa tecnica procede per via agglomerativa o divisiva: nel primo caso, l'albero \`e costruito dalle foglie, agglomerando gli individui pi\`u simili in cluster intermedi fino ad ottenere l'intera popolazione; nel secondo, l'intera popolazione \`e iterativamente frammentata in gruppi sempre pi\`u omogenei fino a giungere ai singoli individui.
Il clustering gerarchico offre una flessibilit\`a nativa nel livello di granularit\`a della soluzione; soprattutto, \`e  compatibile con qualsiasi definizione di similarit\`a e conseguentemente applicabile ad ogni tipo di dato. D'altronde, le principali critiche a questa tecnica risiedono nella mancanza di robustezza che rende la soluzione vulnerabile a rumore e anomalie, nell'inabilit\`a di rivedere le scelte effettuate e correggere eventuali errori di classificazione nei cluster intermedi ed infine nella tendenza a formare cluster sferici. Tuttavia, il limite principale del clustering gerarchico, che preclude la sua adozione per problemi di clustering in grande scala, \`e la complessit\`a almeno quadratica. Le implementazioni di maggiore rilievo sono CURE (dati numerici) \cite{cure}, ROCK (dati categorici) \cite{rock}, BIRCH (ottimizzato per dataset di ampie dimensioni) \cite{birch} e Chameleon \cite{chameleon}.
\subsection{Metodi partitivi}
\label{subsec:metodi_partitivi}
Gli algoritmi partitivi dividono i dati in sottoinsiemi privi di una struttura gerarchica. Poich\'e identificare la soluzione ottima dall'enumerazione di tutte le possibili partizioni \`e notoriamente \mbox{NP-difficile}, sono stati sviluppati algoritmi euristici: partendo da una funzione obiettivo, la soluzione \`e costruita iterando un problema di ottimizzazione, riassegnando gli individui tra i cluster al fine di massimizzare il punteggio del clustering finale. La differenza sostanziale tra algoritmi partitivi \`e gerarchici \`e proprio questo meccanismo di rilocazione, che corregge eventuali errori di `classificazione' degli individui.\\
Gli algoritmi partitivi possono essere ulteriormente caratterizzati:
\begin{description}
\item[centroid-based:] specificato il numero K di cluster, gli algoritmi basati sulla distanza producono un vettore di K centroidi---punti arbitrari nello spazio N-dimensionale dei dati---ed un assegnamento tra individui e centroidi, tale che la somma dei quadrati delle distanze tra individui e centroidi sia minima. Questa tipologia di algoritmi risente fortemente della scelta iniziale dei centroidi; inoltre produce cluster di forma sferica. Algoritmi notevoli sono \mbox{\textit{K-means}}, \mbox{\textit{K-medoids}}, \mbox{\textit{Fuzzy c-means}}; limitatamente agli algoritmi che abbiamo considerato per questo lavoro, si veda \cite{lac}.
\item[model-based:] nella visione probabilistica, ogni cluster presente nei dati \`e espressione di una distribuzione parametrica di probabilit\`a; detto altrimenti, i dati sono concettualmente ottenuti campionando una mescolanza di distribuzioni o \textit{mixture model}, e lo scopo del clustering \`e stimare dai dati i parametri di tali distribuzioni latenti. Il criterio di ottimalit\`a \`e pertanto la verosimiglianza (\textit{likelihood}), che esprime la capacit\`a dei parametri del modello di spiegare i dati. L'algoritmo canonico di questa tipologia \`e \textit{Expectation-Maximization} (EM); limitatamente agli algoritmi che abbiamo considerato per questo lavoro si vedano \cite{moc}, \cite{cesna} e \cite{handcock07}.
\item[density-based] l'idea alla base di queste tecniche \`e che un cluster sia una regione sensibilmente pi\`u densa di punti rispetto allo spazio circostante. Conseguentemente, i cluster sono separati da regioni rarefatte dello spazio, ed i punti che ricadono in queste aree sono solitamente rumore o anomalie, che vengono quindi identificati e scartati automaticamente. Poich\'e i cluster si espandono nella direzione lungo la quale la densit\`a cresce, gli algoritmi basati sulla densit\`a scoprono cluster di forma arbitraria; in ultimo, scalano bene con il volume dei dati. Tuttavia questo approccio presenta anche un evidente limite: la densit\`a nello spazio \`e funzione dalla distanza tra i punti, che come discusso nel paragrafo X.Y.Z perde di significativit\`a al crescere delle dimensioni. Il pi\`u noto algoritmo in questa categoria \`e DBSCAN \cite{dbscan}.
\item[grid-based] i metodi discussi precedentemente sono guidati dai dati, in quanto partizionano l'insieme degli oggetti in accordo con la distribuzione dei dati nello spazio. Alternativamente, gli algoritmi grid-based suddividono lo spazio in una griglia multidimensionale a risoluzione variabile; poich\'e le successive operazioni di clustering saranno eseguite sulle celle di tale griglia, le prestazioni sono indipendenti dalla dimensione dei dati ma dipendenti unicamente dal numero di celle in ciascuna dimensione dello spazio quantizzato. I rappresentanti pi\`u noti di questa categoria sono CLIQUE \cite{clique} e la sua evoluzione, MAFIA \cite{mafia}, e OPTIGRID \cite{optigrid}.
\end{description}

\subsection{Subspace Clustering}
\label{subsec:subspace_clustering}
Il \textit{subspace clustering} \cite{Parsons2004},\cite{Moise2009} \`e una estensione del clustering tradizionale che cerca di determinare la presenza di possibili cluster in tutti i sottospazi\footnote{Per una definizione precisa di sottospazio si rimanda a \cite{Vidal}. Qui ci basta intuire che un sottospazio un sottoinsieme degli attributi originali.} possibili dell'insieme di attributi.
Spesso, quando si ha a che fare con dati ad alta dimensionalit\`a, molti attributi risultano essere irrilevanti e possono nascondere l'effettiva presenza di un naturale raggruppamento \cite{Steinbach03}. Se da un lato la feature selection sceglie per ogni cluster lo stesso insieme di attributi rilevanti, il subspace clustering associa ad ogni cluster un diverso sottospazio, ossia una diversa selezione di attributi significativi \cite{Parsons2004}.\\
Ad esempio l'algoritmo $ CLIQUE $ \cite{clique}, uno dei primi algoritmi progettati per la ricerca di cluster nei sottospazi, si basa su una interessante osservazione: una regione che \`e densa in un particolare sottospazio sar\`a necessariamente densa una volta proiettata in un sottospazio di dimensione inferiore. Il contrario, invece, non \`e necessariamente vero. Basandosi su questa intuizione, l'algoritmo parte analizzando sottospazi densi con poche dimensioni che possono solo suggerire la presenza di cluster con dimensioni pi\`u elevate; detto altrimenti, se una regione a $ m $ dimensioni non \`e densa allora non lo \`e neppure una con $ m^{'} > m $ dimensioni.
Questo ragionamento  pu\`o essere utilizzato per identificare unit\`a dense in un qualunque sottospazio, impiegando, quindi, una procedura molto pi\`u efficiente dell'ispezione completa di tutti i possibili sottospazi. 
\subsection{Ensemble Clustering}
E' noto che un singolo problema pu\`o essere risolto con tecniche diverse; ogni tecnica, d'altro canto, proporr\`a una soluzione con una qualit\`a variabile a seconda delle caratteristiche con cui \`e stata progettata. I metodi \textit{ensemble}, invece di adottare un'unica metodologia per la risoluzione di un problema, utilizzano un insieme di strumenti con l'obiettivo di disporre di un ventaglio di soluzioni diverse. Il vantaggio di questo approccio consiste, ad esempio, nel poter scegliere il risultato pi\`u comune (\textit{majority voting}) o nel combinare i differenti output in un'unica soluzione finale. Tipicamente i metodi ensemble sono impiegati nell'ambito dell'apprendimento supervisionato in cui si creano diversi \textit{training set} per la costruzione di altrettanti classificatori. Se da un lato il vantaggio principale risiede nel fatto di ottenere sensibili miglioramenti nelle performance predittive, dall'altro lo svantaggio pi\`u evidente \`e quello di dover interpretare e unificare le singole soluzioni \cite{Dietterich2000}, \cite{Rokach2010}.\\
Nel dominio dell'apprendimento non supervisionato, e in particolare nel clustering, sono stati sviluppati dei procedimenti che ricalcano i concetti appena esposti: invece di utilizzare un solo algoritmo di clustering che produrr\`a, tipicamente, un unico raggruppamento dei dati, vengono adottati un insieme di algoritmi che produrr\`a, quindi, partizionamenti multipli. Il problema di fondo, come di norma, \`e quello di interpretare e combinare le soluzioni. Un approccio di base consiste nell'assegnare un punteggio ad ogni coppia di punti $ i $ e $ j $ del dataset. La coppia ricever\`a un punteggio massimo o minimo a seconda del fatto che tutti gli algoritmi di clustering li abbiano considerati o meno appartenenti allo stesso raggruppamento. In questo modo si dispone, per ogni coppia di punti, di un indice di similarit\`a che pu\`o essere utilizzato come input di un ulteriore algoritmo basato sulla distanza. Si faccia riferimento a \cite{Strehl03}, \cite{Ghaemi2009} per un approfondimento sulle tecniche di ensemble.

\subsection{Clustering di grafi e reti sociali}
\subsubsection{Definizioni}
Un \textit{grafo} \`e una struttura matematica usata per modellare relazioni tra oggetti. Esso pu\`o, ad esempio, rappresentare l'amicizia all'interno di un gruppo persone, oppure la struttura bio-molecolare di DNA e proteine \cite{Trudeau1994}, \cite{Mason07}. Quando si associa ad ogni oggetto (o \textit{vertice}) un insieme di propriet\`a che lo descrivono si parla di grafo con attributi  \cite{inc_cluster}, \cite{bagc}. Particolarmente interessante \`e l'approccio di \cite{inc_cluster} nel gestire un grafo con attributi: se nel dataset \`e presente una coppia attributo-valore $ (a, v) $ allora viene aggiunto, nel grafo originale, un nodo $ n_{a,v} $ che sar\`a collegato a tutti i vertici che presentano il valore $ v $ per l'attributo $ a $. In questo modo si crea un grafo \textit{aumentato} su cui \`e possibile definire funzioni obiettivo e di similarit\`a che tengano conto, contemporaneamente, della struttura topologica del grafo e dei suoi attributi.\\
Una \textit{rete sociale} \`e un grafo costituito da \textit{individui} (o \textit{attori}) connessi da una relazione sociale. Queste reti hanno delle propriet\`a ampiamente studiate in sociologia che rivestono un ruolo fondamentale anche nella cluster analysis\footnote{In questo contesto \`e pi\`u preciso parlare di \textit{comunit\`a} invece che di cluster e di algoritmi di \textit{community detection} invece che algoritmi di clustering.}: \textit{omofilia}, \textit{transitivit\`a} e \textit{clustering locale}. L'omofilia rappresenta la tendenza secondo cui \`e molto probabile che ci sia una relazione sociale tra individui simili \cite{mcpherson2001}, \cite{lazarsfeld1954}, [52]. La transitivit\`a afferma che se due attori condividono un legame sociale con un terzo attore allora \`e verosimile che essi stessi siano tra loro relazionati \cite{white1976}. Il clustering locale, infine, indica la presenza di un'alta densit\`a di relazioni localmente ad alcuni individui della rete: il che modella, ad esempio, il fatto che i nostri amici sono anche amici tra di loro \cite{Watts1998}, \cite{Newman2003}.

\subsubsection{Clustering su grafi}
Per definire una comunit\`a dal punto di vista topologico, ci si pu\`o basare sull'intuizione che al suo interno devono esserci molti pi\`u archi di quelli che ci sono tra essa e il resto del grafo. Ad esempio, le comunit\`a sociali possono essere definite come dei sottogruppi i cui membri sono tutti "amici" tra loro \cite{Luce1949}, e questo, in termini di grafi, corrisponde ad una \textit{clique}, cio\`e ad un sottoinsieme di vertici tutti collegati gli uni con gli altri. Parallelamente, una comunit\`a pu\`o essere definita utilizzando una funzione di \textit{fitness} che misura quanto essa sia coesa internamente. Una delle funzioni pi\`u semplici \`e la \textit{densit\`a intra-cluster}, che esprime il rapporto tra il numero di archi interni ad un sottografo e il numero di tutti gli archi che questo potrebbe avere. Per esempio, se un certo sottografo ha una densit\`a maggiore di una soglia $ \epsilon $ allora pu\`o essere considerato una comunit\`a\footnote{Trovare una clique o sottografi che hanno una densit\`a intra-cluster maggiore di una certa soglia sono entrambi problemi \textbf{NP}-difficili \cite{Bomze99}, \cite{Garey1990}. Dal punto di vista pratico si rilassano certe condizioni in modo tale da avere a disposizione soluzioni computazionalmente pi\`u efficienti \cite{Alba73}, \cite{Asahiro2002}}. 
Il passo successivo, come al solito, consiste nella progettazione di una funzione di similarit\`a tra nodi e di una funzione che indichi la qualit\`a della divisione della rete. Una misura vastamente utilizzata in letteratura \`e la \textit{modularit\`a}. Essa si basa sul concetto di \textit{modello nullo} di un grafo $ G $ che, secondo Girvan e Newman \cite{NewGir04}, rappresenta una copia di $ G $ in cui gli archi sono riconfigurati in maniera completamente casuale, con il vincolo che il grado atteso di un nodo sia pari al grado che il nodo stesso possiede in $ G $. Per questa ragione il modello nullo presenta la fondamentale caratteristica di non possedere una struttura a comunit\`a \cite{Fortunato2010}. In definitiva, la modularit\`a indica quanto si discosta il grado di coesione di un certo partizionamento dal grado di coesione dello stesso partizionamento nel modello nullo.\\
La maggior parte degli algoritmi di clustering su grafi, in estrema sintesi, definiscono prima la struttura di una comunit\`a e, poi, in maniera algoritmica e incrementale, tentano di identificare i cluster nel grafo massimizzando la modularit\`a \cite{blondel2008fuc} [cit] [cit].

\subsubsection{Campionamento di grafi}
In molte applicazioni si ha la necessit\`a di eseguire complicati algoritmi su grafi: dalla simulazione dei protocolli di routing all'analisi degli scenari del \textit{viral markenting}. Spesso, per\`o, si ha a che fare con grafi di dimensioni elevate che renderebbero l'esecuzione di tali algoritmi non tollerabile dal punto di vista computazionale. Per questa ragione sono state sviluppate numerose tecniche di campionamento in cui il grafo campionato riflette le caratteristiche topologiche di quello originale. Le strategie pi\`u semplici sono quelle che si basano sul campionamento casuale di nodi (\textit{random node selection}) o di archi (\textit{random edge selection}). 
Tecniche pi\`u complicate utilizzano, invece, il concetto di \textit{esplorazione}: l'idea di fondo \`e quella di selezionare un nodo iniziale in maniera casuale e poi esplorare i nodi che sono ad esso vicini. Ad esempio, nel campionamento basato sui \textit{random walk} [cit] si sceglie casualmente un nodo iniziale e da questo si simula una random walk sul grafo: ad ogni passo o si seleziona un vicino del nodo precedente oppure con una probabilit\`a $ c $ si ricomincia dal nodo iniziale. Un'altra tecnica \`e il \textit{forest fire sampling} in cui, si seleziona un vertice iniziale (o \textit{seed}) e si iniziano a \virgolette{bruciare} gli archi uscenti dal seed. Se un arco \`e stato bruciato allora il vertice che si trova all'altra estremit\`a, a sua volta, ha la possibilit\`a di bruciare i propri archi, e cos\`i via di seguito \cite{Leskovec2006}. Altre tecniche interessati sono studiate in \cite{Krishnamurthy05}, \cite{Adler2001}, \cite{Airoldi05}.

\section{Indici e misure}
\label{section:indici_e_misure}
Al fine di studiare l'effetto delle connessioni tra gli individui sulla qualit\`a del clustering, \`e necessario catturare numericamente le sfaccettature della topologia. In prima approssimazione, un grafo \`e una collezione di nodi e archi, ed esiste una plausibile dipendenza tra il volume dei dati---il numero di nodi e archi---e la bont\`a del risultato, in funzione dell'algoritmo scelto.
Le pi\`u semplici misure di connettivit\`a del grafo sono gli indici beta e gamma, rispettivamente il rapporto tra nodi e archi ed il rapporto tra il numero di archi esistenti e il massimo numero teorico di archi in un grafo delle stesse dimensioni. Similmente, la transitivit\`a o coefficiente di clustering globale misura la probabilit\`a che nodi con un vicino in comune siano tra loro connessi \cite{wasserman1994social}.\\
Avendo studiato le propriet\`a teoriche delle reti sociali, \`e interessante quantificarle in un grafo sotto esame. L'assortativit\`a misura il grado di omofilia in una rete, cio\'e la preferenza di un nodo a connettersi a nodi che gli assomigliano. Limitatamente alla topologia, l'assortativit\`a pu\`o essere calcolata usando come criterio di similarit\`a il grado di un nodo, ovvero il numero di archi incidenti. In genere, le reti sociali esibiscono alti valori di assortativit\`a \cite{newman03social,newman02}. Un'altra caratterizzazione delle reti sociali \`e ottenuta dalla distribuzione del grado dei vertici. Una rete \`e detta \textit{scale-free} se la distribuzione di probabilit\`a dei gradi segue una legge di potenza---ovvero la frazione di nodi aventi grado $k$ \`e proporzionale a $k^{-\gamma}$---per un esponente $2<\gamma<3$. Nelle reti sociali, questa propriet\`a prende il nome di \textit{preferential attachment}\cite{Barabasi99emergenceScaling}. Stimando l'esponente $\gamma$ dai dati \`e possibile ricavare un altro indice della `socialit\`a' della rete.\\
La centralizzazione \`e un metodo per sintetizzare a livello di grafo i punteggi di centralit\`a---l'importanza relativa all'interno del grafo---dei singoli vertici \cite{freeman1979centrality,wasserman1994social}. Abbiamo considerato diverse misure di centralit\`a a livello di nodo: il grado, i cammini minimi, l'intermediet\`a, gli autovettori della matrice di adiacenza. La prossimit\`a o \textit{closeness} \`e funzione dal numero di passi necessari a raggiungere un qualsiasi altro nodo partendo dal vertice dato, e decresce man mano che ci si avvicina alla periferia della rete. L'intermediet\`a o \textit{betweenness} quantifica il numero di cammini minimi che attraversano il nodo, e descrive quanto un nodo sia un accentratore o \textit{hub} di relazioni sociali. La centralit\`a degli autovettori o \textit{eigenvector centrality} \cite{bonacich1987power} assume che la centralit\`a di un nodo sia proporzionale alle centralit\`a dei nodi a cui \`e connesso; in generale, i nodi con un punteggio elevato sono quelli connessi a molti altri nodi che, a loro volta, sono connessi a molti altri (e cos\`i via).\\
Complementare alla connettivit\`a \`e la frammentazione, cio\`e quanto un grafo si discosta dall'essere omogeneamente coeso e completamente connesso. La misura pi\`u ovvia di frammentazione di un grafo \`e il numero di componenti connesse o sottografi massimali; tuttavia questa misura trascura tanto la dimensione delle componenti---singoli vertici isolati dovrebbero contribuire poco alla frammentazione---quanto la struttura interna delle componenti. In considerazione della dimensione delle componenti abbiamo adottato la misura \textit{F} \cite{borgatti_2002} e l'entropia; per valutare il grado di coesione nelle componenti, si \`e fatto ricorso alla \textit{distance fragmentation} \cite{borgatti_2002}. Queste misure sono tutte statiche, ossia realizzano un istantanea della frammentazione del grafo trascurando la stabilit\`a delle componenti connesse esistenti. Per valutare la robustezza di un grafo abbiamo studiato l'evoluzione della misura \textit{F} man mano che i nodi pi\`u importanti della rete sono rimossi; in questo scenario, l'importanza di un nodo \`e la sua intermediet\`a o alternativamente la \textit{bridging centrality} \cite{Hwang_2008}.

\subsection{Indici di validit\`a del clustering}
Come affermato nel paragrafo X.Y.Z, una delle sfide fondamentali della cluster analysis \`e valutare i risultati degli algoritmi senza disporre di informazioni ausiliarie. Un approccio comune vede l'utilizzo dei gi\`a citati indici di validit\`a che si dividono in indici esterni (che valutano il raggruppamento basandosi su una struttura pre-specificata, ground truth) e indici interni (che si basano solamente sull'informazione intrinseca dei dati). La loro scelta deve essere preceduta da un'attenta analisi dell'algoritmo di clustering, in quanto quest'ultimo potrebbe ottimizzare proprio gli stessi parametri misurati dall'indice e ci\`o comporterebbe una valutazione non oggettiva dei risultati. Gli indici esterni non sono oggetto di studio del presente lavoro e quindi nominiamo solamente la \textit{F-measure}, la \textit{purezza} e l'\textit{entropia} che \`e possibile approfondire qui \cite{rendon2011}. \\
Riguardo, invece, gli indici interni citiamo e descriviamo quelli che sono stati utilizzati in questa tesi. La tecnica del \textit{Silhouette index} \cite{Rousseeuw1987}, ad esempio, assegna ad ogni oggetto $ x $ del dataset un punteggio che, tramite una funzione di similarit\`a, calcola quanto esso sia appropriatamente raggruppato: se $ x $ \`e simile agli oggetti del cluster cui appartiene e dissimile da tutti gli altri allora ricever\`a un punteggio elevato. La media dei punteggi rappresenta l'indice Silhouette.
Un'altra tecnica che \`e stata ampiamente utilizzata si basa sul calcolo della correlazione tra la matrice di prossimit\`a e la matrice di incidenza (vedi paragrafo X.Y.Z per le definizioni). La correlazione varia nell'intervallo $ [-1, 1] $, in cui valori prossimi ad 1 (-1) indicano che i punti appartenenti allo stesso cluster sono molto simili (dissimili) tra loro; valori vicini allo 0 indicano, sostanzialmente, la presenza di un clustering casuale. Per concludere ricordiamo la gi\`a citata \textit{modularit\`a} che pu\`o essere considerata, a tutti gli effetti, un indice di validit\`a interno.\\
C'\`e da precisare che questi indici non hanno un valore "dogmatico" ma offrono solo un suggerimento utile all'osservatore che potr\`a avvalersi di uno strumento in pi\`u per la valutazione soggettiva dei risultati. Detto altrimenti, la segmentazione di mercato non deve ritenersi risolta con lo studio degli indici di validit\`a ma deve continuare con una valutazione personale del decision maker, che stabilir\`a l'effettiva legittimit\`a dei risultati o ne modificher\`a la struttura basandosi anche su variabili esterne (come profili socio-demografici e geografici), che non hanno mai preso parte al processo di clustering.

\subsubsection{Misure di similarit\`a}
Come gi\`a affermato nei paragrafi precedenti, un passaggio fondamentale della cluster analysis \`e la definizione di una misura di similarit\`a tra gli oggetti del dataset. La scelta di questa funzione deve essere fatta accuratamente tenendo conto di vari fattori: (1) della \textit{tipologia di attributi} (una similarit\`a pensata per dati numerici, come quella euclidea, non avrebbe senso su dati categorici); (2) della \textit{dimensionalit\`a} dei dati, in quanto, come affermato in \cite{Aggarwal01}, in presenza di un alto numero di dimensioni, alcune definizioni di similarit\`a potrebbero essere non significative; (3) della presenza di missing value, poich\'e ci si potrebbe trovare a dover confrontare due istanze con un numero diverso di attributi: in questo caso, ad esempio, \`e possibile tener conto solo degli attributi comuni oppure trattare i due vettori di attributi come degli insiemi. Limitatamente a quanto impiegato nel presente lavoro, definiamo e descriviamo alcune misure interessanti.

\begin{description}
\item[Indice Jaccard] L'indice Jaccard \cite{jaccard} misura la similarit\`a tra due insiemi di oggetti ed \`e definito come il rapporto tra la cardinalit\`a della loro intersezione e la cardinalit\`a della loro unione. Viene utilizzato su dati categorici e risulta essere intrinsecamente flessibile in caso di valori mancanti. Il suo intervallo \`e $ [0, 1] $.

\item[Coseno di similitudine] Dati due vettori numerici di uguale dimensione, il coseno di similitudine \`e un numero compreso tra $ -1 $ e $ 1 $ ed \`e definito come prodotto scalare dei vettori normalizzati. Risulta essere molto efficace su dati con molte dimensioni \cite{ertoez02}

\item[Distanza di Mahalanobis] La distanza di Mahalanobis misura la dissimilarit\`a tra due vettori numerici di uguale dimensione. \`E una generalizzazione della distanza euclidea in quanto tiene conto della matrice di covarianza del dataset: infatti, se la matrice di covarianza \`e pari alla matrice identit\`a si riduce alla distanza euclidea. Viene spesso utilizzata per l'individuazione degli outlier all'interno di dataset dalla forma "ellittica" mostrando performance superiori alla distanza euclidea \cite{mahalanobis}.

\item[Distanza Frazionaria] La distanza frazionaria risulta essere molto efficace su dati con molte dimensioni, migliorando significativamente la qualit\`a degli algoritmi di clustering distance-based \cite{Aggarwal01}. Intuitivamente \`e molto simile alla distanza euclidea con la particolarit\`a di elevare al quadrato invece di usare la radice quadrata e, viceversa, usare la radice quadrata al posto di elevare al quadrato\footnote{In questo caso \`e, forse, molto pi\`u immediata leggere la formula analitica di questa misura per comprendere anche la completa generalizzazione: se $ m $ \`e il numero di dimensioni del dataset, la distanza frazionaria tra due vettori $ x $ e $ y $ \`e calcolata come: $ [\sum\limits_{i=1}^m (x_{i} - y_{i})^{f}]^{1/f} $ con $ f \in (0,1) $.}. \`e applicabile su dati numerici con ugual numero di dimensioni.
\end{description}

% [40] http://www-users.cs.umn.edu/~kumar/papers/high_dim_clustering_19.pdf
% [41] http://dl.acm.org/citation.cfm?id=1007731
% [42] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, and Prabhakar Raghavan. “ Automatic subspace clustering of high-dimensional data for data mining applications ,” In ACM SIGMOD Conference on Management of Data (1998).
% [43]  Subspace and projected clustering: experimental evaluation and analysis
% [44] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.3907&rep=rep1&type=pdf
% [45] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.2898&rep=rep1&type=pdf
% [46] Introduction to Graph Theory By: Richard J. Trudeau
% [47] Graph Theory and Networks in Biology
% [48] IncCluster
% [49] BAGC
% [50] McPherson, M., Smith-Lovin, L. and Cook, J. M. (2001) Birds of a feather: homophily in social networks. A. Rev. Sociol., 27, 415–444.
% [51] Lazarsfeld, P. and Merton, R. (1954) Friendship as social process: a substantive and methodological analysis. In Freedom and Control in Modern Society (eds M. Berger, T. Abel and C. Page), pp. 18–66. New York: Van Nostrand
% [52] Freeman, L. C. (1996) Some antecedents of social network analysis. Connections, 19, 39–42.
% [53] White, H. C., Boorman, S. A. and Breiger, R. L. (1976) Social-structure from multiple networks: I, Blockmodels of roles and positions. Am. J. Sociol., 81, 730–780.
% [54]  Watts, D. J.; Strogatz, S. H. (1998). "Collective dynamics of 'small-world' networks". Nature 393 (6684): 440–442. doi:10.1038/30918. PMID 9623998. edit
% [55] The structure and function of complex networks
% [56]Community detection in graphs Santo Fortunato
% [57] Community Detection based on Structural and Attribute Similarities
% [58] Luce, R. D., and A. D. Perry, 1949, Psychometrika 14(2), 95.
% [59] Bomze, I. M., M. Budinich, P. M. Pardalos, and M. Pelillo, 1999, in Handbook of Combinatorial Optimization, edited by D.-Z. Du and P. Pardalos (Kluwer Academic Publishers, Norwell, USA), pp. 1–74.
% [60] Garey, M. R., and D. S. Johnson, 1990, Computers and Intractability : A Guide to the Theory of NP-Completeness (W. H. Freeman & Co., New York, USA).
% [61] Alba, R. D., 1973, J. Math. Sociol. 3, 113.
% [62] Asahiro, Y., R. Hassin, and K. Iwama, 2002, Discrete Appl. Math. 121(1-3), 15
% [63] Newman, M. E. J., and M. Girvan, 2004, Phys. Rev. E 69(2),026113.
% [64] http://cse.iitkgp.ac.in/~pabitra/paper/tpami02_feature.pdf
% [65] Simultaneous Feature Selection and Clustering Using Mixture Models
% [66] Clustering the Feature Space Dino Ienco
% [67] https://www.lri.fr/~pierres/donn%E9es/save/these/articles/lpr-queue/hall99correlationbased.pdf
% [68] Feature Selection Algorithms: A Survey and Experimental Evaluation
% [69] http://www.machinelearning.org/proceedings/icml2007/papers/444.pdf
% [70] A survey on feature selection methods
% [71] Ren-Gal I., Outlier detection, In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers," Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2.
% [72] A Survey of Outlier Detection Methodologies.
% [73] Outlier Detection : A Survey
% [74] http://aaaipress.org/Papers/KDD/1997/KDD97-044.pdf
% [75] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.3621&rep=rep1&type=pdf
% [76] LOF: Identifying Density-Based Local Outliers
% [77] http://www.dbs.ifi.lmu.de/Publikationen/Papers/PKDD99-Outlier.pdf
% [78] Greenland S, Finkle WD. A critical look at methods for handling missing covariates in epidemiologic regression analyses. Am J Epide- miol 1995;142:1255e64.
% [79] Vach W. Logistic regression with missing values in the covariates.New York: Springer; 1994.
% [80]  Rubin DB. Multiple imputation for non response in surveys. NewYork: Wiley; 1987.
% [81] Schafer JL. Analysis of incomplete multivariate data. London: Chapman & Hall/CRC Press; 1997.
% [82] Little RA. Regression with missing X's; a review. J Am Stat Assoc1992;87:1227e37
% [83] Review: A gentle introduction to imputation of missing values
% [84] Little, R. J. A. and Rubin, D. B. (2002). “Statistical Analysis with Missing Data” 2nd edition, Wiley.
% [85] Shafer, J.L. (1997). “Analysis of Incomplete Multivariate Data” New York: CRC Press.
% [86] imputazione multipla rubin
% [87] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.9598&rep=rep1&type=pdf 
% [88] Sampling from Large Graphs leskovec
% [89] V. Krishnamurthy, M. Faloutsos, M. Chrobak, L. Lao,J.-H. Cui, and A. G. Percus. Reducing large internet topologies for faster simulations. In Networking, 2005.
% [90] M. Adler and M. Mitzenmacher. Towards compressingweb graphs. In Data Compression Conference, 2001.
% [91] E. M. Airoldi and K. M. Carley. Sampling algorithmsfor pure network topologies. SIGKDD Explor., 2005.
% [92] Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions
% [93] A Survey: Clustering Ensembles Techniques di Reza Ghaemi
% [94] http://www.eecs.wsu.edu/~holder/courses/CptS570/fall07/papers/Dietterich00.pdf
% [95] http://commonsenseatheism.com/wp-content/uploads/2013/07/Rokach-Ensemble-based-classifiers.pdf
% [96] Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysi
% [97] Internal versus External cluster validation indexes
% [98] Davies, David L.; Bouldin, Donald W. (1979). "A Cluster Separation Measure"
% [99]  A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters
% [100] Mitchel Machine Learning
% [101] Jaccard, Paul (1901), "\'etude comparative de la distribution florale dans une portion des Alpes et des Jura", Bulletin de la Soci\'et\'e Vaudoise des Sciences Naturelles 37: 547–579.
% [102] distanza di malhanoib Gnanadesikan, R., and J.R. Kettenring (1972). Robust estimates, residuals, and outlier detection with multiresponse data. Biometrics 28:81-124
% [103] Toward Integrating Feature Selection Algorithms for Classification and Clustering
% [104] On the Surprising Behavior of Distance Metrics in High Dimensional Space
% [105] A New Shared Nearest Neighbor Clustering Algorithm and its Applications 
